\documentclass{ximera}

\input{../preamble.tex}

\title{Similar Matrices and Jordan Normal Form}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 \label{S:6.5}

%In Section~\ref{S:LNFPS} we discussed solutions to differential equations
%$\dot{X}=CX$ for three classes of matrices $C$.  See Table~\ref{T:3sys}.
In a certain sense every $2\times 2$ matrix can be thought of as a member 
of one of three families of matrices.  Specifically we show that every 
$2\times 2$ matrix is similar to one of the matrices listed in Theorem~\ref{T:putinform}, 
where similarity is defined as follows.

\begin{definition}  \label{D:similar}
The $n\times n$ matrices $B$ and $C$ are {\em similar\/} if
there exists an invertible $n\times n$ matrix $P$ such that
\[
C = P\inv BP.
\]
\end{definition} \index{similar}\index{similar!matrices} \index{invertible}

Our interest in similar matrices stems from the fact that if we
know the solutions to the system of differential equations $\dot{Y}=CY$, 
then we also know the solutions to the system of differential equations 
$\dot{X}=BX$.  More precisely,
\begin{lemma}  \label{L:simsoln}
Suppose that $B$ and $C=P\inv BP$ are similar matrices.  If
$Y(t)$ is a solution to the system of differential equations
$\dot{Y}=CY$, then $X(t)=PY(t)$ is a solution to the system of 
differential equations $\dot{X}=BX$.
\end{lemma}

\begin{proof}   
Since the entries in the matrix $P$ are constants, it follows that
\[
\frac{dX}{dt} = P\frac{dY}{dt}.
\]
Since $Y(t)$ is a solution to the $\dot{Y}=CY$ equation, it follows that
\[
\frac{dX}{dt} = PCY.
\]
Since $Y=P\inv X$ and $PCP\inv = B$,
\[
\frac{dX}{dt} = PCP\inv X = BX.
\]
Thus $X(t)$ is a solution to $\dot{X}=BX$, as claimed.  
\end{proof}


\subsection*{Invariants of Similarity}

\begin{lemma}  \label{L:simdettr}
Let $A$ and $B$ be similar $2\times 2$ matrices.  Then
\begin{eqnarray*}
p_A(\lambda) & = & p_B(\lambda),\\
\det(A) & = & \det(B),\\
\trace(A) & = & \trace(B),
\end{eqnarray*} \index{characteristic polynomial}\index{trace}
and the eigenvalues of $A$ and $B$ are equal.
\end{lemma}

\begin{proof}
The determinant\index{determinant} is a function on $2\times 2$ matrices
that has several important properties.  Recall, in particular, from
Chapter~\ref{chap:matrices}, Theorem~\ref{propdet} that for any pair of
$2\times 2$ matrices $A$ and $B$:
\begin{equation} \label{e:detprod}
\det(AB) =  \det(A)\det(B),
\end{equation}
and for any invertible $2\times 2$ matrix $P$
\begin{equation}  \label{e:detinv}
\det(P\inv)  =  \frac{1}{\det(P)}.
\end{equation}

Let $P$ be an invertible $2\times 2$ matrix so that $B=P\inv AP$.
Using \eqref{e:detprod} and \eqref{e:detinv} we see that
\begin{eqnarray*}
p_B(\lambda) & = & \det(B-\lambda I_2) \\
 & = & \det(P\inv AP-\lambda I_2) \\
& = & \det(P\inv(A-\lambda I_2)P) \\
& = & \det(A-\lambda I_2) \\
& = & p_A(\lambda).
\end{eqnarray*}
Hence the eigenvalues of $A$ and $B$ are the same.  It follows
from \eqref{e:treigen} and \eqref{e:deteigen} of Section~\ref{S:evchp}
that the determinants and traces of $A$ and $B$ are equal.   \end{proof}

For example, if
\[
A = \mattwo{-1}{0}{0}{1} \AND  P = \mattwo{1}{2}{1}{1},
\]
then
\[
P\inv = \mattwo{-1}{2}{1}{-1}
\]
and
\[
P\inv AP = \mattwo{3}{4}{-2}{-3}.
\]
A calculation shows that
\[
\det(P\inv AP)=-1=\det(A) \AND {\rm tr}(P\inv AP)=0={\rm tr}(A),
\]
as stated in Lemma~\ref{L:simdettr}.




\subsection*{Classification of Jordan Normal Form $2\times 2$ Matrices}
\index{normal form}

We now classify all $2\times 2$ matrices up to similarity.

\begin{theorem}  \label{T:putinform}
Let $C$ and $P=(v_1|v_2)$ be $2\times 2$ matrices where the vectors
$v_1$ and $v_2$ are specified below.
\begin{itemize}
\item[(a)]	Suppose that $C$ has two linearly independent
real eigenvectors $v_1$ and $v_2$ with real eigenvalues $\lambda_1$
and $\lambda_2$.  Then
\[
P\inv CP = \mattwoc{\lambda_1}{0}{0}{\lambda_2}.
\]

\item[(b)]	Suppose that $C$ has no real eigenvectors and
complex conjugate eigenvalues $\sigma\pm i\tau$ where
$\tau\neq 0$.  Then
\[
P\inv CP = \mattwo{\sigma}{-\tau}{\tau}{\sigma},
\]
where $v_1 + iv_2$ is an eigenvector of $C$ associated with the
eigenvalue $\lambda_1=\sigma-i\tau$.

\item[(c)]	Suppose that $C$ has exactly one linearly
independent real eigenvector $v_1$ with real eigenvalue $\lambda_1$.
Then
\[
P\inv CP = \mattwoc{\lambda_1}{1}{0}{\lambda_1},
\]
where  $v_2$ is a generalized eigenvector of $C$ that satisfies
\begin{equation}  \label{e:Cw=lw+v}
(C-\lambda_1 I_2) v_2 =  v_1.
\end{equation}

\end{itemize}
\end{theorem}

\begin{proof}
The strategy in the proof of this theorem is to determine the
$1^{st}$ and $2^{nd}$ columns of $P\inv CP$ by computing (in each case)
$P\inv CPe_j$ for $j=1$ and $j=2$.  Note from the definition of $P$
that
\[
Pe_1 = v_1 \AND Pe_2 = v_2.
\]
In addition, if $P$ is invertible, then
\[
P\inv v_1 = e_1 \AND P\inv v_2 = e_2.
\]
Note that if $v_1$ and $v_2$ are linearly independent, then $P$ is invertible.

(a) \quad Since $v_1$ and $v_2$ are assumed to be linearly independent,
$P$ is invertible.  So we can compute
\[
P\inv CPe_1 = P\inv C v_1 = \lambda P\inv v_1 = \lambda e_1.
\]
It follows that the $1^{st}$ column of $P\inv CP$	is
\[
\vectwoc{\lambda_1}{0}.
\]
Similarly, the $2^{nd}$ column of $P\inv CP$ is
\[
\vectwoc{0}{\lambda_2}
\]
thus verifying (a).

(b) \quad  Lemma~\ref{L:rievind} implies that $v_1$ and $v_2$ are linearly
independent and hence that $P$ is invertible.  Using \eqref{e:complexcoord},
with $\tau$ replaced by $-\tau$, $v$ replaced by $v_1$, and $w$ replaced by
$w_1$, we calculate
\[
P\inv CPe_1 = P\inv Cv_1 = \sigma P\inv v_1 + \tau P\inv v_2
= \sigma e_1 + \tau e_2,
\]
and
\[
P\inv CPe_2 = P\inv Cv_2 = -\tau P\inv v_1 + \sigma P\inv v_2
= -\tau e_1 + \sigma e_2.
\]
Thus the columns of $P\inv CP$ are
\[
\vectwo{\sigma}{\tau} \AND \vectwo{-\tau}{\sigma},
\]
as desired.


(c) \quad   Let $v_1$ be an eigenvector and assume that $v_2$ is a
generalized eigenvector satisfying \eqref{e:Cw=lw+v}.  By
Lemma~\ref{L:geneig2} the vectors $v_1$ and $v_2$ exist and are linearly
independent.

For this choice of $v_1$ and $v_2$, compute
\[
P\inv CPe_1 = P\inv Cv_1 = \lambda_1 P\inv v_1 = \lambda_1 e_1,
\]
and
\[
P\inv CPe_2 = P\inv Cv_2 = P\inv v_1+\lambda_1 P\inv v_2 = e_1+\lambda_1 e_2.
\]
Thus the two columns of $P\inv CP$ are:
\[
\vectwoc{\lambda_1}{0} \AND \vectwoc{1}{\lambda_1}.
\]
  \end{proof}

\subsection*{Solutions of Jordan Normal Form Equations}

The eigenvectors of the matrices in Table~\ref{T:3sys}(a) are 
$v_1=(1,0)^t$ and $v_2=(0,1)^t$.  Hence, the closed form solution 
of (a) in that table follows from the direct solution in \eqref{E:RD2}.

The eigenvectors of the matrices in Table~\ref{T:3sys}(b) are $v_1 = v+iw$ 
and $v_2 = v-iw$, where $v=(0,1)^t$ and $w=(1,0)^t$. Hence, the closed form 
solution of (a) in that table follows from the direct solution in \eqref{e:exp1eva} 

Finally, the eigenvector and generalized eigenvector of the matrices in 
Table~\ref{T:3sys}(c) are $v_1 = (1,0)^t$ and $w_1 = (0,1)^t$. Hence, 
the closed form solution of (c) in that table follows from the direct 
solution in \eqref{E:CC1} 

\begin{table*}[t!]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
name  & normal form equations & closed form solution \\
\hline
(a) & $\dot{X} = \mattwo{\lambda_1}{0}{0}{\lambda_2} X$ &
$X(t) = \mattwo{e^{\lambda_1 t}}{0}{0}{e^{\lambda_2 t}}X_0$ \\
\hline
(b) & $\dot{X}=\mattwo{\sigma}{-\tau}{\tau}{\sigma}X$ & $X(t) = e^{\sigma t}
\mattwo{\cos(\tau t)}{-\sin(\tau t)}{\sin(\tau t)}{\cos(\tau t)}X_0$\\
\hline
(c) & $\dot{X} = \mattwo{\lambda_1}{1}{0}{\lambda_1}X$ &
$X(t) = e^{\lambda_1 t}\mattwo{1}{t}{0}{1}X_0$ \\
\hline
\end{tabular}
\caption{Solutions to Jordan normal form ODEs with $X(0)=X_0$.}
\label{T:3sys}
\end{center}
\end{table*}



\subsection*{Closed Form Solutions Using Similarity}
\index{closed form solution}

We now use Lemma~\ref{L:simsoln}, Theorem~\ref{T:putinform}, and the
explicit solutions to the normal form equations Table~\ref{T:3sys}
to find solutions for $\dot{X}=CX$ where $C$ is any $2\times 2$ matrix.
The idea behind the use of similarity to solve systems of ODEs is to
transform a given system into another normal form system whose solution is
already known.  This method is very much like the technique of change of
variables used when finding indefinite integrals in calculus.

We suppose that we are given a system of differential equations $\dot{X}=CX$
and use Theorem~\ref{T:putinform} to transform $C$ by similarity to one of
the normal form matrices listed in that theorem.  We then solve the
transformed equation (see
Table~\ref{T:3sys}) and use Lemma~\ref{L:simsoln} to transform the solution
back to the given system.

For example, suppose that $C$ has a complex eigenvalue $\sigma-i\tau$ with
corresponding eigenvector $v+iw$.  Then Theorem~\ref{T:putinform} states that
\[
B = P\inv CP = \mattwo{\sigma}{-\tau}{\tau}{\sigma},
\]
where $P=(v|w)$ is an invertible matrix.  Using Table~\ref{T:3sys} the
general solution to the system of equations $\dot{Y}=BY$ is:
\[
Y(t) = e^{\sigma t}
\mattwo{\cos(\tau t)}{-\sin(\tau t)}{\sin(\tau t)}{\cos(\tau t)}
\vectwo{\alpha}{\beta}.
\]
Lemma~\ref{L:simsoln} states that
\[
X(t) = PY(t)
\]
is the general solution to the $\dot{X}=CX$ system.  Moreover, we can solve
the initial value problem by solving
\[
X_0 = PY(0) = P\vectwo{\alpha}{\beta}
\]
for $\alpha$ and $\beta$.  In particular,
\[
\vectwo{\alpha}{\beta} = P\inv X_0.
\]
Putting these steps together implies that
\begin{equation} \label{e:exp0ev}
X(t) = e^{\sigma t}
P\mattwo{\cos(\tau t)}{-\sin(\tau t)}{\sin(\tau t)}{\cos(\tau t)}P\inv X_0
\end{equation}
is the solution to the initial value problem.

\subsubsection*{The Example with Complex Eigenvalues Revisited}
\index{eigenvalue!complex}

Recall the example in \eqref{e:complexexample}
\[
\frac{dX}{dt} = \mattwo{-1}{2}{-5}{-3} X,
\]
with initial values
\[
X_0=\vectwo{1}{1}.
\]
This linear system has a complex eigenvalue $\sigma-i\tau=-2-3i$ with
corresponding eigenvector
\[
v+iw = \vectwoc{2}{-1-3i}.
\]
Thus the matrix $P$ that transforms $C$ into normal form is
\[
P = \mattwo{2}{0}{-1}{-3} \AND P\inv = \frac{1}{6}\mattwo{3}{0}{-1}{-2}.
\]
It follows from \eqref{e:exp0ev} that the solution to the initial value problem
is
{\scriptsize \begin{eqnarray*}
X(t) & =  &
e^{-2t}P\mattwo{\cos(3t)}{-\sin(3t)}{\sin(3t)}{\cos(3t)}P\inv X_0 \\ & = &
\frac{1}{6}e^{-2t}\mattwo{2}{0}{-1}{-3}
\mattwo{\cos(3t)}{-\sin(3t)}{\sin(3t)}{\cos(3t)}
\mattwo{3}{0}{-1}{-2}\vectwo{1}{1}.
\end{eqnarray*}\normalsize}
A calculation gives
{\scriptsize\begin{eqnarray*}
X(t) & = & \frac{1}{2}e^{-2t}\mattwo{2}{0}{-1}{-3}
\mattwo{\cos(3t)}{-\sin(3t)}{\sin(3t)}{\cos(3t)}\vectwo{1}{-1}  \\
& = & e^{-2t}
\vectwoc{\cos(3t)+\sin(3t)}{\cos(3t)-2\sin(3t)}.
\end{eqnarray*}\normalsize}            
Thus the solution to \eqref{e:complexexample} that we have found using 
similarity of matrices is identical to the solution \eqref{e:complexexampleans}
that we found by the direct method.

Solving systems with either distinct real eigenvalues or equal eigenvalues
works in a similar fashion.



\includeexercises

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
