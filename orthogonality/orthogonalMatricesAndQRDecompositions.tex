\documentclass{ximera}

\input{../preamble.tex}

\title{Orthogonal Matrices and $QR$ Decompositions}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{S:QR}

In this section we describe an alternative approach to Gram-Schmidt
orthonormalization for constructing an orthonormal basis of a subspace
$W\subset\R^n$.  This method is called the $QR$ decomposition and is
numerically
superior to Gram-Schmidt.  Indeed, the $QR$ decomposition is the method used by
\Matlab to compute orthonormal bases.  To discuss this decomposition we need
to introduce a new type of matrices, the orthogonal matrices.

\subsection*{Orthogonal Matrices}

\begin{Def} \label{def:orthmat}
\index{matrix!orthogonal}
An $n\times n$ matrix $Q$ is {\em orthogonal\/} if its columns form an
orthonormal basis\index{basis!orthonormal}
of $\R^n$.
\end{Def}

The following lemma states elementary properties of orthogonal matrices:
\begin{lemma} \label{lem:orthprop}
Let $Q$ be an $n\times n$ matrix.  Then
\begin{itemize}
\item[(a)] $Q$ is orthogonal if and only if $Q^tQ=I_n$;
\item[(b)] $Q$ is orthogonal if and only if $Q^{-1} = Q^t$;
\item[(c)] If $Q_1,Q_2$ are orthogonal matrices, then $Q_1Q_2$ is
an orthogonal matrix.
\end{itemize}
\end{lemma}
\begin{proof}  (a) Let $Q=(v_1|\cdots|v_n)$.  Since $Q$ is orthogonal, the $v_j$
form an orthonormal basis.  By direct computation note that
$Q^tQ=\{(v_i\cdot v_j)\}=I_n$, since the $v_j$ are orthonormal. Note that
(b) is simply a restatement of (a).

\noindent (c) Now let $Q_1,Q_2$ be orthogonal. Then (a) implies
\[
(Q_1Q_2)^t(Q_1Q_2) = Q_2^tQ_1^tQ_1Q_2 = Q_2^tQ_2 = I_n,
\]
thus proving (c).  \end{proof}

The previous lemma together with \Ref{e:orthosym} in the proof of
Theorem~\ref{T:symmetricmat}(b) lead to the following result:
\begin{prop}  For each symmetric $n\times n$ matrix $A$, there exists an
orthogonal matrix $P$ such that $P^tAP$ is a diagonal matrix.
\end{prop}

\subsubsection*{Reflections Across Hyperplanes: Householder Matrices}

Useful examples of orthogonal matrices are reflections across
hyperplanes.  An $n-1$ dimensional subspace of $\R^n$ is called a
{\em hyperplane\/}.\index{hyperplane}  Let $V$ be a hyperplane and let $u$
be a nonzero vector normal to $V$.  Then a
{\em reflection\/}\index{reflection} across $V$ is
a linear map $H:\R^n\to\R^n$ such that
\begin{itemize}
\item[(a)] $Hv=v$ for all $v\in V$.
\item[(b)] $Hu = -u$.
\end{itemize}
We claim that the matrix of a reflection across a hyperplane is orthogonal
and there is a simple formula for that matrix.
\begin{Def} \label{D:Householder}
A {\em Householder matrix\/} is an $n\times n$ matrix of the form
\begin{equation} \label{eq:householder}
H=I_n - \frac{2}{u^tu} u u^t
\end{equation}
where $u\in\R^n$ is a nonzero vector. \index{matrix!Householder}.
\end{Def}
This definition makes sense since $u^tu=||u||^2$ is a number while the
product $u u^t$ is an $n\times n$ matrix.

\begin{lemma}
Let $u\in\R^n$ be a nonzero vector and let $V$ be the hyperplane orthogonal
to $u$.  Then the Householder matrix $H$ is a
reflection\index{reflection} across $V$ and is orthogonal.
\end{lemma}

\begin{proof}
By definition every vector $v\in V$ satisfies $u^tv=u\cdot v =0$.  Therefore,
\[
Hv = v - \frac{2}{u^tu} u u^tv = v,
\]
and
\[
Hu = u - \frac{2}{u^tu} u u^tu =  u-2u = -u.
\]
Hence $H$ is a reflection across the hyperplane $V$.  It also follows that
$H^2=I_n$ since $H^2v = H(Hv) = Hv = v$ for all $v\in V$ and $H^2u=H(-u)=u$.
So $H^2$ acts like the identity on a basis of $\R^n$ and $H^2=I_n$.

To show that $H$ is orthogonal, we first calculate
\[
H^t = I_n^t - \frac{2}{u^tu} (uu^t)^t= I_n - \frac{2}{u^tu}uu^t = H.
\]
Therefore $I_n = H H = HH^t$ and $H^t=H\inv$.  Now apply
Lemma~\ref{lem:orthprop}(b).   \end{proof}

\subsection*{$QR$ Decompositions}

The Gram-Schmidt process is not used in practice to find orthonormal bases
as there are other techniques available that are preferable for
orthogonalization on a computer.  One such procedure for the construction of
an orthonormal basis is based on $QR$ decompositions using {\em Householder
transformations}\index{matrix!Householder}.  This method is the one
implemented in \Matlab.

An $n\times k$ matrix $R=\{r_{ij}\}$ is {\em upper triangular\/} if
$r_{ij}=0$ whenever $i>j$.

\begin{Def}  \label{qr-Def} \index{QR decomposition}
An $n\times k$ matrix $A$ has a {\em $QR$ decomposition\/} if
\begin{equation} \label{eq:qrdecom}
A=QR.
\end{equation}
where $Q$ is an $n\times n$
orthogonal matrix\index{matrix!orthogonal}
and $R$ is an $n\times k$
upper triangular matrix\index{matrix!upper triangular} $R$.
\end{Def}

$QR$ decompositions can be used to find orthonormal bases as follows.
Suppose that ${\cal W} = \{w_1,\ldots,w_k\}$ is a basis for the subspace
$W\subset\R^n$.  Then define the $n\times k$ matrix $A$ which has the $w_j$
as columns, that is
\[
A = (w_1^t|\cdots|w_k^t).
\]
Suppose that $A=QR$ is a $QR$ decomposition.  Since $Q$ is orthogonal, the
columns of $Q$ are orthonormal.  So write
\[
Q = (v_1^t|\cdots|v_n^t).
\]
On taking transposes we arrive at the equation $A^t=R^tQ^t$:
\[
\left(\begin{array}{c} w_1 \\ \vdots \\ w_k \end{array} \right)
= \left(\begin{array}{cccccc}  r_{11} & 0 & \cdots & 0 & \cdots & 0\\
	r_{12} & r_{22} & \cdots & 0  & \cdots & 0 \\
	\vdots & \vdots & \cdots & \vdots & \cdots & \vdots\\
	r_{1k} & r_{2k} & \cdots & r_{kk}& \cdots & 0 \end{array} \right)
 \left(\begin{array}{c} v_1 \\ \vdots \\ v_n \end{array} \right).
\]
By equating rows in this matrix equation we arrive at the system
\begin{equation} \label{eq:wrv}
\begin{array}{rcl}
w_1 & = & r_{11}v_1 \\
w_2 & = & r_{12}v_1 + r_{22}v_2 \\
& \vdots & \\
w_k & = & r_{1k}v_1 + r_{2k}v_2 + \cdots + r_{kk}v_k.
\end{array}
\end{equation}
It now follows that the $W=\Span\{v_1,\ldots,v_k\}$ and that
$\{v_1,\ldots,v_k\}$ is an orthonormal basis for $W$.  We have proved:

\begin{prop} \label{prop:qrdec}
Suppose that there exist an orthogonal $n\times n$ matrix $Q$ and an upper
triangular $n\times k$ matrix $R$ such that the $n\times k$ matrix $A$ has
a $QR$ decomposition\index{QR decomposition}
\[
A=QR.
\]
Then the first $k$ columns $v_1,\ldots,v_k$ of the matrix $Q$ form an
orthonormal basis\index{basis!orthonormal}
of the subspace $W=\Span\{w_1,\ldots,w_k\}$, where the $w_j$
are the columns of $A$.  Moreover, $r_{ij}=v_i\cdot w_j$ is the coordinate
of $w_j$ in the orthonormal basis.
\end{prop}

Conversely, we can also write down a $QR$ decomposition for a matrix $A$, if
we have computed an orthonormal basis for the columns of $A$.  Indeed, using
the Gram-Schmidt process, Theorem~\ref{T:orthobasis}, we have shown that
$QR$ decompositions always exist.  In the remainder of this section we
discuss a different way for finding $QR$ decompositions using Householder
matrices.


\subsubsection*{Construction of a $QR$ Decomposition Using Householder
Matrices} \index{QR decomposition!using Householder matrices}

The $QR$ decomposition by Householder transformations is based on the
following observation :

\begin{prop} \label{prop:Housej}
Let $z=(z_1,\ldots,z_n)\in\R^n$ be nonzero and let
\[
r=\sqrt{z_j^2  + \cdots + z_n^2}.
\]
Define $u=(u_1,\ldots,u_n)\in\R^n$ by
\[
\left(\begin{array}{c}  u_1 \\ \vdots \\ u_{j-1} \\u_j \\ u_{j+1} \\ \vdots
\\ u_n \end{array}  \right)
=
\left(\begin{array}{c}  0 \\ \vdots \\ 0 \\ z_j-r \\  z_{j+1} \\ \vdots
\\ z_n \end{array}  \right).
\]
Then
\[
2u^t z = u^tu
\]
and
\begin{equation} \label{eq:Housej}
Hz = \left(\begin{array}{c} z_1\\  \vdots\\z_{j-1}\\ r \\ 0\\ \vdots\\ 0
\end{array}\right)
\end{equation}
holds for the Householder matrix $H=I_n-\frac{2}{u^tu}uu^t$.
\end{prop}

\begin{proof}  Begin by computing
\begin{eqnarray*}
u^t z &=& u_j z_j + z_{j+1}^2 + \cdots + z_n^2 \\
& = & z_j^2 -rz_j +  z_{j+1}^2 + \cdots + z_n^2 \\
& = &  - rz_j + r^2.
\end{eqnarray*}
Next, compute
\begin{eqnarray*}
u^t u & = &  (z_j-r)(z_j-r) + z_{j+1}^2 + \cdots + z_n^2 \\
& = &  z_j^2 - 2rz_j + r^2 +  z_{j+1}^2 + \cdots + z_n^2 \\
& = & 2(-rz_j + r^2).
\end{eqnarray*}
Hence $2u^tz = u^t u$, as claimed.

Note that $z-u$ is the vector on the right hand side of \Ref{eq:Housej}.
So, compute
\[
Hz = \left(I_n - \frac{2}{u^tu} u u^t\right)z = z -\frac{2u^t z}{u^tu} u = z-u
\]
to see that \Ref{eq:Housej} is valid.  \end{proof}

An inspection of the proof of Proposition~\ref{prop:Housej} shows that we
could have chosen
\[
u_j = z_j + r
\]
instead of $u_j=z_j - r$.  Therefore, the choice of $H$ is not unique.

Proposition~\ref{prop:Housej} allows us to determine inductively a $QR$
decomposition of the matrix
\[
A=(w_1^0|\cdots|w_k^0),
\]
where each $w_j^0\in\R^n$.  So, $A$ is an $n\times k$ matrix and $k\leq n$.

First, set $z = w_1^0$ and use Proposition~\ref{prop:Housej} to construct the
Householder matrix\index{matrix!Householder} $H_1$ such that
\[
H_1 w_1^0 =\left(\begin{array}{c} r_{11}\\ 0 \\ \vdots\\ 0 \end{array}\right)
\equiv r_1.
\]
Then the matrix $A_1=H_1A$ can be written as
\[
A_1=(r_1|w_2^1|\cdots|w_k^1),
\]
where $w_j^1=H_1w_j^0$ for $j=2,\ldots,k$.

Second, set $z=w_2^1$ in Proposition~\ref{prop:Housej} and construct the
Householder matrix $H_2$ such that
\[
H_2 w_2^1 =\left(\begin{array}{c} r_{12}\\ r_{22}\\ 0 \\ \vdots\\ 0
\end{array}\right) \equiv r_2.
\]
Then the matrix $A_2=H_2A_1=H_2H_1A$ can be written as
\[
A_2=(r_1|r_2|w_3^2|\cdots|w_k^2)
\]
where $w_j^2=H_2w_j^1$ for $j=3,\ldots,k$. Observe that the $1^{st}$ column
$r_1$ is not affected by the matrix multiplication, since $H_2$ leaves the
first component of a vector unchanged.

Proceeding inductively, in the $i^{th}$ step, set $z=w_i^{i-1}$ and use
Proposition~\ref{prop:Housej} to construct the Householder matrix $H_i$ such
that:
\[
H_i w_i^{i-1} = \left(\begin{array}{c} r_{1i}\\ \vdots\\r_{ii}\\ 0 \\
\vdots\\ 0 \end{array}\right) \equiv r_i
\]
and the matrix $A_i=H_iA_{i-1}=H_i\cdots H_1 A$ can be written as
\[
A_i=(r_1|\cdots|r_i|w_{i+1}^i|\cdots|w_k^i),
\]
where $w_i^2=H_iw_j^{i-1}$ for $j=i+1,\ldots,k$.

After $k$ steps we arrive at
\[
H_k\cdots H_1 A = R,
\]
where $R=(r_1|\cdots|r_k)$ is an upper triangular $n\times k$ matrix.
Since the Householder matrices $H_1,\ldots,H_k$ are orthogonal, it follows
from Lemma~\ref{lem:orthprop}(c) that the $Q^t = H_k\cdots H_1$ is
orthogonal.  Thus, $A = QR$ is a $QR$ decomposition of $A$.
\index{QR decomposition}

\subsection*{Orthonormalization with \Matlab}
\index{orthonormalization!with \protect\Matlab}

Given a set $w_1,\ldots,w_k$ of linearly independent vectors in $\R^n$
the \Matlab command {\tt qr} allows us to compute an
orthonormal basis\index{basis!orthonormal}
of the spanning set\index{spanning set}
of these vectors.  As mentioned earlier, the underlying
technique \Matlab uses for the computation of the $QR$ decomposition is based
on
Householder transformations.

The syntax of the $QR$ decomposition in \Matlab is quite simple.  For example,
let $w_1=(1,0,-1,0)$, $w_2=(2,-1,0,1)$ and $w_3=(0,0,-2,1)$ be the three
vectors in \Ref{eq:gsexam}.  In Section~\ref{S:5.5} we computed an orthonormal
basis for the subspace of $\R^4$ spanned by $w_1,w_2,w_3$.  Here we use the
\Matlab command {\tt qr} to find an orthonormal basis for this subspace.
Let $A$ be the matrix having the vectors $w_1^t,w_2^t$ and $w_3^t$
as columns.  So, $A$ is:
\begin{verbatim}
A = [1 2 0; 0 -1 0; -1 0 -2; 0 1 1]
\end{verbatim}
The command \index{\computer!qr}
\begin{verbatim}
[Q R] = qr(A,0)
\end{verbatim}
leads to the answer
\begin{verbatim}
Q =
   -0.7071    0.5000   -0.4523
         0   -0.5000   -0.1508
    0.7071    0.5000   -0.4523
         0    0.5000    0.7538
R =
   -1.4142   -1.4142   -1.4142
         0    2.0000   -0.5000
         0         0    1.6583
\end{verbatim}
A comparison with \Ref{eq:gsoresult} shows that the columns of the matrix
{\tt Q} are the elements in the orthonormal basis.  The only difference
is that the sign of the first vector is opposite.  However, this is
not surprising since we know that there is some freedom in the choice of
Householder matrices, as remarked after Proposition~\ref{prop:Housej}.

In addition, the command {\tt qr} produces the matrix {\tt R} whose
entries $r_{ij}$ are the coordinates of the vectors $w_j$ in the
new orthonormal basis as in \Ref{eq:wrv}.  For instance, the second
column of {\tt R} tells us that
\[
w_2 = r_{12}v_1 + r_{22}v_2 + r_{32}v_3 = -1.4142 v_1 + 2.0000 v_2.
\]


\EXER

\TEXER

\noindent In Exercises~\ref{c7.9.1a} -- \ref{c7.9.1e} decide whether or not
the given matrix is orthogonal.
\begin{exercise} \label{c7.9.1a}
$\left(\begin{array}{rr} 2 & 0\\ 0 & 1\end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c7.9.1b}
$\left(\begin{array}{rrr} 0 & 1 & 0\\ 0 & 0 & 1\\
1 & 0 & 0\end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c7.9.1c}
$\left(\begin{array}{rrr} 0 & -1 & 0\\ 0 & 0 & 1\\
-1 & 0 & 0\end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c7.9.1d}
$\left(\begin{array}{rr} \cos(1) & -\sin(1)\\ \sin(1) & \cos(1)
\end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c7.9.1e}
$\left(\begin{array}{rrr} 1 & 0 & 4\\ 0 & 1 & 0
\end{array}\right)$.
\end{exercise}

\begin{exercise} \label{c7.9.2}
Let $Q$ be an orthogonal $n\times n$ matrix.
Show that $Q$ preserves the length of vectors, that is
\[
\|Qv\| = \|v\|\quad \mbox{for all $v\in\R^n$.}
\]
\end{exercise}

\noindent In Exercises~\ref{c7.9.3a} -- \ref{c7.9.3d}, compute the
Householder matrix $H$ corresponding to the given vector $u$.
\begin{exercise} \label{c7.9.3a}
$u = \left(\begin{array}{r} 1\\ 1 \end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c7.9.3b}
$u = \left(\begin{array}{r} 0\\ -2 \end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c7.9.3c}
$u = \left(\begin{array}{r} -1\\ 1 \\5\end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c7.9.3d}
$u = \left(\begin{array}{r} 1\\ 0 \\ 4\\ -2\end{array}\right)$.
\end{exercise}

\begin{exercise} \label{c7.9.4}
Find the matrix that reflects the plane across the line generated by the
vector $(1,2)$.
\end{exercise}

\begin{exercise}  \label{c7.9.45}
Prove that the rows of an $n\times n$ orthogonal matrix form an orthonormal
basis for $\R^n$.
\end{exercise}

\CEXER

In Exercises~\ref{c7.5.5a} -- \ref{c7.5.5d}, use the \Matlab command
{\tt qr} to compute an orthonormal basis for each of the subspaces spanned
by the given set of vectors.
\begin{exercise} \label{c7.5.5a}
$w_1=(1,-1),\quad w_2=(1,2)$.
\end{exercise}
\begin{exercise} \label{c7.5.5b}
$w_1=(1,-2,3),\quad w_2=(0,1,1)$.
\end{exercise}
\begin{exercise} \label{c7.5.5c}
$w_1=(1,-2,3),\quad w_2=(0,1,1),\quad w_3=(2,2,0)$.
\end{exercise}
\begin{exercise} \label{c7.5.5d}
$v_1=(1,0,-2,0,-1),\quad v_2=(2,-1,4,2,0),\quad v_3=(0,3,5,1,-1)$.
\end{exercise}

\begin{exercise}  \label{c7.5.6}
Find the $4\times 4$ Householder matrices $H_1$ and $H_2$ corresponding to
the vectors
\begin{equation*}
\begin{array}{rcl}
u_1 & = & (1.04,2,0.76,-0.32) \\
u_2 & = & (1.4,-1.3,0.6,1.2).
\end{array}
\end{equation*}
Compute $H=H_1H_2$ and verify that $H$ is an orthogonal matrix.
\end{exercise}




\end{document}
