\documentclass{ximera}

\input{../preamble.tex}

\title{The Spectral Theory of Symmetric Matrices}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{S:symmetric}

Eigenvalues and eigenvectors of symmetric matrices have remarkable properties that can be
summarized in three theorems.

\begin{theorem}  \label{T:symmetricmat}
Let $A$ be a symmetric matrix\index{matrix!symmetric}.
Then every eigenvalue\index{eigenvalue!of symmetric matrix} of $A$ is real.
\end{theorem}

\begin{theorem}  \label{T:symmetricmatvector}
Let $A$ be an $n\times n$ symmetric matrix\index{matrix!symmetric}.
Then there is an orthonormal basis\index{basis!orthonormal}
of $\R^n$ consisting of eigenvectors of $A$.
\end{theorem}

\begin{theorem}  \label{T:symmetricmatdiag}
For each $n\times n$ symmetric matrix $A$, there exists an
orthogonal matrix $P$ such that $P^tAP$ is a diagonal matrix.
\end{theorem}

The proof of Theorem~\ref{T:symmetricmat} uses the {\em Hermitian inner
product}\index{Hermitian inner product} --- a generalization of
dot product\index{dot product} to complex vectors\index{vector!complex}.

\subsubsection*{Hermitian Inner Products}

Let $v,w\in\C^n$ be two complex $n$-vectors.  Define
\[
\langle v,w \rangle = v_1\overline{w}_1 + \cdots + v_n\overline{w}_n.
\]
Note that the coordinates $w_i$ of the second vector enter this formula
with a complex conjugate.  However, if $v$ and $w$ are real vectors, then
\[
\langle v,w \rangle = v\cdot w.
\]
An alternative notation for the Hermitian inner product is given by
matrix multiplication\index{matrix!multiplication}.
Suppose that $v$ and $w$ are column $n$-vectors.
Then
\[
\langle v,w \rangle = v^t\overline{w}.
\]

The properties of the Hermitian inner product are similar to those of dot
product.  We note three.  Let $c\in\C$ be a complex scalar.  Then
\begin{eqnarray*}
\langle v,v \rangle & = & ||v||^2\ge 0\\
\langle cv,w \rangle & = & c\langle v,w \rangle \\
\langle v,cw \rangle & = & \overline{c} \langle v,w \rangle
\end{eqnarray*}
Note the complex conjugation of the complex scalar $c$ in the previous
formula.

Let $C$ be a complex $n\times n$ matrix.  Then the most important observation
concerning Hermitian inner products that we shall use is:
\begin{equation} \label{e:hermite_matrix}
\langle Cv,w \rangle = \langle v,\overline{C}^tw \rangle.
\end{equation}
This fact is verified by calculating
\[
\langle Cv,w \rangle = (Cv)^t\overline{w} = (v^tC^t)\overline{w}
= v^t(C^t\overline{w}) = v^t(\overline{\overline{C}^tw})
= \langle v,\overline{C}^tw \rangle.
\]
So if $A$ is a $n\times n$ real symmetric matrix, then
\begin{equation}   \label{e:symminv}
\langle Av,w \rangle = \langle v,Aw \rangle,
\end{equation}
since $\overline{A}^t= A^t = A$.

\begin{proof}[Proof of Theorem~\ref{T:symmetricmat}]  Let $\lambda$
be an eigenvalue of $A$ and let $v$ be the associated eigenvector. Since
$Av=\lambda v$ we can use \eqref{e:symminv} to compute
\[
\lambda \langle v,v \rangle = \langle Av,v \rangle = \langle v,Av \rangle
= \overline{\lambda} \langle v,v \rangle.
\]
Since $\langle v,v \rangle = ||v||^2 > 0$, it follows that
$\lambda=\overline{\lambda}$ and $\lambda$ is real.  \end{proof}


\begin{proof}[Proof of Theorem~\ref{T:symmetricmatvector}]
Let $A$ be a real symmetric $n\times n$ matrix.  We show that there is an 
orthonormal basis of $\R^n$ consisting of eigenvectors of $A$.  The proof
follows directly from Corollary~\ref{C:symmetric_distinct} if the  
eigenvalues are distinct.  

If some of the eigenvalues are multiple, the proof is more complicated and 
uses Gram-Schmidt orthonormalization.  The proof proceeds inductively on 
$n$.   The theorem is trivially valid for $n=1$; so we assume that it is valid 
for $n-1$.

Theorem~\ref{T:eigens} of Chapter~\ref{C:D&E} implies that $A$ has an 
eigenvalue $\lambda_1$ and Theorem~\ref{T:symmetricmat} states that 
this eigenvalue is real.
Let $v_1$ be a unit length eigenvector corresponding to the eigenvalue
$\lambda_1$.  Extend $v_1$ to an orthonormal basis $v_1,w_2,\ldots,w_n$ of
$\R^n$ and let $P=(v_1|w_2|\cdots|w_n)$ be the matrix whose columns are the
vectors in this orthonormal basis.  Orthonormality and direct multiplication
implies that
\begin{equation}  \label{e:orthosym}
P^tP=I_n.
\end{equation}
Therefore $P$ is invertible; indeed $P\inv=P^t$.
Next, let
\[
B= P\inv AP.
\]
By direct computation
\[
Be_1 = P\inv APe_1 = P\inv Av_1 = \lambda_1 P\inv v_1=\lambda_1e_1.
\]
It follows that that $B$ has the form
\[
B = \mattwo{\lambda_1}{*}{0}{C}
\]
where $C$ is an $(n-1)\times (n-1)$ matrix.   Since $P\inv=P^t$, it follows
that $B$ is a symmetric matrix; to verify this point compute
\[
B^t = (P^t AP)^t = P^t A^t (P^t)^t = P^tAP = B.
\]
It follows that
\[
B =\mattwo{\lambda_1}{0}{0}{C}
\]
where $C$ is a symmetric matrix.  By induction we can use the Gram-Schmidt 
orthonormalization process to choose an orthonormal
basis $z_2,\ldots,z_n$ in $\{0\}\times\R^{n-1}$ consisting of eigenvectors
of $C$.  It follows that $e_1,z_2,\ldots,z_n$ is an orthonormal basis for
$\R^n$ consisting of eigenvectors of $B$.

Finally, let $v_j=P\inv z_j$ for $j=2,\ldots,n$.  Since $v_1=P\inv e_1$,
it follows that  $v_1,v_2,\ldots,v_n$ is a basis of $\R^n$ consisting of
eigenvectors of $A$.  We need only show that the $v_j$ form an orthonormal
basis of $\R^n$.   This is done using \eqref{e:symminv}.  For notational
convenience let $z_1=e_1$ and compute
\begin{align*}
\langle v_i,v_j \rangle  &=\langle P\inv z_i,P\inv z_j\rangle =
                           \langle P^tz_i, P^tz_j \rangle \\
  &= \langle z_i, PP^t z_j \rangle =
\langle z_i,z_j \rangle,
\end{align*}
since $PP^t= I_n$.  Thus the vectors $v_j$ form an orthonormal basis since
the vectors $z_j$ form an orthonormal basis.  \end{proof}



%The previous lemma together with \eqref{e:orthosym} in the proof of Theorem~\ref{T:symmetricmat}(b) lead to the following result:

\begin{proof}[Proof of Theorem~\ref{T:symmetricmatdiag}]
As a consequence of Theorem~\ref{T:symmetricmatvector}, let
${\cal V}=\{v_1,\ldots,v_n\}$ be an orthonormal basis for $\R^n$
consisting of eigenvectors of $A$.  Indeed, suppose
\[
Av_j = \lambda_jv_j
\]
where $\lambda_j\in\R$.  Note that
\[
Av_j\cdot v_i =  \left\{\begin{array}{rl} \lambda_j & \qquad i=j\\
			0 & \qquad i\neq j \end{array}\right.
\]
It follows from \eqref{e:coordorthomat} that
\[
[A]_{\cal V}= \left(\begin{array}{ccc} \lambda_1 & & 0 \\  & \ddots & \\
	0 &  & \lambda_n \end{array}\right)
\]
is a diagonal matrix.  So every symmetric matrix $A$ is similar by 
an orthogonal matrix $P$ to a diagonal
matrix where $P$ is the matrix whose columns are the eigenvectors of $A$; 
namely, $P = [v_1|\cdots|v_n]$.  \end{proof}







\includeexercises



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
