\documentclass{ximera}

\input{../preamble.tex}

\title{Symmetric and Orthogonal Matrices}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{S:symmetric}

Symmetric matrices have some remarkable properties that can be
summarized by:
\begin{theorem}  \label{T:symmetricmat}
Let $A$ be an $n\times n$ symmetric matrix\index{matrix!symmetric}.
Then
\begin{enumerate}
\item[(a)] every eigenvalue\index{eigenvalue!of symmetric matrix}
of $A$ is real, and
\item[(b)] there is an orthonormal basis\index{basis!orthonormal}
of $\R^n$ consisting of
	eigenvectors of $A$.
\end{enumerate}
\end{theorem}

\subsubsection*{Hermitian Inner Products}

The proof of Theorem~\ref{T:symmetricmat} uses the {\em Hermitian inner
product}\index{Hermitian inner product} --- a generalization of
dot product\index{dot product} to complex vectors\index{vector!complex}.
Let $v,w\in\C^n$ be two complex $n$-vectors.  Define
\[
\langle v,w \rangle = v_1\overline{w}_1 + \cdots + v_n\overline{w}_n.
\]
Note that the coordinates $w_i$ of the second vector enter this formula
with a complex conjugate.  However, if $v$ and $w$ are real vectors, then
\[
\langle v,w \rangle = v\cdot w.
\]
A more compact notation for the Hermitian inner product is given by
matrix multiplication\index{matrix!multiplication}.
Suppose that $v$ and $w$ are column $n$-vectors.
Then
\[
\langle v,w \rangle = v^t\overline{w}.
\]

The properties of the Hermitian inner product are similar to those of dot
product.  We note three.  Let $c\in\C$ be a complex scalar.  Then
\begin{eqnarray*}
\langle v,v \rangle & = & ||v||^2\ge 0\\
\langle cv,w \rangle & = & c\langle v,w \rangle \\
\langle v,cw \rangle & = & \overline{c} \langle v,w \rangle
\end{eqnarray*}
Note the complex conjugation of the complex scalar $c$ in the previous
formula.

Let $C$ be a complex $n\times n$ matrix.  Then the main observation
concerning Hermitian inner products that we shall use is:
\[
\langle Cv,w \rangle = \langle v,\overline{C}^tw \rangle.
\]
This fact is verified by calculating
\[
\langle Cv,w \rangle = (Cv)^t\overline{w} = (v^tC^t)\overline{w}
= v^t(C^t\overline{w}) = v^t(\overline{\overline{C}^tw})
= \langle v,\overline{C}^tw \rangle.
\]
So if $A$ is a $n\times n$ real symmetric matrix, then
\begin{equation}   \label{e:symminv}
\langle Av,w \rangle = \langle v,Aw \rangle,
\end{equation}
since $\overline{A}^t= A^t = A$.

\begin{proof}[Proof of Theorem~\ref{T:symmetricmat}(a)]  Let $\lambda$
be an eigenvalue of $A$ and let $v$ be the associated eigenvector. Since
$Av=\lambda v$ we can use \eqref{e:symminv} to compute
\[
\lambda \langle v,v \rangle = \langle Av,v \rangle = \langle v,Av \rangle
= \overline{\lambda} \langle v,v \rangle.
\]
Since $\langle v,v \rangle = ||v||^2 > 0$, it follows that
$\lambda=\overline{\lambda}$ and $\lambda$ is real.  \end{proof}


\begin{proof}[Proof of Theorem~\ref{T:symmetricmat}(b)]
Let $A$ be a real symmetric $n\times n$ matrix.  We want to show that there
is an orthonormal basis of $\R^n$ consisting of eigenvectors of $A$.  The
proof proceeds inductively on $n$.   The theorem is trivially valid for
$n=1$; so we assume that it is valid for $n-1$.

Theorem~\ref{T:eigens} of Chapter~\ref{C:D&E} implies that $A$ has an 
eigenvalue $\lambda_1$ and Theorem~\ref{T:symmetricmat}(a) states that 
this eigenvalue is real.
Let $v_1$ be a unit length eigenvector corresponding to the eigenvalue
$\lambda_1$.  Extend $v_1$ to an orthonormal basis $v_1,w_2,\ldots,w_n$ of
$\R^n$ and let $P=(v_1|w_2|\cdots|w_n)$ be the matrix whose columns are the
vectors in this orthonormal basis.  Orthonormality and direct multiplication
implies that
\begin{equation}  \label{e:orthosym}
P^tP=I_n.
\end{equation}
Therefore $P$ is invertible; indeed $P\inv=P^t$.

Next, let
\[
B= P\inv AP.
\]
By direct computation
\[
Be_1 = P\inv APe_1 = P\inv Av_1 = \lambda_1 P\inv v_1=\lambda_1e_1.
\]
It follows that that $B$ has the form
\[
B = \mattwo{\lambda_1}{*}{0}{C}
\]
where $C$ is an $(n-1)\times (n-1)$ matrix.   Since $P\inv=P^t$, it follows
that $B$ is a symmetric matrix; to verify this point compute
\[
B^t = (P^t AP)^t = P^t A^t (P^t)^t = P^tAP = B.
\]
It follows that
\[
B =\mattwo{\lambda_1}{0}{0}{C}
\]
where $C$ is a symmetric matrix.  By induction we can choose an orthonormal
basis $z_2,\ldots,z_n$ in $\{0\}\times\R^{n-1}$ consisting of eigenvectors
of $C$.  It follows that $e_1,z_2,\ldots,z_n$ is an orthonormal basis for
$\R^n$ consisting of eigenvectors of $B$.

Finally, let $v_j=P\inv z_j$ for $j=2,\ldots,n$.  Since $v_1=P\inv e_1$,
it follows that  $v_1,v_2,\ldots,v_n$ is a basis of $\R^n$ consisting of
eigenvectors of $A$.  We need only show that the $v_j$ form an orthonormal
basis of $\R^n$.   This is done using \eqref{e:symminv}.  For notational
convenience let $z_1=e_1$ and compute
\begin{align*}
\langle v_i,v_j \rangle  &=\langle P\inv z_i,P\inv z_j\rangle =
                           \langle P^tz_i, P^tz_j \rangle \\
  &= \langle z_i, PP^t z_j \rangle =
\langle z_i,z_j \rangle,
\end{align*}
since $PP^t= I_n$.  Thus the vectors $v_j$ form an orthonormal basis since
the vectors $z_j$ form an orthonormal basis.  \end{proof}


\subsection*{Orthogonal Matrices}

\begin{definition} \label{def:orthmat}
\index{matrix!orthogonal}
An $n\times n$ matrix $Q$ is {\em orthogonal\/} if its columns form an
orthonormal basis\index{basis!orthonormal}
of $\R^n$.
\end{definition}



The following lemma states elementary properties of orthogonal matrices:
\begin{lemma} \label{lem:orthprop}
Let $Q$ be an $n\times n$ matrix.  Then
\begin{itemize}
\item[(a)] $Q$ is orthogonal if and only if $Q^tQ=I_n$;
\item[(b)] $Q$ is orthogonal if and only if $Q^{-1} = Q^t$;
\item[(c)] If $Q_1,Q_2$ are orthogonal matrices, then $Q_1Q_2$ is
an orthogonal matrix.
\end{itemize}
\end{lemma}
\begin{proof}  (a) Let $Q=(v_1|\cdots|v_n)$.  Since $Q$ is orthogonal, the $v_j$
form an orthonormal basis.  By direct computation note that
$Q^tQ=\{(v_i\cdot v_j)\}=I_n$, since the $v_j$ are orthonormal. Note that
(b) is simply a restatement of (a).

\noindent (c) Now let $Q_1,Q_2$ be orthogonal. Then (a) implies
\[
(Q_1Q_2)^t(Q_1Q_2) = Q_2^tQ_1^tQ_1Q_2 = Q_2^tQ_2 = I_n,
\]
thus proving (c).  \end{proof}

As a consequence of Theorem~\ref{T:symmetricmat}, let
${\cal V}=\{v_1,\ldots,v_n\}$ be an orthonormal basis for $\R^n$
consisting of eigenvectors of $A$.  Indeed, suppose
\[
Av_j = \lambda_jv_j
\]
where $\lambda_j\in\R$.  Note that
\[
Av_j\cdot v_i =  \left\{\begin{array}{rl} \lambda_j & \qquad i=j\\
			0 & \qquad i\neq j \end{array}\right.
\]
It follows from \eqref{e:coordorthomat} that
\[
[A]_{\cal V}= \left(\begin{array}{ccc} \lambda_1 & & 0 \\  & \ddots & \\
	0 &  & \lambda_n \end{array}\right)
\]
is a diagonal matrix.  So every symmetric matrix is similar to a diagonal
matrix.  We have in fact proved:


%The previous lemma together with \eqref{e:orthosym} in the proof of Theorem~\ref{T:symmetricmat}(b) lead to the following result:

\begin{proposition}  For each symmetric $n\times n$ matrix $A$, there exists an
orthogonal matrix $P$ such that $P^tAP$ is a diagonal matrix.
\end{proposition}



\EXER

\TEXER

\begin{exercise} \label{c7.7.1}
Let
\[
A = \mattwo{a}{b}{b}{d}
\]
be the general real $2\times 2$ symmetric matrix.
\begin{itemize}
\item[(a)]  Prove directly using the discriminant of the characteristic
polynomial that $A$ has real eigenvalues.
\item[(b)]  Show that $A$ has equal eigenvalues only if $A$ is a scalar
multiple of $I_2$.
\end{itemize}

\begin{solution}

(a) We can calculate the discriminant $D$ of matrix $A$ using
\eqref{e:discriminant}:
\[ D = \trace(A)^2 - 4\det(A) = (a + d)^2 - 4(ad - b^2) =
a^2 + 2ad + b^2 - 4ad + 4b^2 = (a - d)^2 + 4b^2. \]
Therefore, $D \geq 0$ for all real symmetric matrices $A$.  The
eigenvalues of $A$ are
\[ \lambda_1 = \frac{(a + d) + \sqrt{D}}{2} \AND
\lambda_2 = \frac{(a + d) - \sqrt{D}}{2}. \]
Thus, $\lambda_1$ and $\lambda_2$ are real since $D$ is non-negative.

(b) Matrix $A$ has equal eigenvalues only if $D = 0$.  According to
the computation in (a) of this problem, $D = 0$ only if $a = d$ and
$b = 0$.  Therefore, if $\lambda_1 = \lambda_2$, then $A$ is a
multiple of $I_2$.

\end{solution}
\end{exercise}

\begin{exercise} \label{c7.7.2}
Let
\[
A=\mattwo{1}{2}{2}{-2}.
\]
Find the eigenvalues and eigenvectors of $A$ and verify that the eigenvectors
are orthogonal.

\begin{solution}

\ans The eigenvalues of $A$ are $\lambda_1 = 2$ and $\lambda_2 = -3$,
with respective eigenvectors $v_1 = (2,1)$ and $v_2 = (1,-2)$.

\soln Indeed, $v_1 \cdot v_2 = (2,1) \cdot (1,-2) = 0$, so the
eigenvectors are orthogonal.

\end{solution}
\end{exercise}

\noindent In Exercises~\ref{c7.9.1a} -- \ref{c7.9.1e} decide whether or not
the given matrix is orthogonal.
\begin{exercise} \label{c7.9.1a}
$\left(\begin{array}{rr} 2 & 0\\ 0 & 1\end{array}\right)$.

\begin{solution}
\ans The matrix is not orthogonal.

\soln By Lemma~\ref{lem:orthprop}, a matrix
$A$ is orthogonal if and only if $A^tA = I_n$.
\[
\mattwo{2}{0}{0}{1}\mattwo{2}{0}{0}{1} = \mattwo{4}{0}{0}{1} \neq I_2.
\]

\end{solution}
\end{exercise}
\begin{exercise} \label{c7.9.1b}
$\left(\begin{array}{rrr} 0 & 1 & 0\\ 0 & 0 & 1\\
1 & 0 & 0\end{array}\right)$.

\begin{solution}
\ans The matrix is orthogonal, since
\[
\matthree{0}{0}{1}{1}{0}{0}{0}{1}{0}
\matthree{0}{1}{0}{0}{0}{1}{1}{0}{0} =
I_3.
\]

\end{solution}
\end{exercise}
\begin{exercise} \label{c7.9.1c}
$\left(\begin{array}{rrr} 0 & -1 & 0\\ 0 & 0 & 1\\
-1 & 0 & 0\end{array}\right)$.

\begin{solution}
The matrix is orthogonal, since
\[
\matthree{0}{0}{-1}{-1}{0}{0}{0}{1}{0}
\matthree{0}{1}{0}{0}{0}{-1}{-1}{0}{0} =
I_3.
\]

\end{solution}
\end{exercise}
\begin{exercise} \label{c7.9.1d}
$\left(\begin{array}{rr} \cos(1) & -\sin(1)\\ \sin(1) & \cos(1)
\end{array}\right)$.

\begin{solution}
The matrix is orthogonal, since
\[
\mattwo{\cos(1)}{\sin(1)}{-\sin(1)}{\cos(1)}
\mattwo{\cos(1)}{-\sin(1)}{\sin(1)}{\cos(1)}
= I_2.
\]

\end{solution}
\end{exercise}
\begin{exercise} \label{c7.9.1e}
$\left(\begin{array}{rrr} 1 & 0 & 4\\ 0 & 1 & 0
\end{array}\right)$.

\begin{solution}
The matrix is not orthogonal, since all orthogonal matrices
are square.

\end{solution}
\end{exercise}

\begin{exercise} \label{c7.9.2}
Let $Q$ be an orthogonal $n\times n$ matrix.
Show that $Q$ preserves the length of vectors, that is
\[
\|Qv\| = \|v\|\quad \mbox{for all $v\in\R^n$.}
\]

\begin{solution}

For this proof, we use the fact that, if $C$ is a complex matrix, then
$(Cv) \cdot w = v \cdot (\overline{C}^tw)$.  This was shown in the discussion
of Hermitian inner products in Section~\ref{S:symmetric}.  In particular,
since $Q$ is a real matrix, $(Qv) \cdot w = v \cdot (Q^tw)$.  Therefore,
since $Q$ is orthogonal:
\[
||Qv||^2 = (Qv) \cdot (Qv) = (Q^tQv) \cdot v = (I_nv) \cdot v
= v \cdot v = ||v||^2.
\]


\end{solution}
\end{exercise}

\begin{exercise}  \label{c7.9.45}
Prove that the rows of an $n\times n$ orthogonal matrix form an orthonormal
basis for $\R^n$.

\begin{solution}

Let $A$ be an orthogonal matrix.  By
Definition~\ref{def:orthmat}, the columns of $A$ form an orthonormal
basis for $\R^n$.  We must show that the rows of $A$ also form an
orthonormal basis for $\R^n$.  By Lemma~\ref{lem:orthprop}(b), $A^t =
A^{-1}$.  From this, we can show
\[
I_n = AA^{-1} = AA^t = (A^t)^t(A^t).
\]
Thus, by Lemma~\ref{lem:orthprop}(a), $A^t$ is an orthogonal matrix;
so the columns of $A^t$, which are the rows of $A$, form an orthonormal
basis for $\R^n$.

\end{solution}
\end{exercise}


\CEXER

\noindent In Exercises~\ref{exer:powita} -- \ref{exer:powitc}
compute the eigenvalues and the eigenvectors of the 
$2\times 2$ matrix.  Then load the matrix into the program {\sf map}
\index{\computer!map} in \Matlab and iterate.  That is, choose an initial
vector $v_0$ and use {\sf map} to compute $v_1=Av_0$, $v_2=Av_1$, \ldots.
How does the result of iteration compare with the eigenvectors and
eigenvalues that you have found?
{\bf Hint:} You may find it convenient to use the feature {\sf Rescale} in
the {\sf MAP Options}.  Then the norm of the vectors is rescaled to $1$
after each use of the command {\sf Map} and the vectors $v_j$ will not
escape from the viewing screen.
\begin{exercise}  \label{exer:powita}
$A=\mattwo{1}{3}{3}{1}$

\begin{solution}

\ans The eigenvectors of $A$ are $w_1 = (1,1)$ and
$w_2 = (1,-1)$ with eigenvalues $\lambda_1 = 4$ and $\lambda_2 = -2$.  

\soln Note that any matrix of the form $\mattwo{a}{b}{b}{a}$
has eigenvectors $w_1$ and $w_2$ with eigenvalues
$\lambda_1 = a + b$ and $\lambda_2 = a - b$.
By iterating using {\tt map}, we see that $v_j$ approaches a multiple
of $(1,1)$ as $j$ increases for $v_0 \neq (1,-1)$.  If $v_0$ is a
multiple of $(1,-1)$, then $v_j$ is a multiple of $(1,-1)$ for all $j$.

\para In Exercises~\ref{exer:powita} -- \ref{exer:powitc} let $u_1$ be the
eigenvector of matrix associated to the eigenvalue $\lambda_1$ where
$|\lambda_1| > |\lambda_2|$. These exercises demonstrate that $v_j$ 
approaches the direction of $u_1$ as $j$ increases when $v_0$ is
not a scalar multiple of $u_2$.

\end{solution}
\end{exercise}
\begin{exercise}  \label{exer:powitb}
$B=\mattwo{11}{9}{9}{11}$

\begin{solution}

\ans The eigenvectors of $B$ are $w_1 = (1,1)$ and $w_2 = (1,-1)$ with  
eigenvalues $\lambda_1 = 20$ and $\lambda_2 = 2$.  

\soln See solution to Exercise~\ref{exer:powita}.

\end{solution}
\end{exercise}
\begin{exercise}  \label{exer:powitc}
$C=\mattwo{0.005}{-2.005}{-2.005}{0.005}$

\begin{solution}

\ans The eigenvectors of $C$ are $w_1 = (1,1)$ and $w_2 = (1,-1)$ with 
eigenvalues $\lambda_1 = -2$ and $\lambda_2 = 2.01$.  

\soln See comment after the solution to Exercise~\ref{exer:powita}.
By iterating using {\tt map}, we see that $v_j$ approaches a multiple
of $(1,-1)$ as $j$ increases for $v_0 \neq (1,1)$.  If $v_0$ is a
multiple of $(1,1)$, then $v_j$ is a multiple of $(1,1)$ for all $j$.

\end{solution}
\end{exercise}

\begin{exercise} \label{c7.7.3}
Perform the same computational experiment as described in
Exercises~\ref{exer:powita} -- \ref{exer:powitc} using the matrix
$A=\mattwo{0}{2}{2}{0}$
and the program {\sf map}.  How do your results differ from the results
in those exercises and why?

\begin{solution}

In this case, for any vector $v_0 = (x,y)$, the result of the iteration
is $v_1 = Av_0 = 2(y,x)$.  That is, any vector multiplied by $A$ is
reflected across the line $y = x$ and doubled in length.  The result
is different from that in Exercises~\ref{exer:powita} -- \ref{exer:powitc}
because the eigenvalues are $\lambda_1 = 2$ and $\lambda_2 = -2$, so
$|\lambda_1| = |\lambda_2|$.




\end{solution}
\end{exercise}



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
