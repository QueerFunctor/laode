\documentclass{ximera}

\input{../preamble.tex}

\title{Orthonormal Bases}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle


\label{S:orthonormal}

In Section~\ref{S:coordinates} we discussed how to write the coordinates of
a vector in a basis.  We now show that finding coordinates of vectors in
certain bases is a very simple task --- these bases are called orthonormal
bases.

Nonzero vectors $v_1,\ldots,v_k$ in $\R^n$ are
{\em orthogonal\/}\index{orthogonal} if the
dot products\index{dot product}
\[
v_i\cdot v_j  =  0
\]
when $i\neq j$.  The vectors are
{\em orthonormal\/}\index{orthonormal} if they are
orthogonal and of unit length, that is,
\[
v_i\cdot v_i=1.
\]
The standard example of a set of orthonormal vectors in $\R^n$ is the
standard basis $e_1,\ldots,e_n$.

\begin{lemma} \label{L:orthog}
Nonzero orthogonal vectors are
linearly independent\index{linearly!independent}.
\end{lemma}

\begin{proof}  Let $v_1,\ldots,v_k$ be a set of nonzero orthogonal vectors in $\R^n$
and suppose that
\[
\alpha_1v_1 + \cdots + \alpha_kv_k = 0.
\]
To prove the lemma we must show that each $\alpha_j=0$.  Since
$v_i\cdot v_j = 0$ for $i\not= j$,
\begin{align*}
  \alpha_jv_j\cdot v_j &= \alpha_1v_1\cdot v_j + \cdots + \alpha_kv_k\cdot v_j \\
  &= (\alpha_1v_1 + \cdots +\alpha_kv_k)\cdot v_j = 0\cdot v_j = 0.
\end{align*}
Since $v_j\cdot v_j = ||v_j||^2> 0$, it follows that $\alpha_j=0$.  \end{proof}

\begin{corollary}
A set of $n$ nonzero orthogonal vectors in $\R^n$ is a basis.
\end{corollary}

\begin{proof}  Lemma~\ref{L:orthog} implies that the $n$ vectors are linearly
independent, and Chapter~\ref{C:vectorspaces}, Corollary~\ref{C:dim=n} states
that $n$ linearly independent vectors in $\R^n$ form a basis.  \end{proof}

Next we discuss how to find coordinates of a vector in an
{\em orthonormal basis}\index{basis!orthonormal},
that is, a basis consisting of orthonormal vectors.

\begin{theorem}  \label{T:orthocoord}
Let $V\subset\R^n$ be a subspace\index{subspace} and
let $\{v_1,\ldots,v_k\}$ be an
orthonormal basis of $V$.  Let $v\in V$ be a vector.   Then
\[
v = \alpha_1v_1 + \cdots + \alpha_kv_k.
\]
where
\[
\alpha_i = v\cdot v_i.
\]
\end{theorem}

\begin{proof}  Since $\{v_1,\ldots,v_k\}$ is a basis of $V$, we can write
\[
v = \alpha_1v_1 + \cdots + \alpha_kv_k
\]
for some scalars $\alpha_j$.  It follows that
\[
v\cdot v_j = (\alpha_1v_1 + \cdots + \alpha_kv_k)\cdot v_j = \alpha_j,
\]
as claimed.   \end{proof}

\subsubsection{An Example in $\R^3$}

Let
\begin{align*}
  v_1 &= \frac{1}{\sqrt{3}}(1,1,1), \\
  v_2 &= \frac{1}{\sqrt{6}}(1,-2,1), \\
  v_3 &= \frac{1}{\sqrt{2}}(1,0,-1).
\end{align*}
A short calculation verifies that these vectors have
unit length and are pairwise orthogonal.  Let $v=(1,2,3)$ be a vector
and determine the coordinates of $v$ in the basis ${\cal V}=\{v_1,v_2,v_3\}$.
Theorem~\ref{T:orthocoord} states that these coordinates are:
\[
[v]_{\cal V} = (v\cdot v_1, v\cdot v_2, v\cdot v_3)
= (2\sqrt{3},\frac{7}{\sqrt{6}},-\sqrt{2}).
\]


\subsection*{Matrices in Orthonormal Coordinates}

Next we discuss how to find the matrix associated with a linear map in an
orthonormal basis.  Let $L:\R^n\to\R^n$ be a linear map and let
${\cal V} = \{v_1,\ldots,v_n\}$ be an orthonormal basis for $\R^n$.  Then
the matrix associated to $L$ in the basis ${\cal V}$ can be calculated 
in terms of dot product.  That matrix is:
\begin{equation}  \label{e:coordorthomat}
[L]_{\cal V} = L(v_j)\cdot v_i.
\end{equation}
To verify this claim, recall from Definition~\ref{D:matrixincoord} that 
the $(i,j)^{th}$ entry of $[L]_{\cal V}$ is
the $i^{th}$ entry in the vector $[L(v_j)]_{\cal V}$ which is
$L(v_j)\cdot v_i$ by Theorem~\ref{T:orthocoord}.

\subsubsection{An Example in $\R^2$}

Let ${\cal V}=\{v_1,v_2\}\subset\R^2$ where
\[
v_1=\frac{1}{\sqrt{2}}\vectwo{1}{1} \AND
v_2=\frac{1}{\sqrt{2}}\vectwo{1}{-1}.
\]
The set ${\cal V}$ is an orthonormal basis of $\R^2$.  Using
\eqref{e:coordorthomat} we can find the matrix associated to the linear map
\[
L_A(x) = \mattwo{2}{1}{-1}{3}x
\]
in the basis ${\cal V}$.  That is, compute
\[
[L]_{\cal V} =
\mattwo{Av_1\cdot v_1}{Av_2\cdot v_1}{Av_1\cdot v_2}{Av_2\cdot v_2}
=\frac{1}{2}\mattwo{5}{-3}{1}{5}.
\]

\subsection*{Remarks Concerning \Matlab}

In the next section we prove that every vector subspace of $\R^n$ has an
orthonormal basis (see Theorem~\ref{T:orthobasis}), and we present a method
for constructing such a basis (the Gram-Schmidt orthonormalization process).
Here we note that certain commands in \Matlab produce bases for vector spaces.
For those commands \Matlab always produces an orthonormal basis.  For example,
{\tt null(A)}\index{\computer!null} produces a basis for the null space
\index{null space} of $A$.  Take the $3\times 5$ matrix
\begin{matlabEquation}
\label{eq:Anull1}
A = \left(\begin{array}{rrrrr} 1 & 2 & 3 & 4 & 5\\ 0 & 1 & 2 & 3 & 4\\
2 & 3 & 4 & 0 & 0 \end{array}\right).
\end{matlabEquation}
Since $\rank(A)=3$, it follows that the null space of $A$ is two-dimensional.
Typing {\tt B = null(A)} in \Matlab produces
\begin{verbatim}
B =
   -0.4666         0
    0.6945    0.4313
   -0.2876   -0.3235
    0.3581   -0.6470
   -0.2984    0.5392
\end{verbatim}
The columns of $B$ form an orthonormal basis for the null space of $A$.
This assertion can be checked by first typing
\begin{verbatim}
v1 = B(:,1);
v2 = B(:,2);
\end{verbatim}
and then typing
\begin{verbatim}
norm(v1)
norm(v2)
dot(v1,v2)
A*v1
A*v2
\end{verbatim}\index{\computer!norm}
yields answers $1,1,0$, $(0,0,0)^t,(0,0,0)^t$
(to within numerical accuracy).  Recall that the \Matlab
command {\tt norm(v)} computes the norm of a vector {\tt v}.





\EXER

\includeexercises



\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../linearAlgebra"
%%% End:
