\documentclass{ximera}

\input{../preamble.tex}

\title{Row Rank Equals Column Rank}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 \label{S:5.8}

Let $A$ be an $m\times n$ matrix.  The {\em row space\/}
\index{row!space} of $A$ is the span of the row vectors of $A$
and is a subspace of $\R^n$.  The {\em column space\/}
\index{column!space} of $A$ is the span of the columns of $A$
and is a subspace of $\R^m$.
\begin{definition} 
The {\em row rank\/} of $A$ is the dimension of the
row space of $A$ and the {\em column rank\/} of $A$ is the
dimension of the column space of $A$.
\end{definition} \index{row!rank}  \index{column!rank}
Lemma~\ref{L:computerank} of Chapter~\ref{C:vectorspaces} states that
\[
\mbox{row rank}(A) = \rank(A).
\]
We show below that row ranks and column ranks are equal.  We
begin by continuing the discussion of the previous section on linear maps
between vector spaces.

\subsection*{Null Space and Range}

Each linear map between vector spaces defines two subspaces.  Let $V$ and $W$ 
be vector spaces and let $L:V\to W$ be a linear map.  Then
\[
\mbox{null space}(L) = \{v\in V: L(v)=0\} \subset V
\]
\index{null space} and \index{range}
\[
\mbox{range}(L) = \{L(v)\in W: v\in V \} \subset W.
\]

\begin{lemma} \label{L:nsr}
Let $L:V\to W$ be a linear map between vector spaces.  Then the null space of
$L$ is a subspace of $V$ and the range of $L$ is a subspace of $W$.
\end{lemma}\index{subspace}

\begin{proof}  The proof that the null space of $L$ is a subspace of $V$ follows
from linearity in precisely the same way that the null space of an
$m\times n$ matrix is a subspace of $\R^n$.  That is, if $v_1$ and $v_2$ are
in the null space of $L$, then
\[
L(v_1+v_2) = L(v_1) + L(v_2) = 0 + 0 = 0,
\]
and for $c\in\R$
\[
L(cv_1) = cL(v_1) = c0 = 0.
\]
So the null space of $L$ is closed under addition and scalar multiplication
and is a subspace of $V$.

To prove that the range of $L$ is a subspace of $W$, let $w_1$ and $w_2$ be
in the range of $L$.  Then, by definition, there exist $v_1$ and $v_2$ in $V$
such that $L(v_j)=w_j$.  It follows that
\[
L(v_1+v_2) = L(v_1) + L(v_2) = w_1 + w_2.
\]
Therefore, $w_1+w_2$ is in the range of $L$.  Similarly,
\[
L(cv_1) = cL(v_1) = cw_1.
\]
So the range of $L$ is closed under addition and scalar multiplication and is
a subspace of $W$.  \end{proof}

Suppose that $A$ is an $m\times n$ matrix and $L_A:\R^n\to\R^m$ is the
associated linear map.  Then the null space of $L_A$ is precisely the null
space of $A$, as defined in Definition~\ref{D:nullspace} of 
Chapter~\ref{C:vectorspaces}.  Moreover, the range of $L_A$ is the column 
space of $A$.  To verify this, write $A=(A_1|\cdots|A_n)$ where $A_j$ is the 
$j^{th}$ column of $A$ and let $v=(v_1,\ldots v_n)^t$.  Then, $L_A(v)$ is the 
linear combination of columns of $A$
\[
L_A(v)=Av = v_1A_1+\cdots+v_nA_n.
\]

There is a theorem that relates the dimensions of the null space and range
with the dimension of $V$.
\begin{theorem}  \label{T:nsr}
Let $V$ and $W$ be vector spaces with $V$ finite dimensional and let
$L:V\to W$ be a linear map.  Then
\[
\dim(V) = \dim(\mbox{\rm null space}(L)) + \dim({\rm range}(L)).
\]\index{dimension}\index{null space} \index{range}
\end{theorem}

\begin{proof}   Since $V$ is finite dimensional, the null space of $L$ is finite 
dimensional (since the null space is a subspace of $V$) and the range of $L$ 
is finite dimensional (since it is spanned by the vectors $L(v_j)$ where 
$v_1,\ldots,v_n$ is a basis for $V$).  Let $u_1,\ldots,u_k$ be a basis for 
the null space of $L$ and let $w_1,\ldots,w_\ell$ be a basis for the range of
$L$.   Choose vectors $y_j\in V$ such that $L(y_j)=w_j$.  We claim that
$u_1,\ldots,u_k,y_1,\ldots,y_\ell$ is a basis for $V$, which proves the
theorem.

To verify that $u_1,\ldots,u_k,y_1,\ldots,y_\ell$ are linear independent,
suppose that
\begin{equation}  \label{E:uy}
\alpha_1u_1+\cdots+\alpha_ku_k+\beta_1y_1+\cdots+\beta_\ell y_\ell = 0.
\end{equation}
Apply $L$ to both sides of \Ref{E:uy} to obtain
\[
\beta_1w_1+\cdots+\beta_\ell w_\ell = 0.
\]
Since the $w_j$ are linearly independent, it follows that $\beta_j=0$ for all
$j$.  Now  \Ref{E:uy} implies that
\[
\alpha_1u_1+\cdots+\alpha_ku_k = 0.
\]
Since the $u_j$ are linearly independent, it follows that $\alpha_j=0$ for
all $j$.

To verify that $u_1,\ldots,u_k,y_1,\ldots,y_\ell$ span $V$, let $v$ be in
$V$.  Since $w_1,\ldots,w_\ell$ span $W$, it follows that there exist scalars
$\beta_j$ such that
\[
L(v) = \beta_1w_1+\cdots+\beta_\ell w_\ell.
\]
Note that by choice of the $y_j$
\[
L(\beta_1y_1+\cdots+\beta_\ell y_\ell) = \beta_1w_1+\cdots+\beta_\ell w_\ell.
\]
It follows by linearity that
\[
u = v - (\beta_1y_1+\cdots+\beta_\ell y_\ell)
\]
is in the null space of $L$.  Hence there exist scalars $\alpha_j$ such that
\[
u = \alpha_1u_1+\cdots+\alpha_ku_k.
\]
Thus, $v$ is in the span of $u_1,\ldots,u_k,y_1,\ldots,y_\ell$, as desired.
\end{proof}

\subsection*{Row Rank and Column Rank}

Recall Theorem~\ref{T:dimsoln} of Chapter~\ref{C:vectorspaces} that states
that the nullity plus the rank of an $m\times n$ matrix equals $n$.  At first 
glance it might seem that this theorem and Theorem~\ref{T:nsr} contain the 
same information, but they do not.  Theorem~\ref{T:dimsoln} of 
Chapter~\ref{C:vectorspaces} is proved using a detailed analysis of solutions 
of linear equations based on Gaussian elimination, back substitution, and 
reduced echelon form, while Theorem~\ref{T:nsr} is proved using abstract 
properties of linear maps.

Let $A$ be an $m\times n$ matrix.  Theorem~\ref{T:dimsoln} of
Chapter~\ref{C:vectorspaces} states that 
\[
{\rm nullity}(A) + \rank(A) = n.
\]
Meanwhile, Theorem~\ref{T:nsr} states that 
\[
\dim(\mbox{\rm null space}(L_A)) + \dim({\rm range}(L_A)) = n.
\]
But the dimension of the null space of $L_A$ equals the nullity of $A$ 
and the dimension of the range of $A$ equals the dimension of the column 
space of $A$.  Therefore, 
\[
{\rm nullity}(A) + \dim(\mbox{column space}(A)) = n.
\]
Hence, the rank of $A$ equals the column rank of $A$.  Since rank and row rank 
are identical, we have proved:
\begin{theorem} \label{T:rowrank=columnrank}
Let $A$ be an $m\times n$ matrix.  Then
\[
\mbox{row rank } A=\mbox{column rank } A.
\]
\end{theorem}\index{row!rank}\index{column!rank}

Since the row rank of $A$ equals the column rank of $A^t$, we have:
\begin{corollary}
Let $A$ be an $m\times n$ matrix.  Then
\[
\rank(A) = \rank(A^t).
\]
\end{corollary}\index{matrix!transpose}


\EXER

\TEXER

\begin{exercise} \label{c5.8.1}
The $3\times 3$ matrix
\[
A = \left(\begin{array}{rrr} 1 & 2 & 5\\ 2 & -1 & 1\\ 3 & 1 & 6
\end{array}\right)
\]
has rank two.  Let $r_1,r_2,r_3$ be the rows of $A$ and
$c_1,c_2,c_3$ be the columns of $A$. Find scalars $\alpha_j$ and
$\beta_j$ such that
\begin{eqnarray*}
\alpha_1r_1+\alpha_2r_2+\alpha_3r_3 & = & 0 \\
\beta_1c_1+\beta_2c_2+\beta_3c_3 & = & 0.
\end{eqnarray*}
\end{exercise}

\begin{exercise} \label{c5.8.2}
What is the largest row rank that a $5\times 3$ matrix can have?
\end{exercise}

\begin{exercise} \label{c5.8.3}
Let
\[
A = \left(\begin{array}{rrrr} 1 & 1 & 0 & 1\\ 0 & -1 & 1 & 2\\
1 & 2 & -1 & 3 \end{array}\right).
\]
\begin{itemize}
\item[(a)]  Find a basis for the row space of $A$ and the row rank of $A$.
\item[(b)]  Find a basis for the column space of $A$ and the column rank of
$A$.
\item[(c)]  Find a basis for the null space of $A$ and the nullity of $A$.
\item[(d)]  Find a basis for the null space of $A^t$ and the nullity of $A^t$.
\end{itemize}
\end{exercise}

\begin{exercise} \label{c5.8.4}
Let $A$ be a nonzero $3\times 3$ matrix such that $A^2=0$.  Show that
$\rank(A)=1$.
\end{exercise}

\begin{exercise} \label{c5.8.5}
Let $B$ be an $m\times p$ matrix and let $C$ be a $p\times n$
matrix. Prove that the rank of the $m\times n$ matrix $A=BC$
satisfies
\[
\rank(A) \leq \min\{\rank(B),\;\rank(C)\}.
\]
\end{exercise}



\CEXER

\begin{exercise} \label{c5.8.6}
Let
\begin{equation*}
A = \left(\begin{array}{rrrr} 1 & 1 & 2 & 2 \\ 0 & -1 & 3 & 1 \\
   2 & -1 & 1 & 0 \\ -1 & 0 & 7 & 4 \end{array}\right).
\end{equation*}
\begin{itemize}
\item[(a)]  Compute $\rank(A)$ and exhibit a basis for the row space of $A$.
\item[(b)]  Find a basis for the column space of $A$.
\item[(c)]  Find all solutions to the homogeneous equation $Ax=0$.
\item[(d)]  Does
\[
Ax = \left( \begin{array}{c} 4 \\ 2\\ 2\\ 1 \end{array} \right)
\]
have a solution?
\end{itemize}
\end{exercise}


\end{document}
