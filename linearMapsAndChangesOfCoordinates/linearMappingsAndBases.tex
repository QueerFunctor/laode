\documentclass{ximera}

\input{../preamble.tex}

\title{Linear Mappings and Bases}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

\label{Sect:linmap}

The examples of linear mappings
\index{linear!mapping} from $\R^n\to\R^m$ that we introduced in
Section~\ref{S:linearity} were matrix mappings.  More precisely,
let $A$ be an $m\times n$ matrix.  Then
\[
L_A(x)=Ax
\]
defines the linear mapping $L_A:\R^n\to\R^m$.  Recall that $Ae_j$
is the $j^{th}$ column of $A$ (see Chapter~\ref{chap:matrices},
Lemma~\ref{columnsA}); it follows that $A$ can be
reconstructed from the vectors $Ae_1,\ldots,Ae_n$.  This remark
implies (Chapter~\ref{chap:matrices}, Lemma~\ref{linequal}) that
linear mappings of $\R^n$ to $\R^m$ are determined by their
values on the standard basis $e_1, \ldots, e_n$.  Next we show
that this result is valid in greater generality.  We begin by
defining what we mean for a mapping between vector spaces to be
linear.

\begin{definition}  \label{D:linearV}
Let $V$ and $W$ be vector spaces and let $L:V\to W$ be a mapping.  The map
$L$ is {\em linear\/} if
\begin{eqnarray*}
L(u+v) & = & L(u) + L(v) \\
L(cv) & = & cL(v)
\end{eqnarray*}
for all $u,v\in V$ and $c\in\R$.
\end{definition} \index{linear}\index{vector!space}

\subsubsection*{Examples of Linear Mappings}

\noindent (a) Let $v\in\R^n$ be a fixed vector.  Use the
dot product\index{dot product} to define the mapping
$L:\R^n\to\R$ by
\[
L(x)= x\cdot v.
\]
Then $L$ is linear.  Just check that
\[
L(x+y) = (x+y)\cdot v = x\cdot v + y\cdot v = L(x) + L(y)
\]
for every vector $x$ and $y$ in $\R^n$ and
\[
L(cx) = (cx)\cdot v = c(x\cdot v) = cL(x)
\]
for every scalar $c\in\R$.

\noindent (b) The map $L:\CCone\to\R$ defined by
\[
L(f) = f'(2)
\]
is linear.  Indeed,
\[
L(f+g) = (f+g)'(2) = f'(2) + g'(2) = L(f) + L(g).
\]
Similarly, $L(cf)=cL(f)$.

\noindent (c) The map $L:\CCone\to\CCone$ defined by
\[
L(f)(t)=f(t-1)
\]
is linear.  Indeed,
\[
L(f+g)(t) = (f+g)(t-1) = f(t-1) + g(t-1) = L(f)(t) + L(g)(t).
\]
Similarly, $L(cf)=cL(f)$.  It may be helpful to compute $L(f)(t)$ when
$f(t)=t^2-t+1$.  That is,
\[
L(f)(t) = (t-1)^2-(t-1)+1 = t^2-2t+1-t+1+1 = t^2-3t+3.
\]


\subsubsection*{Constructing Linear Mappings from Bases}

\begin{theorem} \label{L:linmapfrombasis}
Let $V$ and $W$ be vector spaces.  Let $\{v_1,\ldots,v_n\}$ be a
basis for $V$ and let $\{w_1,\ldots,w_n\}$ be $n$ vectors in $W$.
Then there exists a unique linear map $L:V\to W$ such that $L(v_i)=w_i$.
\end{theorem}\index{linear!mapping!construction}

\begin{proof} Let $v\in V$ be a vector.  Since $\Span\{v_1,\ldots,v_n\}=V$, we may
write $v$ as
\[
v = \alpha_1v_1 + \cdots + \alpha_nv_n,
\]
where $\alpha_1,\ldots,\alpha_n$ in $\R$.   Moreover, $v_1,\ldots,v_n$
are linearly independent, these scalars are uniquely defined.  More
precisely, if
\[
\alpha_1v_1 + \cdots + \alpha_nv_n = \beta_1v_1 + \cdots + \beta_nv_n,
\]
then
\[
(\alpha_1-\beta_1)v_1 + \cdots + (\alpha_n-\beta_n)v_n = 0.
\]
Linear independence implies that $\alpha_j-\beta_j=0$; that is
$\alpha_j=\beta_j$.   We can now define
\begin{equation}  \label{e:v-coord}
L(v) = \alpha_1 w_1+\cdots+\alpha_n w_n.
\end{equation}

We claim that $L$ is linear.  Let $\hat{v}\in V$ be another
vector and let
\[
\hat{v} = \beta_1v_1+\cdots+\beta_nv_n.
\]
It follows that
\[
v+\hat{v} = (\alpha_1+\beta_1)v_1+\cdots+(\alpha_n+\beta_n)v_n,
\]
and hence by \eqref{e:v-coord} that
\begin{eqnarray*}
L(v+\hat{v}) & = &
(\alpha_1+\beta_1)w_1+\cdots+(\alpha_n+\beta_n)w_n\\
& = & (\alpha_1w_1+\cdots+\alpha_nw_n) +
(\beta_1w_1+\cdots+\beta_nw_n)  \\
& = & L(v) + L(\hat{v}).
\end{eqnarray*}

Similarly
\begin{eqnarray*}
L(cv) &  = & L( (c\alpha_1)v_1+\cdots +(c\alpha_n)v_n)\\
& = & c(\alpha_1w_1+\cdots+\alpha_nw_n)\\
& = & cL(v).
\end{eqnarray*}
Thus $L$ is linear.

Let $M:V\to W$ be another linear mapping such that $M(v_i)=w_i$.
Then
\begin{eqnarray*}
L(v) & = & L(\alpha_1v_1+\ldots +\alpha_nv_n)\\
& = & \alpha_1w_1+\cdots+\alpha_nw_n \\
& = & \alpha_1M(v_1) + \cdots +\alpha_nM(v_n)\\
& = & M(\alpha_1v_1 + \cdots +\alpha_nv_n)\\
& = & M(v).
\end{eqnarray*}
Thus $L=M$ and the linear mapping is uniquely defined.  \end{proof}


There are two assertions made in Theorem~\ref{L:linmapfrombasis}.
The first is that a linear map exists mapping $v_i$ to $w_i$.
The second is that there is only one {\em linear\/} mapping
that accomplishes this task.  If we drop the constraint that the
map be linear, then many mappings may satisfy these conditions.
For example, find a linear map from $\R\to\R$ that maps $1$ to $4$.
There is only one: $y=4x$.  However there are many nonlinear maps
that send $1$ to $4$.  Examples are $y=x+3$ and $y=4x^2$.

\subsubsection*{Finding the Matrix of a Linear Map from $\R^n\to\R^m$
Given by Theorem~\protect{\ref{L:linmapfrombasis}}}
\index{linear!mapping!matrix}

Suppose that $V=\R^n$ and $W=\R^m$.  We know that every linear
map $L:\R^n\to\R^m$ can be defined as multiplication by an
$m\times n$ matrix.  The question that we next address is:
How can we find the matrix whose existence is guaranteed by
Theorem~\ref{L:linmapfrombasis}?

More precisely, let $v_1,\ldots,v_n$ be a basis for $\R^n$ and
let $w_1,\ldots,w_n$ be vectors in $\R^m$.  We suppose that all
of these vectors are row vectors.  Then we need to find
an $m\times n$ matrix $A$ such that $Av_i^t=w_i^t$ for all $i$.
We find $A$ as follows.  Let $v\in\R^n$ be a row vector.  Since
the $v_i$ form a basis, there exist scalars $\alpha_i$ such that
\[
v=\alpha_1 v_1 + \cdots + \alpha_n v_n.
\]
In coordinates
\begin{equation}  \label{e:v^t}
v^t = (v_1^t|\cdots|v_n^t)\vect{\alpha}{n},
\end{equation}
where $(v_1^t|\cdots|v_n^t)$ is an $n\times n$
invertible matrix\index{matrix!invertible}.
By definition (see \eqref{e:v-coord})
\[
L(v) = \alpha_1 w_1 + \cdots + \alpha_n w_n.
\]
Thus the matrix $A$ must satisfy
\[
Av^t = (w_1^t|\cdots|w_n^t)\vect{\alpha}{n},
\]
where $(w_1^t|\cdots|w_n^t)$ is an $m\times n$ matrix.
Using \eqref{e:v^t} we see that
\[
Av^t = (w_1^t|\cdots|w_n^t)(v_1^t|\cdots|v_n^t)\inv v^t,
\]
and
\begin{equation}  \label{e:defA}
A = (w_1^t|\cdots|w_n^t)(v_1^t|\cdots|v_n^t)\inv
\end{equation}
is the desired $m\times n$ matrix.

\subsubsection*{An Example of a Linear Map from $\R^3$ to $\R^2$}

As an example we illustrate Theorem~\ref{L:linmapfrombasis} and
\eqref{e:defA} by defining a linear mapping from $\R^3$ to $\R^2$
by its action on a basis.  Let
\[
v_1=(1,4,1)\quad v_2=(-1,1,1) \quad v_3=(0,1,0).
\]
We claim that $\{v_1,v_2,v_3\}$ is a basis of $\R^3$ and that
there is a unique linear map for which $L(v_i)=w_i$ where
\[
w_1=(2,0) \quad w_2=(1,1) \quad w_3=(1,-1).
\]

We can verify that $\{v_1,v_2,v_3\}$ is a basis of $\R^3$ by
showing that the matrix
\[
(v_1^t|v_2^t|v_3^t) = \left(\begin{array}{rrr}
1 & -1 & 0  \\
4 & 1 & 1  \\
1 & 1 & 0  \end{array}\right)
\]
is invertible.  This can either be done in \Matlab using the
{\tt inv} command or by hand by row reducing the matrix

\[
\left(\begin{array}{rrr|ccc}
1 & -1 & 0 & 1 & 0 & 0 \\
4 &  1 & 1 & 0 & 1 & 0 \\
1 &  1 & 0 & 0 & 0 & 1  \end{array}\right)
\]
to obtain
\[
(v_1^t|v_2^t|v_3^t)\inv = \frac{1}{2}\left(\begin{array}{rrr}
 1 & 0 &  1 \\
-1 & 0 &  1 \\
-3 & 2 & -5
\end{array}\right).
\]
Now apply \eqref{e:defA} to obtain
\[
A = \frac{1}{2} \left(\begin{array}{rrr} 2 & 1 & 1\\ 0 & 1 & -1
\end{array}\right) \left(\begin{array}{rrr}
 1 & 0 &  1 \\
-1 & 0 &  1 \\
-3 & 2 & -5
\end{array}\right) = \left(\begin{array}{rrr} -1 & 1 & -1 \\ 1 & -1 & 3
\end{array}\right).
\]
As a check, verify by matrix multiplication that $Av_i=w_i$, as claimed.


\subsection*{Properties of Linear Mappings}

\begin{lemma} \label{L:compose}
Let $U,V,W$ be vector spaces and $L:V\to W$ and $M:U\to V$ be linear maps.
Then $L\compose M :U\to W$ is linear.
\end{lemma}\index{composition!of linear mappings}

\begin{proof} The proof of Lemma~\ref{L:compose} is identical to that of
Chapter~\ref{chap:matrices}, Lemma~\ref{complin}. \end{proof}

A linear map $L:V\to W$ is {\em invertible\/} \index{invertible} if there
exists a linear map $M:W\to V$ such that $L\compose M:W\to W$ is the identity
map on $W$ and $M\compose L:V\to V$ is the identity map on $V$.

\begin{theorem} \label{T:invertbasis}
Let $V$ and $W$ be finite dimensional vector spaces and let $v_1,\ldots,v_n$
be a basis\index{basis} for $V$.  Let $L:V\to W$ be a linear map.
Then $L$ is invertible
if and only if $w_1,\ldots,w_n$ is a basis for $W$ where $w_j=L(v_j)$.
\end{theorem}

\begin{proof}  If $w_1,\ldots,w_n$ is a basis for $W$, then use
Theorem~\ref{L:linmapfrombasis} to define a linear map $M:W\to V$ by
$M(w_j)=v_j$.  Note that
\[
L\compose M(w_j)= L(v_j) =w_j.
\]
It follows by linearity (using the uniqueness part of
Theorem~\ref{L:linmapfrombasis}) that $L\compose M$ is the identity of $W$.
Similarly, $M\compose L$ is the identity map on $V$, and $L$ is invertible.

Conversely, suppose that $L\compose M$ and $M\compose L$ are identity maps
and that $w_j=L(v_j)$.  We must show that $w_1,\ldots,w_n$ is a basis.  We
use Theorem~\ref{basis=span+indep} and verify separately that
$w_1,\ldots,w_n$ are linearly independent and span $W$.

If there exist scalars $\alpha_1,\ldots,\alpha_n$ such that
\[
\alpha_1w_1+\cdots +\alpha_nw_n = 0,
\]
then apply $M$ to both sides of this equation to obtain
\[
0=M(\alpha_1w_1+\cdots +\alpha_nw_n)=\alpha_1v_1+\cdots+\alpha_nv_n.
\]
But the $v_j$ are linearly independent.  Therefore, $\alpha_j=0$ and the
$w_j$ are linearly independent.

To show that the $w_j$ span $W$, let $w$ be a vector in $W$.  Since the $v_j$
are a basis for $V$, there exist scalars $\beta_1,\ldots,\beta_n$ such that
\[
M(w) = \beta_1v_1+\cdots+\beta_nv_n.
\]
Applying $L$ to both sides of this equation yields
\[
w = L\compose M(w) = \beta_1w_1+\cdots+\beta_nw_n.
\]
Therefore, the $w_j$ span $W$.  \end{proof}

\begin{corollary}
Let $V$ and $W$ be finite dimensional vector spaces.  Then there exists
an invertible\index{invertible} linear map $L:V\to W$
if and only if $\dim(V)=\dim(W)$.
\end{corollary}

\begin{proof}  Suppose that $L:V\to W$ is an invertible linear map.  Let
$v_1,\ldots,v_n$ be a basis for $V$ where $n=\dim(V)$.  Then
Theorem~\ref{T:invertbasis} implies that $L(v_1),\ldots,L(v_n)$ is a basis
for $W$ and $\dim(W)=n=\dim(V)$.

Conversely, suppose that $\dim(V)=\dim(W)=n$.  Let $v_1,\ldots,v_n$ be a
basis for $V$ and let $w_1,\ldots,w_n$ be a basis for $W$.  Using
Theorem~\ref{L:linmapfrombasis} define the linear map $L:V\to W$ by
$L(v_j)=w_j$.  Theorem~\ref{T:invertbasis} states that $L$ is invertible. \end{proof}

\EXER

\TEXER

\begin{exercise} \label{c7.2.1}
Use Theorem~\ref{L:linmapfrombasis} and \eqref{e:defA} to 
construct matrix of a linear mapping $L$ from $\R^3$ to $\R^2$ with $L(v_i)=w_i$, $i=1,2,3$, where
\[
v_1=(1,0,2)\quad v_2=(2,-1,1) \quad v_3=(-2,1,0)
\]
and
\[
w_1=(-1,0) \quad w_2=(0,1) \quad w_3=(3,1).
\]

\begin{solution}

Compute $A$, the matrix of $L$, using Equation~\eqref{e:defA}:
\[ A = (w_1^t|w_2^t|w_3^t)(v_1^t|v_2^t|v_3^t)^{-1} =
\left(\begin{array}{rrr} -1 & 0 & 3 \\ 0 & 1 & 1 \end{array}\right)
\matthree{1}{2}{-2}{0}{-1}{1}{2}{1}{0}^{-1} =
\left(\begin{array}{rrr} -7 & -11 & 3 \\ -4 & -7 & 2
\end{array}\right). \]

\end{solution}
\end{exercise}

\begin{exercise}  \label{c7.2.2}
Let ${\cal P}_n$ be the vector space of polynomials $p(t)$ of
degree less than or equal to $n$.  Show that $\{1,t,t^2,\ldots,t^n\}$ is a
basis for ${\cal P}_n$.

\begin{solution}

To show that the set $\{1,t,t^2,\dots,t^n\}$ is a basis for
${\cal P}_n$, we must show that the $n + 1$ polynomials are
linearly independent and span ${\cal P}_n$.  The polynomials are
independent because the general polynomial of degree $n$:
\[
\alpha_1 + \alpha_2t + \alpha_3t^2 + \cdots + \alpha_{n+1}t^n
\]
is identically $0$ for all values of $t$ only when $\alpha_1 =
\alpha_2 = \cdots = \alpha_{n + 1} = 0$.  The polynomials span
${\cal P}_n$ because every polynomial $p(t)$ of degree $n$ has
the form
\[ p(t) = \beta_1 + \beta_2t + \cdots + \beta_{n + 1}t^n \]
which is a linear combination of the polynomials
$\{1,t,t^2,\dots,t^n\}$ for any $p(t)$ in ${\cal P}_n$.

\end{solution}
\end{exercise}

\begin{exercise}  \label{c7.2.2a}
Show that
\[
\frac{d}{dt}:{\cal P}_3\to{\cal P}_2
\]
is a linear mapping.

\begin{solution}

Let $\frac{d}{dt}$ be a transformation that maps $p(t) \mapsto
\frac{d}{dt}p(t)$.  For $p(t) =  p_1 + p_2t + p_3t^2 + p_4t^3$, then
$\frac{d}{dt}p(t) = p_2 + p_3t + p_4t^2$, so $\frac{d}{dt}$ is indeed
a mapping ${\cal P}_3 \rightarrow {\cal P}_2$.  From calculus, we
know that, for any functions $f$ and $g$:
\[ \frac{d}{dt}(f + g)(t) = \frac{d}{dt}f(t) + \frac{d}{dt}g(t), \]
and that, for any scalar $c$:
\[ \frac{d}{dt}(cf)(t) = c\frac{d}{dt}f(t). \]
Let $f$ and $g$ be elements of ${\cal P}_3$.  Then
$\frac{d}{dt}: {\cal P}_3 \rightarrow {\cal P}_2$ is a linear mapping.

\end{solution}
\end{exercise}
\begin{exercise}  \label{c7.2.2b}
Show that
\[
L(p) = \int_0^tp(s)ds
\]
is a linear mapping of ${\cal P}_2\to{\cal P}_3$.

\begin{solution}

Let $p(t) = p_1 + p_2t + p_3t^2$.  Then the transformation $L$
maps $p(t) \mapsto L(p(t)) = p_1t + \frac{1}{2}p_2t^2 +
\frac{1}{3}p_3t^3$, so $L$ is indeed a mapping ${\cal P}_2
\rightarrow {\cal P}_3$.  We know from calculus that, for any
functions $f$ and $g$:
\[ \int_0^t(f + g)(t) = \int_0^tf(t) + \int_0^tg(t) \]
And, for any scalar $c \in \R$,
\[ \int_0^t(cf)(t) = c\int_0^tf(t). \]
Let $f$ and $g$ be elements of ${\cal P}_2$.  Then $L$ is a linear
mapping.

\end{solution}
\end{exercise}
\begin{exercise}  \label{c7.2.2c}
Use Exercises~\ref{c7.2.2a}, \ref{c7.2.2b} and
Theorem~\ref{L:linmapfrombasis} to show that
\[
\frac{d}{dt}\compose L:{\cal P}_2\to{\cal P}_2
\]
is the identity map.

\begin{solution}

Let $M = \frac{d}{dt} \circ L$ be a mapping ${\cal P}_2
\rightarrow {\cal P}_2$.  The fundamental
theorem of calculus states that, for any function $g$,
\[ \frac{d}{dt}\int_0^t g(\tau)d\tau = g(t). \]
Thus $M(g) = g$ is valid for all values of $g$, so $M$ is the
identity map.

\para To prove this fact explicitly for this case, note that $M$ is
the identity mapping if it maps every polynomial in ${\cal P}_2$ to
itself.  Lemma~\ref{L:linmapfrombasis} states that this mapping can
be uniquely determined by a basis of ${\cal P}_2$.  According to
Exercise~\ref{c7.2.2}, $\{1,t,t^2\}$ is a basis for ${\cal P}_2$. 
Therefore, $M$ is the identity map if $M(1) = 1$, $M(t) = t$ and
$M(t^2) = t^2$:
\[ \frac{d}{dt} \circ L (1) = \frac{d}{dt}\left(\int_0^tds\right) =
\frac{d}{dt}(t) = 1. \]
\[ \frac{d}{dt} \circ L (t) = \frac{d}{dt}\left(\int_0^tsds\right) =
\frac{d}{dt}\left(\frac{t^2}{2}\right) = t. \]
\[ \frac{d}{dt} \circ L (t^2) = \frac{d}{dt}\left(\int_0^ts^2ds\right) =
\frac{d}{dt}\left(\frac{t^3}{3}\right) = t^2. \]
So $\frac{d}{dt} \circ L$ is indeed the identity map for ${\cal P}_2$.

\end{solution}
\end{exercise}

\begin{exercise} \label{c7.2.3}
Let $\C$ denote the set of complex numbers.  Verify that
$\C$ is a two-dimensional vector space.  Show that $L:\C\to\C$
defined by
\[
L(z) = \lambda z,
\]
where $\lambda=\sigma+i\tau$ is a linear mapping.

\begin{solution}

The space $\C$ is a two dimensional real vector space since every
element of $\C$ can be written as $c = \sigma + \tau i$, a linear
combination of the linearly independent two dimensional set $\{1,i\}$.

\para Let $z_1$ and $z_2$ be elements of $\R$.  Then,
\[ L(z_1 + z_2) = \lambda(z_1 + z_2) =
\lambda z_1 + \lambda z_2 = L(z_1) + L(z_2). \]
For any real scalar $c$,
\[ L(cz_1) = \lambda(cz_1) = 
c\lambda z_1 = cL(z_1). \]
Therefore, $L(z) = \lambda z$ is a linear mapping.


\end{solution}
\end{exercise}

\begin{exercise} \label{c7.2.4}
Let ${\cal M}(n)$ denote the vector space of $n\times n$
matrices and let $A$ be an $n\times n$ matrix.  Let
$L:{\cal M}(n)\to{\cal M}(n)$ be the mapping defined by
$L(X)=AX-XA$ where $X\in{\cal M}(n)$.  Verify that $L$ is
a linear mapping.  Show that the null space of $L$,
$\{X\in{\cal M}:L(X)=0\}$, is a subspace consisting of all
matrices that commute with $A$.

\begin{solution}

Let $X$ and $Y$ be elements of ${\cal M}(n)$.  Then,
\[ L(X + Y) = A(X + Y) - (X + Y)A = (AX - XA)
+ (AY - YA) = L(X) + L(Y). \]
For any real scalar $c$,
\[ L(cX) = A(cX) - (cX)A = c(AX) - c(XA) = c(AX - XA) = cL(X). \]
Therefore, $L$ is a linear mapping.

\para The null space of $L$ consists of all matrices $X$ such that
$AX - XA = 0$, or $AX = XA$.  By definition, these are the matrices
such that $X$ commutes with $A$.  Let $X$ and $Y$ be elements of
the null space of $L$.  Then show that $X + Y$ is in the null space
by calculating:
\[ L(X + Y) = A(X + Y) - (X + Y)A = AX + AY - XA - YA = AX + AY - AX
- AY = 0. \]
Show that, for any real scalar $c$, $cX$ is in the null space by
calculating:
\[ L(cX) = A(cX) - (cX)A = cAX - cXA = cAX - cAX = 0. \]
Therefore, the null space of $L$ is a subspace consisting of all
matrices that commute with $A$.

\end{solution}
\end{exercise}

\begin{exercise} \label{c7.2.5}
Let $L:\CCone\to\R$ be defined by $L(f) = \int_0^{2\pi}f(t)\cos(t)dt$
for $f\in\CCone$.  Verify that $L$ is a linear mapping.

\begin{solution}

Let $f$ and $g$ be functions in ${\cal C}^1$.  Then,
\[ \begin{array}{rcl}
 L(f + g) & = & \int_0^{2\pi}(f(t) + g(t))\cos(t)dt \\
& = & \int_0^{2\pi}(f(t)\cos(t)dt + g(t)\cos(t)dt) \\
& = & \int_0^{2\pi}f(t)\cos(t)dt + \int_0^{2\pi}g(t)\cos(t)dt \\
& = & L(f) + L(g). \end{array} \]
For any real scalar $c$,
\[ L(cf) = \int_0^{2\pi}cf(t)\cos(t)dt = c\int_0^{2\pi}f(t)\cos(t)dt
= cL(f). \]
So $L$ is a linear mapping.

\end{solution}
\end{exercise}

\begin{exercise} \label{c7.2.6}
Let ${\cal P}$ be the vector space of polynomials in one variable
$x$.  Define $L:{\cal P}\to {\cal P}$ by $L(p)(x)=\int_0^x(t-1)p(t)dt$.
Verify that $L$ is a linear mapping.

\begin{solution}

Let $p$ and $q$ be elements of ${\cal P}$.  Then,
\[ \begin{array}{rcl}
L(p + q)(x) & = & \int_0^x(t - 1)(p(t) + q(t))dt \\
& = & \int_0^x((t - 1)p(t)dt + (t - 1)q(t)dt) \\
& = & \int_0^x(t - 1)p(t)dt + \int_0^x(t - 1)q(t)dt \\
& = & L(p(x)) + L(q(x)). \end{array} \]
For any real scalar $c$,
\[ L(cp(x)) = \int_0^x(t - 1)cp(t)dt = c\int_0^x(t - 1)p(t)dt
= cL(p(x)). \]
Therefore, $L$ is a linear mapping.





\end{solution}
\end{exercise}


\end{document}
