\documentclass{ximera}

\input{../preamble.tex}

\title{Construction of Subspaces}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 \label{S:5.2}

The principle of superposition shows that the set of all
solutions to a homogeneous system of linear equations is closed under 
addition and scalar multiplication and is a
subspace.  Indeed, there are two ways to describe subspaces:
first as solutions to linear systems, and second as the span of
a set of vectors.  We shall see that solving a homogeneous linear system of
equations just means writing the solution set as the span of a
finite set of vectors.

\subsection*{Solutions to Homogeneous Systems Form Subspaces}
\index{homogeneous} \index{subspace}

\begin{definition} \label{D:nullspace}
Let $A$ be an $m\times n$ matrix.  The {\em null space\/} of $A$
is the set of solutions to the homogeneous system of linear equations
\begin{equation} \label{Ax=0}
Ax=0.
\end{equation}
\end{definition} \index{null space}

\begin{lemma}
Let $A$ be an $m\times n$ matrix.  Then the null space of $A$
is a subspace of $\R^n$.
\end{lemma}

\begin{proof}
Suppose that $x$ and $y$ are solutions to \eqref{Ax=0}.  Then
\[
A(x+y) = Ax+Ay = 0+0 = 0;
\]
so $x+y$ is a solution of \eqref{Ax=0}.  Similarly, for $r\in\R$
\[
A(rx) = rAx = r0 = 0;
\]
so $rx$ is a solution of \eqref{Ax=0}.  Thus, $x+y$ and $rx$ are
in the null space of $A$, and the null space is closed under addition 
and scalar multiplication.  So Theorem~\ref{T:subspaces} implies that
the null space is a subspace of the vector space $\R^n$.   \end{proof}

\subsubsection*{Solutions to Linear Systems of Differential Equations Form
Subspaces}

Let $C$ be an $n\times n$ matrix and let $W$ be the set of solutions to
the linear system of ordinary differential equations
\begin{equation} \label{Cx(t)}
\frac{dx}{dt}(t) = Cx(t).
\end{equation}
We will see later that a solution to \eqref{Cx(t)} has coordinate
functions $x_j(t)$ in $\CCone$.  The principle of superposition
then shows that $W$ is a subspace of $(\CCone)^n$.  Suppose
$x(t)$ and $y(t)$ are solutions of \eqref{Cx(t)}.  Then
\[
\frac{d}{dt}(x(t)+y(t)) = \frac{dx}{dt}(t) + \frac{dy}{dt}(t) =
Cx(t) + Cy(t) = C(x(t)+y(t));
\]
so $x(t)+y(t)$ is a solution of \eqref{Cx(t)} and in $W$.  A
similar calculation shows that $rx(t)$ is also in $W$ and
that $W\subset(\CCone)^n$ is a subspace.


\subsection*{Writing Solution Subspaces as a Span}
\index{homogeneous}\index{span}

The way we solve homogeneous systems of equations gives a second
method for defining subspaces.  For example, consider the system
\[
Ax=0,
\]
where
\[
A=\left(\begin{array}{rccc} 2 & 1 & 4 & 0 \\ -1 & 0 & 2 & 1
        \end{array}\right).
\]
The matrix $A$ is row equivalent to the reduced echelon form matrix
\[
E=\left(\begin{array}{ccrr} 1 & 0 & -2 & -1 \\ 0 & 1 & 8 & 2
        \end{array}\right).
\]
Therefore $x=(x_1,x_2,x_3,x_4)$ is a solution of $Ex=0$ if and
only if $x_1 = 2x_3+x_4$ and $x_2 = -8x_3 - 2x_4$.
It follows that every solution of $Ex=0$ can be written as:
\[
x = x_3\left(\begin{array}{r} 2 \\ -8\\ 1\\ 0 \end{array}\right)
+x_4\left(\begin{array}{r} 1 \\ -2\\ 0\\ 1 \end{array}\right).
\]
Since row operations do not change the set of solutions, it
follows that every solution of $Ax=0$ has this form. We have
also shown that every solution is generated by two vectors by
use of vector addition\index{vector!addition} and
scalar multiplication\index{scalar multiplication}.  We say that
this subspace is {\em spanned\/} by the two vectors
\[
\left(\begin{array}{r} 2 \\ -8\\ 1\\ 0 \end{array}\right)
\AND
\left(\begin{array}{r} 1 \\ -2\\ 0\\ 1 \end{array}\right).
\]
For example, a calculation verifies that the vector
\[
\left(\begin{array}{r} -1 \\ -2\\ 1\\ -3 \end{array}\right)
\]
is also a solution of $Ax=0$.  Indeed, we may write it as
\begin{equation}
\label{eq:SpanSol}
\left(\begin{array}{r} -1 \\ -2\\ 1\\ -3 \end{array}\right)
=
\left(\begin{array}{r} 2 \\ -8\\ 1\\ 0 \end{array}\right) -
3 \left(\begin{array}{r} 1 \\ -2\\ 0\\ 1 \end{array}\right).
\end{equation}


\subsection*{Spans}

Let $v_1,\ldots,v_k$ be a set of vectors in a vector space $V$.  A vector
$v\in V$ is a {\em linear combination\/} of $v_1,\ldots,v_k$
if
\[
v = r_1v_1 + \cdots + r_kv_k
\]
for some scalars $r_1,\ldots,r_k$.  \index{linear!combination}


\begin{definition}  \label{span}
The set of all linear combinations of the vectors $v_1,\ldots,v_k$
in a vector space $V$ is the {\em span\/} of $v_1,\ldots,v_k$ and is
denoted by $\Span\{v_1,\ldots,v_k\}$.
\end{definition} \index{span}

For example, the vector on the left hand side in \eqref{eq:SpanSol}
is a linear combination of the two vectors on the right hand side.

The simplest example of a span is $\R^n$ itself.  Let $v_j=e_j$
where $e_j\in\R^n$ is the vector with a $1$ in the $j^{th}$
coordinate and $0$ in all other coordinates.  Then every vector
$x=(x_1,\ldots,x_n)\in\R^n$ can be written as
\[
x = x_1e_1 + \cdots + x_ne_n.
\]
It follows that
\[
\R^n=\Span\{e_1,\ldots,e_n\}.
\]
Similarly, the set $\Span\{e_1,e_3\}\subset\R^3$ is just the
$x_1x_3$-plane, since vectors in this span are
\[
x_1e_1+x_3e_3 = x_1(1,0,0) + x_3(0,0,1) = (x_1,0,x_3).
\]

\begin{proposition} \label{spansubspace} Let $V$ be a vector space and
let $w_1,\ldots,w_k\in V$. Then $W=\Span\{w_1,\ldots,w_k\}
\subset V$ is a subspace.
\end{proposition} \index{span} \index{subspace}\index{vector!space}

\begin{proof}  Suppose $x,y\in W$.  Then
\begin{eqnarray*}
x & = & r_1w_1 + \cdots + r_kw_k \\
y & = & s_1w_1 + \cdots + s_kw_k
\end{eqnarray*}
for some scalars $r_1,\ldots,r_k$ and $s_1,\ldots,s_k$.  It
follows that
\[
x+y = (r_1+s_1)w_1 + \cdots + (r_k+s_k)w_k
\]
and
\[
rx = (rr_1)w_1 + \cdots + (rr_k)w_k
\]
are both in $\Span\{w_1,\ldots,w_k\}$. Hence $W\subset V$ is
closed under addition and scalar multiplication, and is a
subspace by Theorem~\ref{T:subspaces}. \end{proof}

For example, let
\begin{equation}  \label{e:vandw}
v=(2,1,0) \AND w=(1,1,1)
\end{equation}
be vectors in $\R^3$. Then linear combinations of the vectors
$v$ and $w$ have the form
\[
\alpha v + \beta w = (2\alpha+\beta, \alpha+\beta, \beta)
\]
for real numbers $\alpha$ and $\beta$.  Note that every one of
these vectors is a solution to the linear equation
\begin{equation} \label{ex1}
x_1 - 2x_2 + x_3 = 0,
\end{equation}
that is, the $1^{st}$ coordinate minus twice the $2^{nd}$ coordinate 
plus the $3^{rd}$ coordinate equals zero.  Moreover, you may verify 
that every solution of \eqref{ex1} is a linear combination
of the vectors $v$ and $w$ in \eqref{e:vandw}.  Thus, the set of
solutions to the homogeneous\index{homogeneous} linear equation
\eqref{ex1} is a
subspace, and that subspace can be written as the span of
all linear combinations of the vectors $v$ and $w$.

In this language we see that the process of solving a
homogeneous system of linear equations is just the process of
finding a set of vectors that span the subspace of all
solutions.  Indeed,
we can now restate Theorem~\ref{number} of Chapter~\ref{lineq}.
Recall that a matrix $A$ has {\em rank\/} $\ell$ if it is row
equivalent to a matrix in echelon form with $\ell$ nonzero rows.
\index{rank}

\begin{proposition}  \label{P:n-rank}
Let $A$ be an $m\times n$ matrix with rank $\ell$. Then the
null space of $A$ is the span of $n-\ell$ vectors.
\end{proposition} \index{null space} \index{span}

We have now seen that there are two ways to describe subspaces ---
as solutions of homogeneous systems of linear equations and as a
span of a set of vectors, the {\em spanning set}\index{spanning set}.
Much of linear algebra is concerned
with determining how one goes from one description of a subspace
to the other.

\EXER

\includeexercises


\end{document}
