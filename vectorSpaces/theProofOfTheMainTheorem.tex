\documentclass{ximera}

\input{../preamble.tex}

\title{The Proof of the Main Theorem}

\begin{document}
\begin{abstract}
\end{abstract}
\maketitle

 \label{S:5.6}

We begin the proof of Theorem~\ref{basis=span+indep} with two
lemmas on linearly independent and spanning sets.

\begin{lemma}  \label{reducetoindep}
Let $\{w_1,\ldots,w_k\}$ be a set of vectors in a vector space
$V$ and let $W$ be the subspace spanned by these vectors.  Then
there is a linearly independent subset of $\{w_1,\ldots,w_k\}$
that also spans $W$.
\end{lemma}\index{linearly!independent}

\begin{proof} If $\{w_1,\ldots,w_k\}$ is linearly independent, then the
lemma is proved.  If not, then the set $\{w_1,\ldots,w_k\}$ is
linearly dependent.  If this set is linearly dependent, then at
least one of the vectors is a linear combination of the others.
By renumbering if necessary, we can assume that $w_k$ is a
linear combination of $w_1,\ldots,w_{k-1}$; that is,
\[
w_k = a_1w_1 + \cdots + a_{k-1}w_{k-1}.
\]
Now suppose that $w\in W$.  Then
\[
w = b_1w_1 + \cdots + b_kw_k.
\]
It follows that
\[
w = (b_1+b_ka_1)w_1 + \cdots + (b_{k-1}+b_ka_{k-1})w_{k-1},
\]
and that $W=\Span\{w_1,\ldots,w_{k-1}\}$.  If the vectors
$w_1,\ldots,w_{k-1}$ are linearly independent, then the proof of
the lemma is complete.  If not, continue inductively until a
linearly independent subset of the $w_j$ that also spans $W$ is
found.  \end{proof}

The important point in proving that linear independence together
with spanning imply that we have a basis is discussed in the next
lemma.

\begin{lemma}  \label{lem:lindep}
Let $W$ be an $m$-dimensional vector space and let $k>m$ be an integer.
Then any set of $k$ vectors in $W$ is linearly dependent.
\end{lemma} \index{linearly!dependent}

\begin{proof} Since the dimension of $W$ is $m$ we know that
this vector space can be written as $W=\Span\{v_1,\ldots,v_m\}$.
Moreover, Lemma~\ref{reducetoindep} implies that the vectors
$v_1,\ldots,v_m$ are linearly independent.  Suppose that
$\{w_1,\ldots,w_k\}$ is another set of vectors where $k>m$.
We have to show that the vectors $w_1,\ldots,w_k$ are linearly
dependent; that is, we must show that there exist scalars
$r_1,\ldots,r_k$ not all of which are zero that satisfy
\begin{equation} \label{independence1}
r_1w_1 + \cdots + r_kw_k = 0.
\end{equation}
We find these scalars by solving a system of linear equations, as
we now show.

The fact that $W$ is spanned by the vectors $v_j$ implies that
\begin{eqnarray*}
w_1 & = & a_{11}v_1 + \cdots + a_{m1}v_m\ \\
w_2 & = & a_{12}v_1 + \cdots + a_{m2}v_m\ \\
 & \vdots & \\\
w_k & = & a_{1k}v_1 + \cdots + a_{mk}v_m.
\end{eqnarray*}
It follows that $r_1w_1 + \cdots + r_kw_k$ equals
\[
\begin{array}{ll}
r_1(a_{11}v_1 + \cdots + a_{m1}v_m) & +  \\
r_2(a_{12}v_1 + \cdots + a_{m2}v_m) & + \cdots + \\
r_k(a_{1k}v_1 + \cdots + a_{mk}v_m) &
\end{array}
\]
Rearranging terms leads to the expression:
\begin{equation}   \label{e:r1v1etc}
\begin{array}{ll}
(a_{11}r_1 + \cdots + a_{1k}r_k)v_1\ & + \\
(a_{21}r_1 + \cdots + a_{2k}r_k)v_2\ & + \cdots + \\
(a_{m1}r_1 + \cdots + a_{mk}r_k)v_m. &
\end{array}
\end{equation}
Thus, \eqref{independence1} is valid if and only if \eqref{e:r1v1etc}
sums to zero.  Since the set $\{v_1,\ldots,v_m\}$ is linearly
independent, \eqref{e:r1v1etc} can equal zero if and only if
\begin{eqnarray*}
a_{11}r_1 + \cdots + a_{1k}r_k & = & 0\ \\
a_{21}r_1 + \cdots + a_{2k}r_k & = & 0\ \\
          & \vdots &   \\
a_{m1}r_1 + \cdots + a_{mk}r_k & = & 0.
\end{eqnarray*}
Since $m<k$, Chapter~\ref{lineq}, Theorem~\ref{number} implies that
this system of homogeneous linear equations always has a nonzero
solution $r=(r_1,\ldots,r_k)$ --- from which it follows that the
$w_i$ are linearly dependent.  \end{proof}

\begin{corollary}  \label{basis<n}
Let $V$ be a vector space of dimension $n$ and let $\{u_1,\ldots,u_k\}$ be a 
linearly independent set of vectors in $V$.  Then $k\le n$.
\end{corollary} \index{linearly!independent}
\index{dimension}\index{vector!space}

\begin{proof} If $k>n$ then Lemma~\ref{lem:lindep} implies that
$\{u_1,\ldots,u_k\}$ is linearly dependent.  Since we have
assumed that this set is linearly independent, it follows that
$k\leq n$. \end{proof}

\vspace{0.1in}

\begin{proof}[Theorem~\ref{basis=span+indep}]
Suppose that ${\cal B}=\{w_1,\ldots,w_k\}$ is a basis for $W$.
By definition, ${\cal B}$ spans $W$ and $k=\dim W$.  We must
show that ${\cal B}$ is linearly independent.  Suppose ${\cal B}$
is linearly dependent, then Lemma~\ref{reducetoindep} implies
that there is a proper subset of ${\cal B}$ that spans $W$ (and
is linearly independent). This contradicts the fact that as a
basis ${\cal B}$ has the smallest number of elements of any
spanning set for $W$.

Suppose that ${\cal B}=\{w_1,\ldots,w_k\}$ both spans $W$ and is
linearly independent.  Linear independence and Corollary~\ref{basis<n}
imply that $k\leq\dim W$.  Since, by definition, any spanning set
of $W$ has at least $\dim W$ vectors, it follows that $k\geq\dim W$.
Thus, $k = \dim W$ and  ${\cal B}$ is a basis.   \end{proof}

\subsection*{Extending Linearly Independent Sets to Bases} \index{basis}

Lemma~\ref{reducetoindep} leads to one approach to finding
bases.  Suppose that the subspace $W$ is spanned by a finite
set of vectors $\{w_1,\ldots,w_k\}$.  Then, we can throw out
vectors one by one until we arrive at a linearly independent
subset of the $w_j$.  This subset is a basis for $W$.

We now discuss a second approach to finding a basis for a
nonzero subspace $W$ of a finite dimensional vector space $V$.

\begin{lemma}  \label{extendindep}
Let $\{u_1,\ldots,u_k\}$ be a linearly independent set of
vectors in a vector space $V$ and assume that
\[
u_{k+1}\not\in\Span\{u_1,\ldots,u_k\}.
\]
Then $\{u_1,\ldots,u_{k+1}\}$ is also a linearly independent
set.
\end{lemma} \index{linearly!independent}

\begin{proof}  Let $r_1,\ldots,r_{k+1}$ be scalars such that
\begin{equation}  \label{rk+1}
r_1u_1 + \cdots + r_{k+1}u_{k+1} = 0.
\end{equation}
To prove independence, we need to show that all $r_j=0$.
Suppose $r_{k+1}\neq 0$.  Then we can solve \eqref{rk+1} for
\[
u_{k+1} = -\frac{1}{r_{k+1}}(r_1u_1+\cdots +r_ku_k),
\]
which implies that $u_{k+1}\in \Span\{u_1,\ldots,u_k\}$.  This
contradicts the choice of $u_{k+1}$.  So $r_{k+1}=0$ and
\[
r_1u_1 + \cdots + r_ku_k = 0.
\]
Since $\{u_1,\ldots,u_k\}$ is linearly independent, it follows
that $r_1=\cdots =r_k=0$.  \end{proof}

The second method for constructing a basis is:
\index{basis!construction}
\begin{itemize}
\item        Choose a nonzero vector $w_1$ in $W$.
\item If $W$ is not spanned by $w_1$, then choose a vector $w_2$
that is not on the line spanned by $w_1$.
\item        If $W\neq\Span\{w_1,w_2\}$, then choose a vector
$w_3\not\in
\Span\{w_1,w_2\}$.
\item        If $W\neq\Span\{w_1,w_2,w_3\}$, then choose a vector
$w_4\not\in
\Span\{w_1,w_2,w_3\}$.
\item Continue until a spanning set\index{spanning set} for $W$ is
found.  This set is a basis for $W$.
\end{itemize}

We now justify this approach to finding bases for subspaces.
Suppose that $W$ is a subspace of a finite dimensional vector
space $V$.  For example, suppose that $W\subset\R^n$. Then
our approach to finding a basis of $W$ is as follows.  Choose a
nonzero vector $w_1\in W$.  If $W=\Span\{w_1\}$, then we are
done.  If not, choose a vector $w_2\in W\setmin\Span\{w_1\}$.
It follows from Lemma~\ref{extendindep} that $\{w_1,w_2\}$ is
linearly independent.  If $W=\Span\{w_1,w_2\}$, then
Theorem~\ref{basis=span+indep} implies that $\{w_1,w_2\}$ is
a basis for $W$, $\dim W=2$, and we are done.  If not, choose
$w_3\in W\setmin\Span\{w_1,w_2\}$ and $\{w_1,w_2,w_3\}$ is
linearly independent.  The finite dimension of $V$ implies that
continuing inductively must lead to a spanning set of linear
independent vectors for $W$ --- which by
Theorem~\ref{basis=span+indep} is a basis. This discussion proves:
\begin{corollary}  \label{c:extendindependent}
Every linearly independent subset of a finite dimensional vector
space $V$ can be extended to a basis of $V$.
\end{corollary}


\subsection*{Further consequences of Theorem~\ref{basis=span+indep}}

We summarize here several important facts about dimensions.

\begin{corollary}  \label{dimensiondecreases}
Let $W$ be a subspace of a finite dimensional vector space $V$.
\begin{itemize}
\item[(a)]   Suppose that $W$ is a proper subspace\index{subspace!proper}.
Then $\dim W < \dim V$\index{dimension}.
\item[(b)]   Suppose that $\dim W = \dim V$.  Then $W=V$.
\end{itemize}
\end{corollary}

\begin{proof}
(a) Let $\dim W = k$ and let $\{w_1,\ldots,w_k\}$ be a basis for
$W$.  Since $W$ is a proper subspace of $V$, there is a vector
$w\in V\setmin W$.  It follows from Lemma~\ref{extendindep} that
$\{w_1,\ldots,w_k,w\}$ is a linearly independent set.  Therefore,
Corollary~\ref{basis<n} implies that $k+1\le n$.

(b) Let $\{w_1,\ldots,w_k\}$ be a basis for $W$.
Theorem~\ref{basis=span+indep} implies that this set is linearly
independent.  If $\{w_1,\ldots,w_k\}$ does not span $V$, then it
can be extended to a basis as above.  But then $\dim V > \dim W$,
which is a contradiction.  \end{proof}

\begin{corollary} \label{C:dim=n}
Let ${\cal B}=\{w_1,\ldots,w_n\}$ be a set of $n$ vectors in an
$n$-dimensional vector space $V$.  Then the following are equivalent:
\begin{itemize}
\item[(a)]  ${\cal B}$ is a spanning set of $V$, \index{span}
\item[(b)]  ${\cal B}$ is a basis for $V$, and \index{basis}
\item[(c)] ${\cal B}$ is a linearly independent set.
\index{linearly!independent}
\end{itemize}
\end{corollary}

\begin{proof}    By definition, (a) implies (b) since a basis is a spanning
set with the number of vectors equal to the dimension of the space.
Theorem~\ref{basis=span+indep} states that a basis is a linearly
independent set; so (b) implies (c). If ${\cal B}$ is a linearly
independent set of $n$ vectors, then it spans a subspace $W$ of
dimension $n$.  It follows from Corollary~\ref{dimensiondecreases}(b)
that $W=V$ and that (c) implies (a).  \end{proof}


\subsubsection*{Subspaces of $\R^3$}
\index{$\R^3$!subspaces}

We can now classify all subspaces of $\R^3$.  They are:  the origin, lines
through the origin, planes through the origin, and $\R^3$.  All of these
sets were shown to be subspaces in Example~\ref{EX:subspaces}(a--c).

To verify that these sets are the only subspaces of $\R^3$, note that
Theorem~\ref{basis=span+indep} implies that proper subspaces of $\R^3$ have
dimension equal either to one or two. (The zero dimensional subspace is the
origin and the only three dimensional subspace is $\R^3$ itself.)  One
dimensional subspaces of $\R^3$ are spanned by one nonzero vector and are just
lines through the origin.  See Example~\ref{EX:subspaces}(b).  We claim that
all two dimensional subspaces are planes through the origin.

Suppose that $W\subset\R^3$ is a subspace spanned by two non-collinear vectors
$w_1$ and $w_2$.\index{collinear}  We show that $W$ is a plane \index{plane}
through the origin using results in Chapter~\ref{lineq}.  Observe that there
is a vector $N=(N_1,N_2,N_3)$ perpendicular to $w_1=(a_{11},a_{12},a_{13})$
and $w_2=(a_{21},a_{22},a_{23})$.  Such a vector $N$ satisfies the two linear
equations:
\begin{eqnarray*}
w_1\cdot N & = & a_{11}N_1 + a_{12}N_2 + a_{13}N_3 = 0 \\
w_2\cdot N & = & a_{21}N_1 + a_{22}N_2 + a_{23}N_3 = 0.
\end{eqnarray*}
Chapter~\ref{lineq}, Theorem~\ref{number} implies that a system of two linear
equations in three unknowns has a nonzero solution.  Let $P$ be the plane
perpendicular \index{perpendicular} to $N$ that contains the origin.  We show
that $W=P$ and hence that the claim is valid.

The choice of $N$ shows that the vectors $w_1$ and $w_2$ are both in $P$. In
fact, since $P$ is a subspace it contains every vector in $\Span\{w_1,w_2\}$.
Thus $W\subset P$.  If $P$ contains just one additional vector $w_3\in\R^3$
that is not in $W$, then the span of $w_1,w_2,w_3$ is three dimensional and
$P=W=\R^3$.



\EXER

\TEXER



\noindent In Exercises~\ref{c5.7.1a} -- \ref{c5.7.1c} you are
given a pair of vectors $v_1,v_2$ spanning a subspace of $\R^3$.
Decide whether that subspace is a line or a plane through the
origin.  If it is a plane, then compute a vector $N$ that is
perpendicular to that plane.
\begin{exercise} \label{c5.7.1a}
$v_1=(2,1,2) \AND v_2=(0,-1,1)$.

\begin{solution}

\ans The span of $v_1$ and $v_2$ is a plane with normal vector
$N = n_3(-\frac{3}{2}, 1, 1)$, where $n_3$ is a nonzero scalar.

\soln If $v_1$ and $v_2$ are linearly independent, then they span a plane
in $\R^3$.  If they are linearly dependent, that is, if $v_1 =
\alpha v_2$ for some scalar $\alpha$, then they span a line in $\R^3$.
In this case, there is no scalar $\alpha$ such that $(2,1,2) =
\alpha(0,-1,1)$, so the span of $v_1$ and $v_2$ has dimension two.
The vector $N = (n_1,n_2,n_3)$ is found by observing that:
\[
\begin{array}{rrrrrcl}
2n_1 & + & n_2 & + & 2n_3 & = & 0 \\
& & -n_2 & + & n_3 & = & 0 \end{array}
\]
which is a linear system in two equations.  Solve for $N$ by row
reducing the corresponding matrix:
\[
\left(\begin{array}{rrr} 2 & 1 & 2 \\ 0 & -1 & 1 \end{array}\right)
\longrightarrow \left(\begin{array}{rrr} 1 & 0 & \frac{3}{2} \\ 0 &
1 & -1 \end{array}\right).
\]

\end{solution}
\end{exercise}

\begin{exercise} \label{c5.7.1b}
$v_1=(2,1,-1) \AND v_2=(-4,-2,2)$.

\begin{solution}
\ans The subspace spanned by $v_1$ and $v_2$ is a line,
since, if $\alpha = -\frac{1}{2}$, then $(2,1,-1) = \alpha(-4,-2,2)$.

\end{solution}
\end{exercise}
\begin{exercise} \label{c5.7.1c}
$v_1=(0,1,0) \AND v_2=(4,1,0)$.

\begin{solution}
\ans The span of $v_1$ and $v_2$ is a plane with normal
vector $N = n_3(0,0,1)$, where $n_3$ is a nonzero scalar.

\soln There is no scalar $\alpha$ such that $(0,1,0) = \alpha(4,1,0)$. 
Let $N = (n_1,n_2,n_3)$ be the vector perpendicular to the plane.  Then:
\[
\begin{array}{rrrrrcl}
& & n_2 & & & = & 0 \\
4n_1 & + & n_2 & & & = & 0 \end{array}
\]
Solve for $N$ by substitution to find that $n_1 = n_2 = 0$, and
$n_3$ can be any nonzero real scalar.

\end{solution}
\end{exercise}

\begin{exercise} \label{c5.7.2}
The pairs of vectors
\[
     v_1=(-1,1,0) \AND v_2=(1,0,1)
\]
span a plane $P$ in $\R^3$.  The pairs of vectors
\[
        w_1=(0,1,0) \AND w_2=(1,1,0)
\]
span a plane $Q$ in $\R^3$.  Show that $P$ and $Q$
are different and compute the subspace of $\R^3$ that
is given by the intersection $P\cap Q$.

\begin{solution}

\ans The intersection of the planes is $P \cap Q = s(1,-1,0)$ for any
real scalar $s$.

\soln The planes $P$ and $Q$ are not equal if the normal vectors $P_N$
and $Q_N$ point in different directions.  Solving by row reduction
yields $P_N = (-1,-1,1)$ and $Q_N = (0,0,1)$, so $P \neq Q$.

\para Since $P$ and $Q$ are not the same plane and also are not
parallel, they intersect in a line.  The intersection $P \cap Q$ is
the simultaneous solutions to the equations for planes $P$ and $Q$,
that is:
\[ \begin{array}{rrrrrrl}
-x & - & y & + & z & = & 0 \\
& & & & z & = & 0. \end{array} \]
Solve by row reduction or substitution to obtain $x = -y$ and $z = 0$.

\end{solution}
\end{exercise}

\begin{exercise} \label{c5.6.1}
Let $A$ be a $7\times 5$ matrix with $\rank(A)=r$.\index{rank}
\begin{itemize}
\item[(a)]	What is the largest value that $r$ can have?
\item[(b)]	Give a condition equivalent to the system of
	equations $Ax=b$ having a solution.
\item[(c)]	What is the dimension of the null space of $A$?
\item[(d)]	If there is a solution to $Ax=b$, then how many
parameters are needed to describe the set of all solutions?
\end{itemize}

\begin{solution}

(a) The largest value that $r$ can have is $5$, since the matrix has
$5$ columns.  Thus, the reduced echelon form matrix can have at most
$5$ pivot points.

(b) The equation $Ax = b$ has a solution if the rank of the augmented
matrix $(A|b)$ is $r$.  If $\rank (A|b)$ is greater than $r$, then
there is a pivot in the $6^{th}$ column and the system is
inconsistent, so there is no solution.

(c) The null space has dimension $5 - r$.

(d) The number of parameters needed to describe the solution to
$Ax = b$ is $5 - r$, since $5 - r$ parameters are needed to describe
the solutions to $Ax = 0$, and the solutions to the inhomogeneous
system are obtained by adding the solutions of the homogeneous system
to one solution of the inhomogeneous system.


\end{solution}
\end{exercise}

\begin{exercise} \label{c5.6.2}
Let
\[
A=\left(\begin{array}{rrrr} 1 & 3 & -1 & 4\\ 2& 1 & 5 & 7\\ 3 & 4 & 4 & 11
\end{array}\right).
\]
\begin{itemize}
\item[(a)] Find a basis for the subspace ${\cal C}\subset\R^3$ spanned by the
columns of $A$.
\item[(b)] Find a basis for the subspace ${\cal R}\subset\R^4$ spanned by the
rows of $A$.
\item[(c)] What is the relationship between $\dim {\cal C}$ and
$\dim {\cal R}$?
\end{itemize}

\begin{solution}

\ans (a) The vectors $(1,2,3)$ and $(3,1,4)$ form a basis for the subspace
	$\CC$ of $\R^3$ spanned by the columns of $A$.

(b) The vectors $(1,3,-1,4)$ and $(2,1,5,7)$ form a basis for the subspace
	$\R$ of $\R^4$ spanned by the rowss of $A$.

(c) $\dim \CC = \dim \R$.

\soln (a) Note that 
\begin{eqnarray*} 
\vecthree{-1}{5}{4} = \frac{16}{5}\vecthree{1}{2}{3} 
	- \frac{7}{5}\vecthree{3}{1}{4} \\ 
\vecthree{4}{7}{11} = \frac{17}{5}\vecthree{1}{2}{3} 
	+ \frac{1}{5}\vecthree{3}{1}{4}
\end{eqnarray*}
So the two vectors $(1, 2, 3)$ and $(3,1,4)$ span $\CC$.  Since they are 
linearly independent, these vectors are a basis for $\CC$ and $\dim =\CC=2$.

(b) Note that 
\[
(3,4,4,11) = (1,3,-1,4) + (2,1,5,7).
\]
Therefore, $\{(1,3,-1,4),(2,1,5,7)\}$ is a basis for $\mathcal{R}$ and 
$\dim \mathcal{R}=2$


\end{solution}
\end{exercise}


\begin{exercise} \label{c5.6.3}
Show that the vectors
\[
v_1=(2,3,1) \AND v_2=(1,1,3)
\]
are linearly independent.   Show that the span of $v_1$ and $v_2$
forms a plane in $\R^3$ by showing that every linear combination
 is the solution to a single linear equation.  Use this equation
to determine the normal vector $N$ to this plane.  Verify
Lemma~\ref{extendindep} by verifying directly that $v_1,v_2,N$
are linearly independent vectors.

\begin{solution}

There is no scalar $\alpha$ such that $(2,3,1) = \alpha(1,1,3)$, so
$v_1$ and $v_2$ are linearly independent.
The set of linear combinations of $v_1$ and $v_2$ is the set of
solutions to
\[
\begin{array}{rrrrrrl}
2a_1 & + & 3a_1 & + & a_3 & = & 0 \\
a_1 & + & a_2 & + & 3a_3 & = & 0. \end{array}
\]
Row reducing this system yields $a_1 = -8a_3$ and $a_2 = 5a_3$.
Let $a_3 = 0$.  Then every linear combination of $v_1$ and $v_2$ is
of the form
\[
8x_1 - 5x_2 - x_3 = 0
\]
which is the equation of a plane in $\R^3$.  The normal vector to this
plane is $N = (8,-5,-1)$.  Row reduce the matrix whose columns
are the vectors $v_1$, $v_2$, and $N$ to verify that these vectors
are linearly independent.
\[
\matthree{2}{1}{8}{3}{1}{-5}{1}{3}{-1} \longrightarrow
\matthree{1}{0}{0}{0}{1}{0}{0}{0}{1}.
\]
So the vectors are indeed linearly independent, verifying
Lemma~\ref{extendindep}.

\end{solution}
\end{exercise}

\begin{exercise}  \label{c5.6.3A}
Let $W$ be an infinite dimensional subspace of the vector space $V$.
Show that $V$ is infinite dimensional.

\begin{solution}
Let $W$ be an infinite dimensional subspace of the vector 
space $V$.  We want to show that $V$ is infinite dimensional.  Suppose that
$V$ is finite dimensional with $\dim V=n$.  Then Corollary~\ref{basis<n}
states that any set of linear independent 
vectors in $V$ has at most $n$ vectors.  Since $W$ is infinite dimensional, 
there exists a linearly independent set of vectors in $W\subset V$ with more 
than $n$ vectors.  This is a contradiction and $V$ must be infinite 
dimensional.

\end{solution}
\end{exercise}


\CEXER

\begin{exercise} \label{c5.6.4}
Consider the following set of vectors
\begin{align*}
w_1 &= (2, -2, 1), \\
w_2 &= (-1, 2, 0), \\
w_3 &= (3, -2, \lambda), \\
w_4 &= (-5, 6, -2),
\end{align*}
where $\lambda$ is a real number.
\begin{itemize}
\item[(a)] Find a value for $\lambda$ such that the
dimension of $\Span\{w_1,w_2,w_3,w_4\}$ is three. Then decide
whether $\{w_1,w_2,w_3\}$ or $\{w_1,w_2,w_4\}$ is a basis for
$\R^3$.
\item[(b)] Find a value for $\lambda$ such that the
dimension of $\Span\{w_1,w_2,w_3,w_4\}$ is two.
\end{itemize}

\begin{solution}

(a) \ans The span has dimension $3$ for $\lambda \neq 2$, and 
the set $\{w_1,w_2,w_3\}$ is a basis for $\R^3$.

\soln Find the dimension of the span by creating a matrix with rows
$w_1$, $w_2$, $w_3$, and $w_4$, then row reducing:
\begin{equation} \label{exeq:5.6.4}
\left(\begin{array}{rrr} 2 & -2 & 1 \\ -1 & 2 & 0 \\ 3 & -2 &
\lambda \\ -5 & 6 & -2 \end{array}\right) \longrightarrow
\left(\begin{array}{rrc} 1 & -1 & \frac{1}{2} \\ 0 & 1 & \frac{1}{2}
\\ 0 & 0 & \lambda - 2 \\ 0 & 0 & 0 \end{array}\right).
\end{equation}
If $\lambda = 2$, then the dimension of the span will
be $2$ and if $\lambda \neq 2$, then the dimension of the span
will be $3$.  For example, let $\lambda = -1$.

\para Verify by row reduction that the set $\{w_1,w_2,w_3\}$ is a basis
for $\R^3$ and that the set $\{w_1,w_2,w_4\}$ is not a basis for $R^3$. 

(b) If $\lambda = 2$, then the dimension of span$\{w_1,w_2,w_3,w_4\}$
is $2$, as shown by equation~\eqref{exeq:5.6.4}.



\end{solution}
\end{exercise}

\begin{exercise} \label{c5.6.5}
Find a basis for $\R^5$ as follows.  Randomly choose vectors $x_1,x_2\in\R^5$
by typing {\tt x1 = rand(5,1)} and {\tt x2 = rand(5,1)}.  Check that these
vectors are linearly independent.  If not, choose another pair of vectors
until you find a linearly independent set.  Next choose a vector $x_3$ at
random and check that $x_1,x_2,x_3$ are linearly independent.  If not,
randomly choose another vector for $x_3$. Continue until you have five
linearly independent vectors --- which by a dimension count must be a
basis and span $\R^5$.  Verify this comment by using \Matlab
to write the vector
\[
\left(\begin{array}{r} 2 \\ 1 \\ 3\\ -2 \\ 4 \end{array}\right)
\]
as a linear combination of $x_1,\ldots,x_5$.

\begin{solution}

Here is a sample \Matlab output for this problem.  Type:
\begin{verbatim}
x1 = rand(5,1);
x2 = rand(5,1);
x3 = rand(5,1);
x4 = rand(5,1);
x5 = rand(5,1);
\end{verbatim}
A summary of the results is:
\begin{verbatim}
x1 =          x2 =          x3 =          x4 =          x5 =
    0.9501        0.7621        0.6154        0.4057        0.0579
    0.2311        0.4565        0.7919        0.9355        0.3529
    0.6068        0.0185        0.9218        0.9169        0.8132
    0.4860        0.8214        0.7382        0.4103        0.0099
    0.8913        0.4447        0.1763        0.8936        0.1389
\end{verbatim}
The command {\tt A = [x1 x2 x3 x4 x5]} creates the matrix with columns
$x_1,...,x_5$.  Type {\tt rref(A)} to verify that the vectors are
linearly independent.  The following steps display the vector
$b = (2,1,3,-2,4)$ as a linear combination of $x_1,...,x_5$.  Type:
\begin{verbatim}
b = [2;1;3;-2;4];
C = [A b];
rref(C)
\end{verbatim}
which yields:

\begin{verbatim}
ans =
    1.0000         0         0         0         0   28.7614
         0    1.0000         0         0         0  -96.6468
         0         0    1.0000         0         0   70.9112
         0         0         0    1.0000         0   30.0838
         0         0         0         0    1.0000 -129.8826
\end{verbatim}
So, in this example, $b = 28.7614x_1 - 96.6468x_2 + 70.9112x_3
+ 30.0838x_4 - 129.8826x_5$.

\end{solution}
\end{exercise}

\begin{exercise} \label{c5.6.6}
Find a basis for the subspace of $\R^5$ spanned by
\begin{matlabEquation}\label{MATLAB:63}
\begin{array}{rcl}
u_1 & = & (1,1,0,0,1) \\
u_2 & = & (0,2,0,1,-1)  \\
u_3 & = & (0,-1,1,0,2)   \\
u_4 & = & (1,4,1,2,1)  \\
u_5 & = & (0,0,2,1,3).
\end{array}
\end{matlabEquation}

\begin{solution}

\ans The vectors $(1,0,0,-\frac{1}{2},\frac{3}{2})$, $(0,1,0,\frac{1}{2},
-\frac{1}{2})$, and $(0,0,1,\frac{1}{2},\frac{3}{2})$ form a basis
for the subspace spanned by $u_1, \dots ,u_5$.

\soln Row reduce the matrix {\tt M}, whose
rows are $u_1$, $u_2$, $u_3$, $u_4$ and $u_5$.  According to 
Lemma~\ref{extendindep}, the rows of the
reduced echelon matrix form a basis for $\{u_1,\dots ,u_5\}$.  The
command {\tt rref(M)} yields:
\begin{verbatim}
ans =
    1.0000         0         0   -0.5000    1.5000
         0    1.0000         0    0.5000   -0.5000
         0         0    1.0000    0.5000    1.5000
         0         0         0         0         0
         0         0         0         0         0
\end{verbatim}
\end{solution}
\end{exercise}

\AEXER

\begin{exercise} \label{A5.6.1}
Let $V$ be the subspace of $\R^4$ defined by the equations
\begin{align*}
x_1-x_3-x_4&=0\\
x_2-2x_3+x_4&=0
\end{align*}
Consider the vectors
\[
v_1=\begin{bmatrix}-1\\1\\0\\-1\end{bmatrix} \qquad 
v_2=\begin{bmatrix}2\\5\\3\\-1\end{bmatrix} \qquad  
v_3=\begin{bmatrix}2\\1\\1\\1\end{bmatrix} \qquad  
v_4=\begin{bmatrix}2\\-2\\0\\2\end{bmatrix}
\]
Find all bases of $V$ of the form $\left\{v_i,v_j\right\}$ with $1\leq i< j\leq 4.$ (Hint: use Corollary~\ref{C:dim=n}.)

\begin{solution}
\ans The sets $\{v_1,v_3\}$ and $\{v_3,v_4\}$ are bases. 

\soln The vector $v_2$ does not satisfy the second equation, so they are not elements of $V$.  
The subspace $V$ is two-dimensional, so any two linearly independent vectors contained in it form a basis. 
The possibilities are 
\[
\{v_1,v_3\} \qquad  \{v_1,v_4\} \qquad \{v_3,v_4\}
\]
The set $\{v_1,v_4\}$ is not a basis because $v_4= -2v_1$. The sets $\{v_1,v_3\}$ and $\{v_3,v_4\}$ are 
linearly independent sets of two vectors in $V$; so they are bases. 
\end{solution}
\end{exercise}

\begin{exercise} \label{A5.6.2}
Let $A$ be an $m\times n$ matrix and $B$ be an $n\times k$ matrix. 
\begin{enumeratea}
\item Show that null space$(B) \subseteq$ null space$(AB)$.
\item Show that nullity$(B) \leq$ nullity$(AB)$.
\end{enumeratea}

\begin{solution}
\soln Note that if $x\in$ null space$(B)$ then 
\[
ABx=A(Bx)=A0=0
\]
so $x\in$ null space$(AB)$. It follows that null space$(AB)\subseteq$ null space$(B)$. The desired result immediately follows from Corollary~\ref{dimensiondecreases}. Alternatively, suppose by way of contradiction that $\dim$ null space$(AB)>\dim$ null space$(B)$, and let $\{v_1,\ldots,v_d\}$ be a basis for null space$(AB)$, where $d=\dim$ null space$(AB)$. Then $\{v_1,\ldots,v_d \}$ is a set of $d>\dim$ null space$(B)$ linearly independent vectors in null space$(B)$, which contradicts Corollary~\ref{basis<n}.
\end{solution}
\end{exercise}

\begin{exercise} \label{A5.6.3}
Let $\{v_1,v_2,v_3\}$ and $\{w_1,w_2\}$ be linearly independent sets of vectors in a vector space $V$. Show that if
\[
\text{span}\{v_1,v_2,v_3\} \cap \text{span}\{w_1,w_2\}=\{0\}
\]
then 
\[
\dim(\text{span}\{v_1,v_2,v_3,w_1,w_2\}) = 5
\]
{\bf Hint}: First show that if $v\in \text{span}\{v_1,v_2,v_3\}$, $w\in \text{span}\{w_1,w_2\}$, and $v+w=0$, then $v=w=0$. 


\begin{solution} 
\soln First, we verify the claim in the hint. Let $v\in \text{span}\{v_1,v_2,v_3\}$, $w\in \text{span}\{w_1,w_2\}$, and $v+w=0$. Since $\text{span}\{w_1,w_2,w_3\}$ is closed under scalar multiplication, it follows that $ v= -w \in  \text{span}\{w_1,w_2\}$. Therefore
\[
v\in \text{span}\{v_1,v_2,v_3\} \cap \text{span}\{w_1,w_2\} = \{0\}
\]
and $v = w = 0$.

Next, we show that $\{v_1,v_2,v_3,w_1,w_2 \}$ is a linearly independent set, which implies that it is a basis for $\text{span}\{v_1,v_2,v_3, w_1, w_2\}$.  Suppose that $a_1,a_2, a_3$ and $b_1, b_2$ are scalars so that
\begin{equation} \label{A5.6.3_1}
a_1 v_1 + a_2 v_2 + a_3 v_3 + b_1 w_1 + b_2 w_2 = 0
\end{equation}
Let $v = a_1 v_1 + a_2 v_2 + a_3 v_3$ and $w = b_1 w_1 + b_2 w_2$.  Then $v + w = 0$. So, by the hint, $v = w = 0$.  Since $\{v_1,v_2,v_3\}$ is a linearly independent set so
\[
a_1 v_1 + a_2 v_2 + a_3 v_3 = v =0
\]
implies that $a_1 = a_2 = a_3 = 0$.  Similarly, $\{w_1, w_2\}$ is a linearly independent set so
\[
b_1 w_1 + b_2 w_2 = w = 0
\]
implies that $b_1 = b_2 = 0$.  Therefore, the only solution to \eqref{A5.6.3_1} is 
$a_1 = a_2 = a_3 = b_1 = b_2 = 0$.  Thus, the set $\{v_1,v_2,v_3,w_1,w_2\}$ is linearly independent and therefore a basis for its span.  It follows that
\[
\dim(\text{span}\{v_1,v_2,v_3,w_1,w_2\}) = 3 + 2 = 5.
\]
\end{solution}
\end{exercise}

In Exercises~\ref{A5.6.4}-\ref{A5.6.9} decide whether the statement is true or false, and explain your answer.

\begin{exercise}  \label{A5.6.4}
Every set of three vectors in $\R^3$ is a basis for $\R^3$.
\begin{solution}
\ans False  

\soln
The vectors could be linearly independent. For example $\{e_1,e_2,e_1+e_2\}$ is not a basis for $\R^3$.
\end{solution}
\end{exercise}

\begin{exercise}  \label{A5.6.5}
Every set of four vectors in $\R^3$ is linearly dependent.
\begin{solution}
\ans True

\soln
The dimension of $\R^3$ is three, so any set of more than three vectors in $R^3$ is necessarily linearly dependent.
\end{solution}
\end{exercise}

\begin{exercise}  \label{A5.6.6}
If $\{v_1,v_2\}$ is a basis for the plane $z = 0$ in $\R^3$, then $\{v_1,v_2,e_3\}$ is a basis for $\R^3$.
\begin{solution}
\ans True
\soln
The vector $e_3$ is not contained in the plane $z=0$, so Lemma~\ref{extendindep} implies that $\{v_1,v_2,e_3\}$  is linearly independent. Therefore, $\{v_1,v_2,e_3\}$  is a basis for $\R^3$, because any set of three linearly independent vectors in a vector space of dimension three is a basis for that vector space. 

Alternatively, $\{e_1,e_2,e_3\}$ is a basis for $\R^3$, so if $w\in \R^3$, then $w = a_1 e_1 +a_2 e_2 +a_3 e_3$ for some scalars $a_1$ and $a_2$. It follows that $a_1 e_1 + a_2 e_2$ is a vector in the plane $z=0$, so there exist scalars $b_1$ and $b_2$ so $a_1 e_1+ a_2 e_2 = b_1 v_1 +b_2 v_2$. Therefore, $w = b_1 v_1 + b_2 v_2 + a_3 e_3$, so $w\in \text{span}\{v_1,v_2,e_3\}$. Therefore, $\{v_1,v_2,e_3\}$ spans $\R^3$, and is a basis for $\R^3$ because any set of three linearly independent vectors in a vector space of dimension three is a basis for that vector space. 
\end{solution}
\end{exercise}

\begin{exercise}  \label{A5.6.7}
If $\{v_1,v_2,v_3\}$ is a basis for $\R^3$, the only subspaces of $R^3$ of dimension one are $\text{span}\{v_1\}, \text{span}\{v_2\}$, and $\text{span}\{v_3\}$. 
\begin{solution}
\ans False 
\soln For example, $\{e_1,e_2,e_3\}$ is a basis for $\R^3$ and $\text{span}\{e_1+e_2\}$ does not equal the $x$, $y$, or $z$-axes.
\end{solution}
\end{exercise}

\begin{exercise}  \label{A5.6.8}
The only subspace of $\R^3$ that contains finitely many vectors is $\{ 0 \}$.
\begin{solution}
\ans True
\soln If a subspace of $\R^3$ contains a nonzero vector, it must contain all scalar multiples of that vector.
\end{solution}
\end{exercise}

\begin{exercise}  \label{A5.6.9}
If $U$ is a subspace of $\R^3$ of dimension 1 and $V$ is a subspace of $\R^3$ of dimension 2, then $U\cap V=\{0\}$.
\begin{solution}
\ans False 
\soln $U\cap V$ is always subspace of $\mathbb{R}^3,$ but its dimension could be one. For example, if $U$ is the $x$-axis ( $U=\text{span}\{e_1\}$) and $V$ is the $xy$-plane ($V=\text{span}\{e_1,e_2\}$) then $U\cap V=U.$
$U\cap V$ is always subspace of $\R^3$, but its dimension could be one. For example, if $U$ is the $x$-axis ($U=\text{span}\{e_1\}$) and $V$ is the $xy$-plane ($V=\text{span}\{e_1,e_2\}$), then $U\cap V=U$.
\end{solution}
\end{exercise}













\end{document}
