\chapter{Vector Spaces}

\subsection*{Section~\protect{\ref{S:5.1}} Vector Spaces and Subspaces}
\rhead{S:5.1}{VECTOR SPACES AND SUBSPACES}

\exer{c5.1.1}
The set $V_1 \subset \R^3$ is a subspace.
The set $V_1$ contains all vectors $(a,-a,-2a)$,
where $a \in \R$.  We can show that it is closed under
vector addition, since
\[
a(1,-1,-2) + b(1,-1,-2) = (a + b)(1,-1,-2) \in V_1
\]
where $a$ and $b$ are scalars.  The set $V_1$ is closed under scalar
multiplication since
\[
b(a(1,-1,-2) = (ba)(1,-1,-2) \in V_1.
\]

\exer{c5.1.2}
The set $V_2$ is a vector space.
Let $A$ and $B$ be matrices in $V_2$:
\[
A = \left(\begin{array}{rrr} a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \end{array}\right) \AND
B = \left(\begin{array}{rrr} b_{11} & b_{12} & b_{13} \\
b_{21} & b_{22} & b_{23} \end{array}\right).
\]
The set $V_2$ is closed under vector addition because
\[
A + B
= \left(\begin{array}{rrr} a_{11} + b_{11} & a_{12} + b_{12} &
a_{13} + b_{13} \\ a_{21} + b_{21} & a_{22} + b_{22} &
a_{23} + b_{23} \end{array}\right) \in V_2,
\]
and is closed under scalar multiplication because, for $r \in \R$,
\[
rA = \left(\begin{array}{rrr} ra_{11} & ra_{12} & ra_{13} \\
ra_{21} & ra_{22} & ra_{23} \end{array}\right) \in V_2.
\]
Next, verify that $V_2$ satisfies the eight properties of vector
spaces, shown in Table~\ref{vectorspacelist}.

\begin{itemize}
\item[(A1)] Addition is commutative in $V_2$, since
\[
A + B  = \left(\begin{array}{rrr} a_{11} + b_{11} & a_{12} + b_{12} &
a_{13} + b_{13} \\ a_{21} + b_{21} & a_{22} + b_{22} &
a_{23} + b_{23} \end{array}\right) = B + A.
\]
\item[(A2)] Given a third matrix $C \in V_2$,
\[
(A + B) + C = \left(\begin{array}{rrr} a_{11} + b_{11} + c_{11}
& a_{12} + b_{12} + c_{12} & a_{13} + b_{13} + c_{13} \\ a_{21} + b_{21}
+ c_{21} & a_{22} + b_{22} + c_{22} & a_{23} + b_{23} + c_{23}
\end{array}\right) = A + (B + C).
\]
Thus, addition is associative.
\item[(A3)] The additive identity is the matrix
\[
0 = \left(\begin{array}{rrr} 0 & 0 & 0 \\ 0 & 0 & 0 \end{array}\right).
\]
\item[(A4)] For each matrix $A \in V_2$, there exists an element $-A$ such
that $A + (-A) = 0$, where
\[
-A = \left(\begin{array}{rrr} -a_{11} & -a_{12} & -a_{13} \\ -a_{21} &
-a_{22} & -a_{23} \end{array}\right).
\]
\item[(M1)] Scalar multiplication is associative in $V_2$ since, for any
$r$, $s \in \R$ and $A \in V_2$,
\[
(rs)A = \left(\begin{array}{rrr} rsa_{11} & rsa_{12} & rsa_{13} \\
rsa_{21} & rsa_{22} & rsa_{23} \end{array}\right) = r(sA).
\]
\item[(M2)] There exists a multiplicative identity in $V_2$, since $1A = A$
for any $A \in V_2$.
\item[(D1)] For any scalars $r$, $s \in \R$, and any matrix $A \in V_2$,
\[
(r + s)A = \left(\begin{array}{rrr}
ra_{11} + sa_{11} & ra_{12} + sa_{12} & ra_{13} + sa_{13} \\
ra_{21} + sa_{21} & ra_{22} + sa_{22} & ra_{23} + sa_{23} \end{array}\right)
= rA + sA.
\]
\item[(D2)] For any scalar $r \in \R$ and any matrices $A, B \in V_2$,
\[
r(A + B) = \left(\begin{array}{rrr} ra_{11} + rb_{11} & ra_{12} + rb_{12} &
ra_{13} + rb_{13} \\ ra_{21} + rb_{21} & ra_{22} + rb_{22} &
ra_{23} + rb_{23} \end{array}\right)
= rA + rB.
\]
\end{itemize}
Thus, $V_2$ is a vector space, since it is closed under addition
and scalar multiplication and satisfies the eight properties of vector spaces.

\exer{c5.1.3}
The set $V_3$ is a subspace of $R^3$ since the solution set to
any equation $Ax = 0$ is a space.  This is demonstrated by the
principle of superposition introduced in Section~\ref{S:Superposition}.
Also, $V_3 = V_1$.  

\para We can show that $V_3 = V_1$ by row reducing to find the
solutions to $Ax = 0$:
\[
\left(\begin{array}{rrr} 1 & 1 & 0 \\ 1 & -1 & 1
\end{array}\right) \longrightarrow \left(\begin{array}{rrr} 1 & 0 &
\frac{1}{2} \\ 0 & 1 & -\frac{1}{2} \end{array}\right).
\]
So all vectors in $V_3$ are of the form $x = s(-\frac{1}{2},
\frac{1}{2}, 1)$, where $s \in \R$.  The vector $x$ is an element
of $V_1$ for each $s$.

\newpage
\exer{c5.1.4a} $W$ is a subspace of $V$, since $W$ is closed under
addition and scalar multiplication.

\exer{c5.1.4b} \ans $W$ is not a subspace of $V$.

\soln The subset $W$ is closed neither under addition nor under scalar
multiplication.  For example, let $w_1 = (1,4,2)$ and $w_2 = (1,-1,3)$
be elements of $W$.  Then,
\[
w_1 + w_2 = (1,4,2) + (1,-1,3) = (2,3,5)
\]
which is not an element of $W$.


\exer{c5.1.4d} \ans $W$ is not a subspace of $V$.

\soln The subset $W$ is closed neither under addition nor under scalar
multiplication.  For example, let $w_1 = (3,-2)$ and $w_2 = (0,1)$ be
elements of $W$.  Then,
\[
w_1 + w_2 = (3,-2) + (0,1) = (3,-3).
\]
The sum of the elements $3 - 3 = 0 \neq 1$.

\exer{c5.1.4c} $W$ is a subspace of $V$, since $W$ is closed under
addition and scalar multiplication.

\exer{c5.1.4g} $W$ is a subspace of $V$, since $W$ is closed under
addition and scalar multiplication.

\exer{c5.1.4e} $W$ is a subspace of $V$, since $W$ is closed under
addition and scalar multiplication.

\exer{c5.1.4f} \ans $W$ is not a subspace of $V$.

\soln The subset $W$ is closed neither under addition nor under scalar
multiplication.  For example, let $w_1(t) = t$ and let $w_2(t) = t^2$.
\[
(w_1 + w_2)(1) = w_1(1) + w_2(1) = 1 + 1 = 2,
\]
so $(w_1 + w_2)(t)$ is not an element of $W$.


\exer{c5.1.5a} \ans The set $S$ is not a subspace.

\soln The set $S$ is closed under addition but not under scalar
multiplication.  To demonstrate, let $x = (1,4,2)$ be an element of $S$. 
Then
\[
-2x = -2(1,4,2) = (-2,-8,-4)
\]
which is not an element of $S$.

\exer{c5.1.5b} The set $S$ is a subspace, since it is closed under
addition and scalar multiplication.

\newpage 
\exer{c5.1.5c} The set $S$ is a subspace, since it is closed under
addition and scalar multiplication.

\exer{c5.1.5d} The set $S$ is a subspace, since it is closed under
addition and scalar multiplication.

\exer{c5.1.5e} \ans The set $S$ is not a subspace.

\soln The set $S$ is closed under neither addition nor scalar
multiplication.  For example, let $x_1$ and $x_2$ be solutions to the
equation $Ax = b$.  Then,
\[
A(x_1 + x_2) = Ax_1 + Ax_2 = b + b = 2b,
\]
so $(x_1 + x_2)$ is not an element of $S$.

\exer{c5.1.6}
The subset $W_1 \cap W_2$ is a subspace of $V$.  To show that this
subset is closed under addition and scalar multiplication, 
let $x$ and $y$ be vectors in $W_1 \cap W_2$.  It follows that
$x,y \in W_1$ and $x,y \in W_2$.  Therefore, by the
definition of a subspace, $x + y \in W_1$ and $x + y \in W_2$, so
$x + y \in W_1 \cap W_2$.  Also by definition, $rx \in W_1$ and
$rx \in W_2$, for some scalar $r$, so $rx \in W_1 \cap W_2$.

\exer{c5.1.7a}
\ans Let $V$ be the subset of solutions $(x,y)$ to $ax + by = c$.
The subset $V$ is a subspace when $c = 0$ and is not a subspace
when $c \neq 0$. 

\soln Let $(x_1,y_1)$ and $(x_2,y_2)$ be elements of $V$.  Then
\[
a(x_1 + x_2) + b(y_1 + y_2) = (ax_1 + by_1) + (ax_2 + by_2) =
c + c = 2c.
\]
Thus $V$ is closed under addition only when $2c = c$, so $c = 0$.
Similarly, for any scalar $r$,
\[
r(ax_1 + by_1) = cr.
\]
So $V$ is closed under scalar multiplication only when $rc = c$ for
any scalar $r$.  Thus, $c = 0$.

\exer{c5.1.7b}
\ans By the same proof as in Exercise~\ref{c5.1.7a}, the solutions to the
equation $ax + by + cz = d$ form a subspace of $\R^3$ when $d = 0$,
and do not form a subspace when $d \neq 0$.

\exer{c5.1.8}
The set of all solutions to the differential equation $\dot{x} = 2x$ is
indeed a subspace of $\CCone$.  To demonstrate, let $x_1$ and $x_2$
be elements of this set.  The set is closed under addition since
\[
\frac{d}{dt}(x_1 + x_2) = \frac{d}{dt}(x_1) + \frac{d}{dt}(x_2)
= 2x_1 + 2x_2 = 2(x_1 + x_2)
\]
and closed under scalar multiplication since, for any real scalar $r$,
\[
\frac{d}{dt}(rx_1) = r\frac{d}{dt}(x_1) = 2(rx_1).
\]
Note that this problem provides another example of the principle of
superposition.

\exer{c5.1.9}
Let $V \subset (\CCone)^2$ be the set of solutions to \Ref{e:solnODE}.
The set is closed under both addition and scalar multiplication and
is a subspace.
To demonstrate, let
\[
x_1(t) = \alpha_1 e^{2t}\vectwo{1}{1} +
\beta_1 e^{-4t}\vectwo{1}{-1} \AND x_2(t) =
\alpha_2 e^{2t}\vectwo{1}{1} + \beta_2 e^{-4t}\vectwo{1}{-1}
\]
be elements of this set.  Adding $x_1$ and $x_2$ yields
\[
\left(\alpha_1 e^{2t}\vectwo{1}{1} +
\beta_1 e^{-4t}\vectwo{1}{-1}\right) +
\left(\alpha_2 e^{2t}\vectwo{1}{1}
+ \beta_2 e^{-4t}\vectwo{1}{-1}\right)
\]
\[
= (\alpha_1 + \alpha_2)e^{2t}\vectwo{1}{1} +
(\beta_1 + \beta_2) e^{-4t}\vectwo{1}{-1} \in V
\]
and multiplying $x_1$ by any real scalar $r$ yields
\[
rx_1 = r\left(\alpha_1 e^{2t}\vectwo{1}{1} +
\beta_1 e^{-4t}\vectwo{1}{-1}\right) = r\alpha_1 e^{2t}\vectwo{1}{1} +
r\beta_1 e^{-4t}\vectwo{1}{-1} \in V.
\]


\subsection*{Section~\protect{\ref{S:5.2}} Construction of Subspaces}
\rhead{S:5.2}{CONSTRUCTION OF SUBSPACES}

\exer{c5.2.1a} \ans The subspace of solutions can be spanned by the vectors 
$(1,0,-4)^t$ and $(0,1,2)^t$.

\soln All solutions to $4x - 2y + z = 0$ can be written in the form
\[
\vecthree{x}{y}{z} = \cvecthree{x}{y}{2y - 4x}
= x\vecthree{1}{0}{-4} + y\vecthree{0}{1}{2}.
\]

\exer{c5.2.1b}  \ans The subspace of solutions can be spanned by the vectors 
$(1,1,0)^t$ and $(-3,0,1)^t$.

\soln All solutions to $x - y + 3z = 0$ can be written in the form
\[
\vecthree{x}{y}{z} = \cvecthree{y-3z}{y}{z}
= y\vecthree{1}{1}{0} + z\vecthree{-3}{0}{1}.
\]

\exer{c5.2.1c}  \ans The subspace of solutions can be spanned by the vectors 
$(1,0,-1)^t$ and $(0,1,-1)^t$.

\soln All solutions to $x + y + z = 0$ can be written in the form
\[
\vecthree{x}{y}{z} = \cvecthree{x}{y}{-x-y}
= x\vecthree{1}{0}{-1} + z\vecthree{0}{1}{-1}.
\]



\exer{c5.2.1d} \ans The subspace of solutions can be spanned by the vectors 
$(1,0,0)^t$ and $(0,1,1)^t$.

\soln All solutions to $y = z$ can be written in the form
\[
\vecthree{x}{y}{z} = \cvecthree{x}{y}{y}
= x\vecthree{1}{0}{0} + z\vecthree{0}{1}{1}.
\]


\exer{c5.2.2a}
\ans The subspace of solutions is spanned by the vectors
\[
(-2,1,0,0,0)^t \AND (-1,0,-4,1,0)^t.
\]

\soln Let $x = (x_1,\dots ,x_5)$ be a solution to $Ax = 0$.  All
solutions to this equation have the form
\[
\left(\begin{array}{r} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
\end{array}\right) = \left(\begin{array}{c} -2x_2 - x_4 \\ x_2 \\
-4x_4 \\ x_4 \\ 0 \end{array}\right) = x_2\left(\begin{array}{r}
-2 \\ 1 \\ 0 \\ 0 \\ 0 \end{array}\right) +
x_4\left(\begin{array}{r} -1 \\ 0 \\ -4 \\ 1 \\ 0
\end{array}\right).
\]

\exer{c5.2.2b}
\ans The subspace of solutions to $Bx = 0$ is spanned by the vectors
\[
(-3,1,0,0)^t \AND (-5,0,-2,1)^t.
\]

\soln Let $x = (x_1,x_2,x_3,x_4)$ be a solution to $Bx = 0$.  All
solutions to this equation have the form
\[
\left(\begin{array}{r} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right)
= \left(\begin{array}{c} -3x_2 - 5x_4 \\ x_2 \\ -2x_4 \\ x_4
\end{array}\right) = x_2\left(\begin{array}{r} -3 \\ 1 \\ 0 \\ 0
\end{array}\right) + x_4\left(\begin{array}{r} -5 \\ 0 \\ -2 \\ 1
\end{array}\right).
\]

\exer{c5.2.2c}
\ans The subspace of solutions to $Ax = 0$ is spanned by the vector
$(-2,-1,1)^t$.

\soln Let $x = (x_1,x_2,x_3)$ be a solution to $Ax = 0$.  All solutions
to this equation have the form
\[
\vecthree{x_1}{x_2}{x_3} = \vecthree{-2x_3}{-x_3}{x_3} =
x_3\vecthree{-2}{-1}{1}.
\]

\exer{c5.2.2d}
\ans The subspace of solutions to $Bx = 0$ is spanned by the vectors
\[
\left(\begin{array}{r} 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \end{array}\right), \quad
\left(\begin{array}{r} -5 \\ 0 \\ -2 \\ 1 \\ 0 \\ 0 \end{array}\right), \quad
\left(\begin{array}{r} 0 \\ 0 \\ -2 \\ 0 \\ -2 \\ 1 \end{array}\right).
\]

\soln Let $x = (x_1,\dots,x_6)$ be a solution to $Bx = 0$.  All solutions
to this equation have the form
\[
\left(\begin{array}{r} x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5 \\ x_6
\end{array}\right) =
\left(\begin{array}{c} x_2 - 5x_4 \\ x_2 \\ -2x_4 - 2x_6 \\ x_4 \\ -2x_6
\\ x_6 \end{array}\right) =
x_2\left(\begin{array}{r} 1 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \end{array}\right) +
x_4\left(\begin{array}{r} -5 \\ 0 \\ -2 \\ 1 \\ 0 \\ 0 \end{array}\right) +
x_6\left(\begin{array}{r} 0 \\ 0 \\ -2 \\ 0 \\ -2 \\ 1 \end{array}\right).
\]


\exer{c5.2.3}
\ans The matrix $A$ whose subspace of solutions in $\R^4$ is the span of
$v_1$ and $v_2$ is
\[
A = \left(\begin{array}{rrrr} 1 & 1 & 0 & 0 \\ 0 & 0 & 1 & 1
\end{array}\right).
\]

\soln Note that all vectors $x$ in the spanning set of $v_1$ and $v_2$
are of the form:
\[
x = \left(\begin{array}{r} x_1 \\ x_2 \\ x_3 \\ x_4
\end{array}\right)
= a\left(\begin{array}{r} 1 \\ -1 \\ 0 \\ 0 \end{array}\right) + 
b\left(\begin{array}{r} 0 \\ 0 \\ 1 \\ -1 \end{array}\right) =
\left(\begin{array}{r} a \\ -a \\ b \\ -b \end{array}\right).
\]
Therefore, $x_1 = -x_2$ and $x_3 = -x_4$.  So,
\[
\begin{array}{rrrrrrrrl}
x_1 & + & x_2 & & & & & = & 0 \\
& & & & x_3 & + & x_4 & = & 0. \end{array}
\]
The matrix of this system is $A$.

\exer{c5.2.4}
\[ A = \mattwo{2}{2}{-3}{0} = 2\mattwo{1}{1}{0}{0} -
3\mattwo{0}{0}{1}{0} = 2B - 3C. \]

\exer{c5.2.5}
\ans The vector $(2,20,0)^t$ is in the span of $w_1$ and $w_2$. 
Specifically, $v = -4w_1 + 6w_2$.

\soln Note that, for some real numbers $a$ and $b$,
\[
(2,20,0)^t = aw_1 + bw_2 = a(1,1,3)^t + b(1,4,2)^t
\]
if $v$ is in the span of $w_1$ and $w_2$.
This corresponds to the linear system
\[
\begin{array}{rrrrr}
a & + & b & = 2 \\
a & + & 4b & = 20 \\
3a & + & 2b & = 0 \end{array}
\]
To find $a$ and $b$, row reduce the augmented matrix of the system:
\[
\left(\begin{array}{rr|r} 1 & 1 & 2 \\ 1 & 4 & 20 \\
3 & 2 & 0 \end{array}\right) \longrightarrow
\left(\begin{array}{rr|r} 1 & 0 & -4 \\ 0 & 1 & 6 \\
0 & 0 & 0 \end{array}\right).
\]
The system is consistent; $a = -4$ and $b = 6$.

\exer{c5.2.6a}
\ans The function $y(t) = 1 - t^2$ is an element of $W$ and the set
$\{y(t),x_2(t)\}$ is a spanning set for $W$.



\soln The space $W$ equals $\Span\{x_1(t),x_2(t)\}$ where $x_1(t)=1$ and 
$x_2(t)=t^2$.  To show that $y(t)$ is an element of $W$, let
$a = 1$ and $b = -1$, and compute
\[
ax_1(t) + bx_2(t) = x_1(t) - x_2(t) = 1 - t^2 = y(t). 
\]
To show that $\{y(t),x_2(t)\}$ is a spanning set for $W$, rewrite every
linear combination of $x_1(t)$ and $x_2(t)$ in terms of $y(t)$ and $x_2(t)$, 
as follows:
\[ 
ax_1(t) + bx_2(t) = a + bt^2 = a(1 - t^2) + (a + b)t^2
= ay(t) + (a + b)x_2(t). 
\]

\exer{c5.2.6b} The function $y(t) = t^4$ is not in $W$.

\exer{c5.2.6c} The function $y(t) = \sin(t)$ is not in $W$.

\exer{c5.2.6d}
\ans The function $y(t) = 0.5t^2$ is an element of $W$, but the set
$\{y(t),x_2(t)\}$ does not span $W$.

\soln When $a = 0$ and $b = 0.5$,
\[ 
ax_1(t) + bx_2(t) = 0.5x_2(t) = 0.5t^2 = y(t). 
\]
In this case, there exist functions in $W$ that are not in 
$\Span\{y(t),x_2(t)\}$.  For example, the function $x_1(t) = 1$ cannot
be written as a linear combination of $x_2(t)$ and $y(t)$.

\exer{c5.2.7}
\ans The span of $W$ is the set of solutions to the system
\[ 
\begin{array}{rrrrrrrrr}
x_1 & + & x_2 & - & x_3 & & & = & 0 \\
& & 3x_2 & - & x_3 & - & x_4 & = & 0 \end{array}. 
\]
where $x = (x_1,x_2,x_3,x_4) \in W$.  Row reduction of the associated
matrix demonstrates that this system is a valid solution set.

\soln Solve for $x$ as a linear combination of $w_1$ and $w_2$
by creating the matrix whose columns are $w_1$ and $w_2$,
then setting up the equation:
\[ 
\left(\begin{array}{rr} -1 & 2 \\ 2 & 1 \\ 1 & 3 \\ 5 & 0 
\end{array}\right) \vectwo{a}{b} = \left(\begin{array}{r} x_1 \\
x_2 \\ x_3 \\ x_4 \end{array}\right) 
\]
where $a$ and $b$ are scalars.  Then row reduce the associated
augmented matrix:
\[ 
\left(\begin{array}{rr|r} -1 & 2 & x_1 \\ 2 & 1 & x_2 \\ 1 & 3
& x_3 \\ 5 & 0 & x_4 \end{array}\right) \longrightarrow
\left(\begin{array}{rr|l} 1 & 3 & x_3 \\ 0 & -5 & x_2 - 2x_3 \\
0 & 0 & x_1 + x_2 - x_3 \\ 0 & 0 & -3x_2 + x_3 + x_4
\end{array}\right). 
\]
Extract from this solution the values that are independent of $a$
and $b$ to obtain the linear system above.


\exer{c5.2.8a}
Every vector $x \in \Span\{v\}$ is of the form $x = av
= av + 0v$, so $x \in \Span\{v,v\}$.  Therefore, $\Span\{v\}
\subset \Span\{v,v\}$.  Every vector $y \in \Span\{v,v\}$ is of the
form 
\[
y = bv + cv = (b + c)v \in \Span\{v\}.
\]
Therefore $\Span\{v,v\} \subset \Span\{v,v\}$, so the two spans are equal.

\exer{c5.2.8b} Every vector $x \in \Span\{v,w\}$ is of the form 
\[
x = av + bw = av + bw + 0(v + 3w) \in \Span\{v,w,v+3w\}.
\]
  Also, every vector $y \in \Span\{v,w,v+3w\}$ is of the form
\[
y = cv + dw + f(v + 3w) = (c + f)v + (d + 3f)w \in \Span\{v,w\}.
\]
Therefore, $\Span\{v,w\} = \Span\{v,w,v+3w\}$.

\exer{c5.2.9}
Since $W = \Span\{w_1,\dots ,w_k\}$, every vector $x \in W$ can be
written as the linear combination
\[ x = a_1w_1 + \cdots + a_kw_k \]
for some choice of $a_1 \dots a_k$.  Since $w_{k + 1}$ is a vector in
$W$, it can therefore be written as
\[ w_{k + 1} = b_1w_1 + \cdots + b_kw_k \]
and any vector $x \in W$ can be written as
\[ \begin{array}{rcl}
x & = &
a_1w_1 + \cdots + a_kw_k + a_{k+1}w_{k+1} \\
& = & a_1w_1 + \cdots + a_kw_k + a_{k+1}b_1w_1 + \cdots + a_{k+1}b_kw_k
\\ & = & (a_1 + a_{k+1}b_1)w_1 + \cdots + (a_k + a_{k+1}b_k)w_k.
\end{array} \]
So, $W = \Span\{w_1,\dots ,w_{k+1}\}$.

\exer{c5.2.10}
\ans The relationship of the constants is $m \geq n = r = s$.

\soln The rank of matrix $A$ cannot be greater than the rank of matrix
$(A|b)$, since $(A|b)$ consists of $A$ plus one column.  The rank of $A$
is the number of pivots in the row reduced matrix.  $(A|b)$ can be row 
reduced through the same operations, and will have either the same number
of pivots as $A$ or, if there is a pivot in the last column, one more
pivot than $A$.  Since the system has a unique solution, it is consistent,
and therefore $(A|b)$ cannot have a pivot in the $(n + 1)^{st}$ column, so
$r = \rank(A) = \rank(A|b) = s$.

\para The set of solutions is parameterized by $n - r$ parameters,
where $n$ is the number of columns of $A$.  Since there is a unique
solution, the set of solutions is parameterized by $0$ parameters,
so $n = r$.

\para The number $m$ of rows of the matrix must be greater than or
equal to $n$ in order for the system to have a unique solution, since
there must be $n$ pivots, and each pivot must be in a separate row.


\subsection*{Section~\protect{\ref{S:5.3}} Spanning Sets and \Matlab}
\rhead{S:5.3}{SPANNING SETS AND MATLAB}

\exer{c5.3.1a}
Type {\tt null(A)} in \Matlab to find that the set of solutions to
$Ax = 0$ is spanned by the vectors
\[
\left(\begin{array}{r} 0.3225 \\ 0.8931 \\ -0.0992 \\ 0.2977
\end{array}\right) \AND \left(\begin{array}{r} 0 \\ -0.1961 \\
0.5883 \\ 0.7845 \end{array}\right).
\]

\exer{c5.3.1b} \ans The solution to $Ax = 0$ is the vector $(0,0).$

\soln This can be demonstrated by typing {\tt null(A)} in \Matlab, which
yields
\begin{verbatim}
ans =
   Empty matrix: 2-by-0
\end{verbatim}

\exer{c5.3.1c} The set of solutions to $Ax = 0$ is spanned by the vector
\[
\vecthree{-0.8452}{-0.1690}{0.5071}.
\]

\newpage
\exer{c5.3.2}
Enter matrix {\tt A} into \Matlab and type {\tt null(A)}, obtaining
\begin{verbatim}
ans =
   -0.8957         0
    0.4414    0.1204
   -0.0519    0.9631
    0.0130   -0.2408
         0         0
\end{verbatim}
The \Matlab answer is a valid solution if the vectors found
by row reduction can be written as linear combinations of the \Matlab
answers.  In \Matlab, row reduce the augmented matrices {\tt null(A)|x},
and {\tt null(A)|y} where $x = (-2,1,0,0,0)$ and $y = (-1,0,-4,1,0)$ 
to find that
\[
\left(\begin{array}{r} -2 \\ 1 \\ 0 \\ 0 \\ 0 \end{array}\right) =
2.2328\left(\begin{array}{r} -0.8957 \\ 0.4414 \\ -0.0519 \\ 0.0130 \\ 0
\end{array}\right) + 0.1204\left(\begin{array}{r} 0 \\ 0.1204 \\ 0.9631 \\
-0.2408 \\ 0 \end{array}\right)
\]
and
\[
\left(\begin{array}{r} -1 \\ 0 \\ -4 \\ 1 \\ 0 \end{array}\right) =
1.1164\left(\begin{array}{r} -0.8957 \\ 0.4414 \\ -0.0519 \\ 0.0130 \\ 0
\end{array}\right) - 4.0931\left(\begin{array}{r} 0 \\ 0.1204 \\ 0.9631 \\
-0.2408 \\ 0 \end{array}\right).
\]

Enter matrix {\tt B} into \Matlab and type {\tt null(B)}, obtaining
\begin{verbatim}
ans =
   -0.9661         0
    0.2070   -0.5976
   -0.1380   -0.7171
    0.0690    0.3586
\end{verbatim}
Again, the \Matlab answer can be checked by verifying by row reduction
of the augmented matrix that the vectors $(-3,1,0,0)$ and $(-5,0,-2,1)$
can be written as linear combinations of the \Matlab solution vectors.
In particular,
\[
\left(\begin{array}{r} -3 \\ 1 \\ 0 \\ 0 \end{array}\right) =
3.1053\left(\begin{array}{r} -0.9661 \\ 0.2070 \\ -0.1380 \\ 0.0690
\end{array}\right) - 0.5976\left(\begin{array}{r} 0 \\ -0.5976 \\ -0.7171 \\
0.3586 \end{array}\right)
\]
and
\[
\left(\begin{array}{r} -5 \\ 0 \\ -2 \\ 1 \end{array}\right) =
5.1755\left(\begin{array}{r} -0.9661 \\ 0.2070 \\ -0.1380 \\ 0.0690
\end{array}\right) + 1.7928\left(\begin{array}{r} 0 \\ -0.5976 \\ -0.7171 \\
0.3586 \end{array}\right).
\]

\exer{c5.3.3}
\ans The solution set of $Bx = 0$ is
\[
\left(\begin{array}{r} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right)
= \left(\begin{array}{c} -x_3 + \frac{3}{4}x_4 \\ -3x_3 + 2x_4 \\
x_3 \\ x_4 \end{array}\right) = x_3\left(\begin{array}{r} -1 \\ -3 \\
1 \\ 0 \end{array}\right) + x_4\left(\begin{array}{r} \frac{3}{4} \\
2 \\ 0 \\ 1 \end{array}\right).
\]

\soln Row reduce $B$:
\[
\left(\begin{array}{rrrr} -4 & 0 & 4 & 3 \\ -4 & 1 & -1 & 1
\end{array}\right) \longrightarrow \left(\begin{array}{rrrr}
1 & 0 & 1 & -\frac{3}{4} \\ 0 & 1 & 3 & -2 \end{array}\right).
\]

The solution obtained by row reduction is not the same as the one
obtained using {\tt null}, but the solution vectors are linear
combinations of the \Matlab solution vectors, so the answers are
equivalent.  By row reducing the matrix {\tt [null(B) x]}, where
$x = (-1,-3,1,0)$, we find that
\[
\left(\begin{array}{r} -1 \\ -3 \\ 1 \\ 0 \end{array}\right) =
-3.1009\left(\begin{array}{r} 0.3225 \\ 0.8931 \\ -0.0992 \\ 0.2977
\end{array}\right) + 1.1767\left(\begin{array}{r} 0 \\ -0.1961 \\
0.5883 \\ 0.7845 \end{array}\right).
\]
By row reducing the matrix {\tt [null(B) y]} where $y = (\frac{3}{4},
2,0,1)$ we find that:
\[
\left(\begin{array}{r} \frac{3}{4} \\ 2 \\ 0 \\ 1 \end{array}\right) =
2.3257\left(\begin{array}{r} 0.3225 \\ 0.8931 \\ -0.0992 \\ 0.2977
\end{array}\right) + 0.3922\left(\begin{array}{r} 0 \\ -0.1961 \\
0.5883 \\ 0.7845 \end{array}\right).
\]

\exer{c5.3.4a} \ans Vector $v_1$ is an element of $W$.

\soln The vector $v_1$ is an element of $W$ if there exist scalars $a$,
$b$, and $c$ such that
\[
aw_1 + bw_2 + cw_3 = v_1.
\]
Using \Matlab, create the matrix {\tt A = [w1' w2' w3']}, which has
$w_1$, $w_2$, and $w_3$ as its columns.  Then create the augmented
matrix {\tt aug1 = [A v1']}.  The command {\tt rref(aug1)} yields
\begin{verbatim}
ans =
     1     0     0     2
     0     1     0    -2
     0     0     1     1
     0     0     0     0
     0     0     0     0
\end{verbatim}
Since there is no pivot point in the last column, the linear system
$aw_1 + bw_2 + cw_3 = v_1$ is consistent, and $v_1 = 2w_1 - 2w_2 + w_3$.

\exer{c5.3.4b} \ans Vector $v_2$ is not an element of $W$.

\soln Create the augmented matrix {\tt aug2 = [A v2']}.  Row reducing
{\tt aug2} yields
\begin{verbatim}
ans =
     1     0     0     0
     0     1     0     0
     0     0     1     0
     0     0     0     1
     0     0     0     0
\end{verbatim}
There is a pivot point in the last column, so the linear system
$aw_1 + bw_2 + cw_3 = v_2$ is inconsistent.

\exer{c5.3.4c} \ans Vector $v_3$ is an element of $W$.

\soln Row reduce the augmented matrix {\tt aug3 = [A v3']} to obtain
\begin{verbatim}
ans =
     1     0     0    -3
     0     1     0     5
     0     0     1    12
     0     0     0     0
     0     0     0     0
\end{verbatim}
which is the matrix of a consistent linear system.  Therefore,
$v_3 = -3w_1 + 5w_2 + 12w_3$.


\subsection*{Section~\protect{\ref{S:5.4}} Linear Dependence and Linear
Independence}
\rhead{S:5.4}{LINEAR DEPENDENCE AND LINEAR INDEPENDENCE}

\exer{c5.4.1}
To show that the set of vectors $\{w_1,w_2\}$ is linearly dependent,
show that there exist nonzero $a$ and $b$ such that
$aw_1 + bw_2 = 0$.  For the set $\{w,0\}$, if $a = 0$ and $b = 1$,
then $0w + 1(0) = 0$, so the set is linearly dependent.  For the
set $\{w,-w\}$, if $a = 1$ and $b = 1$, then
$w - w = 0$, so the set is linearly dependent.

\exer{c5.4.2}
\ans The set is linearly independent if $b \neq -\frac{1}{3}$.

\soln Note that a set of two vectors is linearly dependent if one is
a multiple of the other.  So this set is dependent for any values of
$b$ at which
\[
(3,-1) = \alpha(1,b).
\]
When equality holds $\alpha = 3$.  Therefore, $b = -\frac{1}{3}$.  

\exer{c5.4.3}
\ans The set is linearly dependent.

\soln Let $A$ be the matrix whose columns are $u_1$, $u_2$, and $u_3$. 
The set $\{u_1,u_2,u_3\}$ is linearly dependent if there exists
a nonzero vector $r = (r_1,r_2,r_3)$ such that $r_1u_1 + r_2u_2 +
r_3u_3 = 0$, that is, if the homogeneous system $Ar = 0$ has a
nonzero solution.  Row reduce:
\[
\matthree{1}{2}{10}{-1}{1}{2}{1}{-2}{-6} \longrightarrow
\matthree{1}{0}{2}{0}{1}{4}{0}{0}{0}.
\]
So, $Ar = 0$ when $r = r_3(-2,-4,1)$.
The value of $r$ is nonzero for $r_3 \neq 0$, so the set is indeed
linearly dependent.
As an example, let $r_3 = 1$.  Then,
\[
-4u_1 - 2u_2 + u_3 = -2(1,-1,1) - 4(2,1,-2) + (10,2,-6) =
(0,0,0) = 0.
\]

\exer{c5.4.4}
\ans The vectors $(1,b,2b)$ and $(2,1,4)$ are linearly independent
for any value of $b$.

\soln Two vectors are linearly independent unless one is a multiple
of the other; in this case, unless
\[
(1,b,2b) = \alpha(2,1,4).
\]
Equality holds if $2\alpha = 1$, $\alpha = b$, and $4\alpha = 2b$.
Therefore, $\alpha = \frac{1}{2}$, $b = \frac{1}{2}$ and $b = 1$,
which is inconsistent, so the vectors are linearly independent.

\exer{c5.4.5}
\ans The polynomials $p_1(t) = 2 + t$, $p_2(t) = 1 + t^2$, and $p_3(t) =
t - t^2$ are linearly independent in $\CCone$.  

\soln We can determine this
by noting that the polynomials are linearly dependent if there exists
a nonzero vector $r = (r_1,r_2,r_3)$ such that $r_1p_1 + r_2p_2 +
r_3p_3 = 0$.  It is convenient to represent each polynomial as a
vector $(a,b,c) = p(t) = a + bt + ct^2$.  Thus, $p_1(t) = (2,1,0)$, 
$p_2(t) = (1,0,1)$, and $p_3(t) = (0,1,-1)$.  Solve the homogeneous
system $Ar = 0$, where $A$ is the matrix whose columns are $p_1$,
$p_2$, and $p_3$, by row reduction.
\[ \matthree{2}{1}{0}{1}{0}{1}{0}{1}{-1} \longrightarrow
\matthree{1}{0}{0}{0}{1}{0}{0}{0}{1}. \]
Therefore, there are no nonzero values of $r$ for which $r_1p_1 + 
r_2p_2 + r_3p_3 = 0$, and the polynomials are linearly independent.

\newpage
\exer{c5.4.6}
The three functions are linearly dependent vectors in $\CCone$ since
there exists a nonzero vector $r = (r_1,r_2,r_3)$ such that
$r_1f_1(t) + r_2f_2(t) + r_3f_3(t) = 0$.  We can find this vector $r$
using trigonometric identities:
\[ f_3(t) = \cos\left(t + \frac{\pi}{3}\right) =
\cos\left(\frac{\pi}{3}\right)\cos t + \sin\left(\frac{\pi}{3}\right)\sin t
= \frac{1}{2}\cos t - \frac{\sqrt{3}}{2}\sin t =
\frac{1}{2}f_1(t) - \frac{\sqrt{3}}{2}f_2(t). \]
That is, $\frac{1}{2}f_1(t) + \frac{\sqrt{3}}{2}f_2(t) - f_3(t) = 0$.

\exer{c5.4.7}
To show that the vectors $u_1 + u_2$, $u_2 + u_3$ and $u_3 + u_1$
are linearly independent, we assume that there exist scalars $r_1$,
$r_2$, $r_3$ such that
\[ r_1(u_1 + u_2) + r_2(u_2 + u_3) + r_3(u_3 + u_1) = 0. \]
We then prove that $r_1 = r_2 = r_3 = 0$, as follows.
Use distribution to obtain
\[ (r_1 + r_3)u_1 + (r_1 + r_2)u_2 + (r_2 + r_3)u_3 = 0. \]
Since the set $\{u_1,u_2,u_3\}$ is linearly independent,
\[ \begin{array}{rrrrrcl}
r_1 & & & + & r_3 & = & 0 \\
r_1 & + & r_2 & & & = & 0 \\
& & r_2 & + & r_3 & = & 0. \end{array} \]
Solving this system yields $r_1 = r_2 = r_3 = 0$,
so the set $\{u_1 + u_2,u_2 + u_3,u_3 + u_1\}$ is linearly
independent.

\exer{c5.4.8a} \ans The set $\{v_1,v_2,v_3\}$ is linearly dependent.

\soln The set is linearly dependent if there exist scalars $r_1$, $r_2$,
and $r_3$ such that $r_1v_1 + r_2v_2 + r_3v_3 = 0$.  Create a matrix
{\tt A} whose columns are $v_1$, $v_2$ and $v_3$.  Then row reduce
{\tt A} to solve the homogeneous system $Ar = 0$.  Specifically, row
reducing the matrix {\tt A = [v1 v2 v3]} yields
\begin{verbatim}
ans =
     1     0     5
     0     1     2
     0     0     0
     0     0     0
\end{verbatim}
So $-5v_1 - 2v_2 + v_3 = 0$.

\exer{c5.4.8b} \ans The set $\{w_1,w_2,w_3,w_4\}$ is linearly dependent.

\soln Create the matrix {\tt A} associated to the set
$\{w_1,w_2,w_3,w_4\}$, and row reduce to solve for
$r = (r_1,r_2,r_3,r_4)$, obtaining
\begin{verbatim}
ans =
    1.0000         0         0    0.1429
         0    1.0000         0    0.2857
         0         0    1.0000    0.7143
\end{verbatim}
Therefore, $-0.1429w_1 - 0.2857w_2 - 0.7143w_3 + w_4 = 0$.

\exer{c5.4.8c} \ans The set $\{x_1,x_2,x_3\}$ is linearly independent.

\soln The matrix {\tt A} associated to the set $\{x_1,x_2,x_3\}$ row
reduces to
\begin{verbatim}
ans =
     1     0     0
     0     1     0
     0     0     1
     0     0     0
     0     0     0
\end{verbatim}
In this case, there are no nonzero solutions to
$r_1x_1 + r_2x_2 + r_3x_3 = 0$.
 
\exer{c5.4.9}
(a) The set of commands to perform this experiment is:
\begin{verbatim}
y1 = rand(3,1);
y2 = rand(3,1);
y3 = rand(3,1);
A = [y1 y2 y3];
rref(A)
\end{verbatim}
If the resulting matrix is $I_3$, then the set is linearly
independent.

(b) The most likely outcome is that all five trials result in
linearly independent sets.

(c) Every trial yields a linearly dependent set of vectors.


\subsection*{Section~\protect{\ref{S:5.5}} Dimension and Bases}
\rhead{S:5.5}{DIMENSION AND BASES}

\exer{c5.5.1}  
By Theorem~\ref{basis=span+indep},
${\cal U}$ is a basis for $\R^3$ if the vectors of ${\cal U}$ are
linearly independent and span $\R^3$.  By Lemma~\ref{L:computerank},
the dimension of ${\cal U}$ is equal to the rank of the matrix whose
rows are $u_1$, $u_2$, and $u_3$.  Row reduce this matrix:
\[
\matthree{1}{1}{0}{0}{1}{0}{-1}{0}{1} \longrightarrow
\matthree{1}{0}{0}{0}{1}{0}{0}{0}{1}.
\]
So $\dim({\cal U}) = 3 = \dim(\R^3)$, and we need now only show that
$u_1$, $u_2$, and $u_3$ are linearly independent, which we can do by
row reducing the matrix whose columns are the vectors of ${\cal U}$ as
follows:
\[
\matthree{1}{0}{-1}{1}{1}{0}{0}{0}{1} \longrightarrow
\matthree{1}{0}{0}{0}{1}{0}{0}{0}{1}.
\]
Therefore, there is no nonzero solution to the equation
${\cal U}r = 0$, so the vectors of ${\cal U}$ are linearly independent
and ${\cal U}$ is a basis for $\R^3$.

\exer{c5.5.2}
\ans The dimension of $S$ is 2, and vectors $v_1$ and $v_2$ form a
basis for $S$.

\soln Row reduce the matrix $A$ whose rows are $v_1$, $v_2$, and $v_3$. 
By Lemma~\ref{extendindep}, the number
of nonzero rows in the reduced matrix is the dimension of $S$ and these
rows form a basis for $S$.  So:
\[
\left(\begin{array}{rrrr} 1 & 0 & -1 & 0 \\ 0 & 1 & 1 & 1 \\ 5
& 4 & -1 & 4 \end{array}\right) \longrightarrow \left(\begin{array}
{rrrr} 1& 0 & -1 & 0 \\ 0 & 1 & 1 & 1 \\ 0 & 0 & 0 & 0
\end{array}\right).
\]

\exer{c5.5.3}
\ans The vectors $(1,1,1,0)$ and $(-2,-2,0,1)$ form a basis for the
nullspace of $A$; therefore the dimension of the nullspace is $2$.

\soln Find the set of solutions to $Ax = 0$ by solving
\[
\left(\begin{array}{rrrr} 1 & 0 & -1 & 2 \\ 1 & -1 & 0 & 0 \\ 4
& -5 & 1 & -2 \end{array}\right) \left(\begin{array}{r} x_1 \\ x_2
\\ x_3 \\ x_4 \end{array}\right) = 0.
\]
To solve, row reduce $A$, obtaining
\[
\left(\begin{array}{rrrr} 1 & 0 & -1 & 2 \\ 0 & 1 & -1 & 2 \\ 0
& 0 & 0 & 0 \end{array}\right).
\]
So the set of solutions to $Ax = 0$ can be written
\[
\left(\begin{array}{r} x_1 \\ x_2 \\ x_3 \\ x_4
\end{array}\right) = \left(\begin{array}{c} x_3 - 2x_4 \\ x_3 - 2x_4
\\ x_3 \\ x_4 \end{array}\right) = x_3\left(\begin{array}{r} 1 \\ 1
\\ 1 \\ 0 \end{array}\right) + x_4\left(\begin{array}{r} -2 \\ -2
\\ 0 \\ 1 \end{array}\right).
\]

\exer{c5.5.4}
The set $V$ is a vector space because the operations of addition and
scalar multiplication satisfy the eight properties of
vector spaces described in Table~\ref{vectorspacelist}.  For $2 \times
2$ matrices, matrix addition is defined for two matrices such that:
\[
\mattwo{a_1}{b_1}{c_1}{d_1} + \mattwo{a_2}{b_2}{c_2}{d_2} =
\mattwo{a_1 + a_2}{b_1 + b_2}{c_1 + c_2}{d_1 + d_2}
\]
and scalar multiplication is defined for a matrix and a scalar such that:
\[
s\mattwo{a}{b}{c}{d} = \mattwo{sa}{sb}{sc}{sd}.
\]
So, using these definitions, addition is commutative and associative,
and the additive identity is the $2 \times 2$ matrix of zeroes.  If
\[
W = \mattwo{w_{11}}{w_{12}}{w_{21}}{w_{22}} \mbox{then}
\quad W^{-1} = \mattwo{-w_{11}}{-w_{12}}{-w_{21}}{-w_{22}}.
\]
Scalar multiplication is associative.  There is a multiplicative
identity, $I_2$, and scalar multiplication is distributive both
for scalars and for matrices.  So $V$ is a vector space.  One basis
for $V$ is
\[
\mattwo{1}{0}{0}{0}, \mattwo{0}{1}{0}{0}, \mattwo{0}{0}{1}{0}
\AND \mattwo{0}{0}{0}{1}.
\]

\para The set of $m \times n$ matrices is also a vector space, since
it also satisfies the eight properties of vector spaces.  In this
case, the additive identity is the $m \times n$ zero matrix, and
the multiplicative identity is $I_n$.  The dimension of the set is
$mn$, since one basis consists of the $mn$ matrices with $a_{ij}
= 1$ and all other entries $0$, for $1 \leq i \leq m$ and $1 \leq j
\leq n$.

\exer{c5.5.5}
The set $P_n$ is a subspace if it is closed under addition and
scalar multiplication.  Let $x(t) = a_0 + a_1t + \cdots +
a_nt^n$, $y(t) = b_0 + b_1t + \cdots + b_nt^n$ and $s \in \R$.
Then
\[
\begin{array}{l}
x(t) + y(t) = (a_0 + b_0) + (a_1 + b_1)t + \cdots + (a_n + b_n)t^n \in P_n.
\\
cx(t) = c(a_0 + a_1t + \cdots + a_nt^n) = ca_0 + ca_1t + \cdots + ca_nt^n
\in P_n.
\end{array}
\]
The dimension of $P_2$ is 3, since $x_1 = 1$, $x_2 = t$, and
$x_3 = t^2$ form a basis for $P_2$.  The dimension of $P_n$ is
$n + 1$.

\exer{c5.5.6}
First, note that the dimension of ${\cal P}^3$ is $4$.  We can show
this by noting that the $4$ polynomials $b_1(t) = t^3$, $b_2(t) =
t^2$, $b_3(t) = t$, and $b_4(t) = 1$ are linearly independent and
span ${\cal P}^3$.  The dimension of a space is equal to the number of
linearly independent vectors in a spanning set for that space.
Therefore, $\{p, \frac{dp}{dt}, \frac{d^2p}{dt^2}, \frac{d^3p}{dt^3}\}$
is a basis if the polynomials are linearly independent.

\para The general polynomial of degree $3$ has the form $q(t) =
a_3t^3 + a_2t^2 + a_1t + a_0$.  We can identify $q(t)$ by the vector 
$(a_3,a_2,a_1,a_0)$.  Thus, the set
\[
\begin{array}{rcl}
\dps p(t) & = & t^3 + a_2t^2 + a_1t + a_0 \\
\dps \frac{dp}{dt}(t) & = & 3t^2 + 2a_2t + a_1 \\
\dps \frac{d^2p}{dt^2}(t) & = & 6t + 2a_2 \\
\dps \frac{d^3p}{dt^3}(t) & = & 6 \end{array}
\]
is identified with the matrix $A$, whose columns are 
$\frac{dp}{dt}$, $\frac{d^2p}{dt^2}$, and $\frac{d^3p}{dt^3}$:
\[ A = \left(\begin{array}{cccc} 1 & 0 & 0 & 0 \\
a_2 & 3 & 0 & 0 \\ a_1 & 2a_2 & 6 & 0 \\ a_0 & a_1 & 2a_2 & 6
\end{array}\right). \]
The matrix is lower triangular and therefore row reduces to $I_4$.  So
this set of polynomials is linearly independent and spans ${\cal P}^3$.

\exer{c5.5.7}
(a) The matrix $A$ is symmetric because, by \Ref{e:transposeprod},
\[
A^t = (u^tu)^t = u^tu = A.
\]
To show that $\rank(A) = 1$, let $v$ be a vector such that $u \cdot
v = 0$.  This implies $uv^t = 0$, and thus $Av^t = u^t(uv^t) = 0$
for all vectors $v$.  The span of vectors perpendicular to $u$ has
dimension $n - 1$, so $\null(A) \geq n - 1$.  We know that $uu^t
= u \cdot u = ||u||$.  Thus, $Au^t = u^tuu^t = ||u||u^t$.  Since
$u$ is a nonzero vector, $||u|| \neq 0$, so $Au^t$ is a nonzero
multiple of $u^t$.  Therefore, $A$ is not the zero matrix, so
$\null(A) = n - 1$, and therefore $\rank(A) = 1$.

(b) The matrix $P$ is invertible if $P$ is row equivalent to $I_n$,
that is, if $\rank(P) = n$.  To show that this is true, again let $v$
be any vector perpendicular to $u$.  Then:
\[
Pv^t = (I_n + u^tu)v^t = v^t + 0 = v^t.
\]
Thus, $\rank(P) \geq n - 1$.  In addition
\[
Pu^t = (I_n + u^tu)u^t = u^t + ||u||u^t = (1 + ||u||)u^t
\]
which, since $||u|| > 0$, is a nonzero multiple of $u^t$.  Therefore,
$\rank(P) = n$ and $P$ is invertible.


\subsection*{Section~\protect{\ref{S:5.6}} The Proof of the Main Theorem}
\rhead{S:5.6}{THE PROOF OF THE MAIN THEOREM}

\exer{c5.7.1a}
\ans The span of $v_1$ and $v_2$ is a plane with normal vector
$N = n_3(-\frac{3}{2}, 1, 1)$, where $n_3$ is a nonzero scalar.

\soln If $v_1$ and $v_2$ are linearly independent, then they span a plane
in $\R^3$.  If they are linearly dependent, that is, if $v_1 =
\alpha v_2$ for some scalar $\alpha$, then they span a line in $\R^3$.
In this case, there is no scalar $\alpha$ such that $(2,1,2) =
\alpha(0,-1,1)$, so the span of $v_1$ and $v_2$ has dimension two.
The vector $N = (n_1,n_2,n_3)$ is found by observing that:
\[
\begin{array}{rrrrrcl}
2n_1 & + & n_2 & + & 2n_3 & = & 0 \\
& & -n_2 & + & n_3 & = & 0 \end{array}
\]
which is a linear system in two equations.  Solve for $N$ by row
reducing the corresponding matrix:
\[
\left(\begin{array}{rrr} 2 & 1 & 2 \\ 0 & -1 & 1 \end{array}\right)
\longrightarrow \left(\begin{array}{rrr} 1 & 0 & \frac{3}{2} \\ 0 &
1 & -1 \end{array}\right).
\]

\exer{c5.7.1b} \ans The subspace spanned by $v_1$ and $v_2$ is a line,
since, if $\alpha = -\frac{1}{2}$, then $(2,1,-1) = \alpha(-4,-2,2)$.

\exer{c5.7.1c} \ans The span of $v_1$ and $v_2$ is a plane with normal
vector $N = n_3(0,0,1)$, where $n_3$ is a nonzero scalar.

\soln There is no scalar $\alpha$ such that $(0,1,0) = \alpha(4,1,0)$. 
Let $N = (n_1,n_2,n_3)$ be the vector perpendicular to the plane.  Then:
\[
\begin{array}{rrrrrcl}
& & n_2 & & & = & 0 \\
4n_1 & + & n_2 & & & = & 0 \end{array}
\]
Solve for $N$ by substitution to find that $n_1 = n_2 = 0$, and
$n_3$ can be any nonzero real scalar.

\exer{c5.7.2}
\ans The intersection of the planes is $P \cap Q = s(1,-1,0)$ for any
real scalar $s$.

\soln The planes $P$ and $Q$ are not equal if the normal vectors $P_N$
and $Q_N$ point in different directions.  Solving by row reduction
yields $P_N = (-1,-1,1)$ and $Q_N = (0,0,1)$, so $P \neq Q$.

\para Since $P$ and $Q$ are not the same plane and also are not
parallel, they intersect in a line.  The intersection $P \cap Q$ is
the simultaneous solutions to the equations for planes $P$ and $Q$,
that is:
\[ \begin{array}{rrrrrrl}
-x & - & y & + & z & = & 0 \\
& & & & z & = & 0. \end{array} \]
Solve by row reduction or substitution to obtain $x = -y$ and $z = 0$.

\exer{c5.6.1}
(a) The largest value that $r$ can have is $5$, since the matrix has
$5$ columns.  Thus, the reduced echelon form matrix can have at most
$5$ pivot points.

(b) The equation $Ax = b$ has a solution if the rank of the augmented
matrix $(A|b)$ is $r$.  If $\rank (A|b)$ is greater than $r$, then
there is a pivot in the $6^{th}$ column and the system is
inconsistent, so there is no solution.

(c) The null space has dimension $5 - r$.

(d) The number of parameters needed to describe the solution to
$Ax = b$ is $5 - r$, since $5 - r$ parameters are needed to describe
the solutions to $Ax = 0$, and the solutions to the inhomogeneous
system are obtained by adding the solutions of the homogeneous system
to one solution of the inhomogeneous system.


\exer{c5.6.2}
\ans (a) The vectors $(1,2,3)$ and $(3,1,4)$ form a basis for the subspace
	$\CC$ of $\R^3$ spanned by the columns of $A$.

(b) The vectors $(1,3,-1,4)$ and $(2,1,5,7)$ form a basis for the subspace
	$\RR$ of $\R^4$ spanned by the rowss of $A$.

(c) $\dim \CC = \dim \RR$.

\soln (a) Note that 
\begin{eqnarray*} 
\vecthree{-1}{5}{4} = \frac{16}{5}\vecthree{1}{2}{3} 
	- \frac{7}{5}\vecthree{3}{1}{4} \\ 
\vecthree{4}{7}{11} = \frac{17}{5}\vecthree{1}{2}{3} 
	+ \frac{1}{5}\vecthree{3}{1}{4}
\end{eqnarray*}
So the two vectors $(1, 2, 3)$ and $(3,1,4)$ span $\CC$.  Since they are 
linearly independent, these vectors are a basis for $\CC$ and $\dim =\CC=2$.

(b) Note that 
\[
(3,4,4,11) = (1,3,-1,4) + (2,1,5,7).
\]
Therefore, $\{(1,3,-1,4),(2,1,5,7)\}$ is a basis for $\mathcal{R}$ and 
$\dim \mathcal{R}=2$


\exer{c5.6.3}
There is no scalar $\alpha$ such that $(2,3,1) = \alpha(1,1,3)$, so
$v_1$ and $v_2$ are linearly independent.
The set of linear combinations of $v_1$ and $v_2$ is the set of
solutions to
\[
\begin{array}{rrrrrrl}
2a_1 & + & 3a_1 & + & a_3 & = & 0 \\
a_1 & + & a_2 & + & 3a_3 & = & 0. \end{array}
\]
Row reducing this system yields $a_1 = -8a_3$ and $a_2 = 5a_3$.
Let $a_3 = 0$.  Then every linear combination of $v_1$ and $v_2$ is
of the form
\[
8x_1 - 5x_2 - x_3 = 0
\]
which is the equation of a plane in $\R^3$.  The normal vector to this
plane is $N = (8,-5,-1)$.  Row reduce the matrix whose columns
are the vectors $v_1$, $v_2$, and $N$ to verify that these vectors
are linearly independent.
\[
\matthree{2}{1}{8}{3}{1}{-5}{1}{3}{-1} \longrightarrow
\matthree{1}{0}{0}{0}{1}{0}{0}{0}{1}.
\]
So the vectors are indeed linearly independent, verifying
Lemma~\ref{extendindep}.

\exer{c5.6.3A}  Let $W$ be an infinite dimensional subspace of the vector 
space $V$.  We want to show that $V$ is infinite dimensional.  Suppose that
$V$ is finite dimensional with $\dim V=n$.  Then Corollary~\ref{basis<n}
states that any set of linear independent 
vectors in $V$ has at most $n$ vectors.  Since $W$ is infinite dimensional, 
there exists a linearly independent set of vectors in $W\subset V$ with more 
than $n$ vectors.  This is a contradiction and $V$ must be infinite 
dimensional.

\exer{c5.6.4}
(a) \ans The span has dimension $3$ for $\lambda \neq 2$, and 
the set $\{w_1,w_2,w_3\}$ is a basis for $\R^3$.

\soln Find the dimension of the span by creating a matrix with rows
$w_1$, $w_2$, $w_3$, and $w_4$, then row reducing:
\begin{equation} \label{exeq:5.6.4}
\left(\begin{array}{rrr} 2 & -2 & 1 \\ -1 & 2 & 0 \\ 3 & -2 &
\lambda \\ -5 & 6 & -2 \end{array}\right) \longrightarrow
\left(\begin{array}{rrc} 1 & -1 & \frac{1}{2} \\ 0 & 1 & \frac{1}{2}
\\ 0 & 0 & \lambda - 2 \\ 0 & 0 & 0 \end{array}\right).
\end{equation}
If $\lambda = 2$, then the dimension of the span will
be $2$ and if $\lambda \neq 2$, then the dimension of the span
will be $3$.  For example, let $\lambda = -1$.

\para Verify by row reduction that the set $\{w_1,w_2,w_3\}$ is a basis
for $\R^3$ and that the set $\{w_1,w_2,w_4\}$ is not a basis for $R^3$. 

(b) If $\lambda = 2$, then the dimension of span$\{w_1,w_2,w_3,w_4\}$
is $2$, as shown by equation~\Ref{exeq:5.6.4}.



\exer{c5.6.5}
Here is a sample \Matlab output for this problem.  Type:
\begin{verbatim}
x1 = rand(5,1);
x2 = rand(5,1);
x3 = rand(5,1);
x4 = rand(5,1);
x5 = rand(5,1);
\end{verbatim}
A summary of the results is:
\begin{verbatim}
x1 =          x2 =          x3 =          x4 =          x5 =
    0.9501        0.7621        0.6154        0.4057        0.0579
    0.2311        0.4565        0.7919        0.9355        0.3529
    0.6068        0.0185        0.9218        0.9169        0.8132
    0.4860        0.8214        0.7382        0.4103        0.0099
    0.8913        0.4447        0.1763        0.8936        0.1389
\end{verbatim}
The command {\tt A = [x1 x2 x3 x4 x5]} creates the matrix with columns
$x_1,...,x_5$.  Type {\tt rref(A)} to verify that the vectors are
linearly independent.  The following steps display the vector
$b = (2,1,3,-2,4)$ as a linear combination of $x_1,...,x_5$.  Type:
\begin{verbatim}
b = [2;1;3;-2;4];
C = [A b];
rref(C)
\end{verbatim}
which yields:
\newpage
\begin{verbatim}
ans =
    1.0000         0         0         0         0   28.7614
         0    1.0000         0         0         0  -96.6468
         0         0    1.0000         0         0   70.9112
         0         0         0    1.0000         0   30.0838
         0         0         0         0    1.0000 -129.8826
\end{verbatim}
So, in this example, $b = 28.7614x_1 - 96.6468x_2 + 70.9112x_3
+ 30.0838x_4 - 129.8826x_5$.

\exer{c5.6.6}
\ans The vectors $(1,0,0,-\frac{1}{2},\frac{3}{2})$, $(0,1,0,\frac{1}{2},
-\frac{1}{2})$, and $(0,0,1,\frac{1}{2},\frac{3}{2})$ form a basis
for the subspace spanned by $u_1, \dots ,u_5$.

\soln Row reduce the matrix {\tt M}, whose
rows are $u_1$, $u_2$, $u_3$, $u_4$ and $u_5$.  According to 
Lemma~\ref{extendindep}, the rows of the
reduced echelon matrix form a basis for $\{u_1,\dots ,u_5\}$.  The
command {\tt rref(M)} yields:
\begin{verbatim}
ans =
    1.0000         0         0   -0.5000    1.5000
         0    1.0000         0    0.5000   -0.5000
         0         0    1.0000    0.5000    1.5000
         0         0         0         0         0
         0         0         0         0         0
\end{verbatim}











