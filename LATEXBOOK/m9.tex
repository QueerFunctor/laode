\documentclass{ximera}

\input{./preamble.tex}

\title{m9.tex}

\begin{document}
\begin{abstract}
BADBAD
\end{abstract}
\maketitle

\chapter{Linear Maps and Changes of Coordinates}

\subsection*{Section~\protect{\ref{Sect:linmap}} Linear Mappings and Bases}
\rhead{Sect:linmap}{LINEAR MAPPINGS AND BASES}

\exer{c7.2.1}
Compute $A$, the matrix of $L$, using Equation~\Ref{e:defA}:
\[ A = (w_1^t|w_2^t|w_3^t)(v_1^t|v_2^t|v_3^t)^{-1} =
\left(\begin{array}{rrr} -1 & 0 & 3 \\ 0 & 1 & 1 \end{array}\right)
\matthree{1}{2}{-2}{0}{-1}{1}{2}{1}{0}^{-1} =
\left(\begin{array}{rrr} -7 & -11 & 3 \\ -4 & -7 & 2
\end{array}\right). \]

\exer{c7.2.2}
To show that the set $\{1,t,t^2,\dots,t^n\}$ is a basis for
${\cal P}_n$, we must show that the $n + 1$ polynomials are
linearly independent and span ${\cal P}_n$.  The polynomials are
independent because the general polynomial of degree $n$:
\[
\alpha_1 + \alpha_2t + \alpha_3t^2 + \cdots + \alpha_{n+1}t^n
\]
is identically $0$ for all values of $t$ only when $\alpha_1 =
\alpha_2 = \cdots = \alpha_{n + 1} = 0$.  The polynomials span
${\cal P}_n$ because every polynomial $p(t)$ of degree $n$ has
the form
\[ p(t) = \beta_1 + \beta_2t + \cdots + \beta_{n + 1}t^n \]
which is a linear combination of the polynomials
$\{1,t,t^2,\dots,t^n\}$ for any $p(t)$ in ${\cal P}_n$.

\exer{c7.2.2a}
Let $\frac{d}{dt}$ be a transformation that maps $p(t) \mapsto
\frac{d}{dt}p(t)$.  For $p(t) =  p_1 + p_2t + p_3t^2 + p_4t^3$, then
$\frac{d}{dt}p(t) = p_2 + p_3t + p_4t^2$, so $\frac{d}{dt}$ is indeed
a mapping ${\cal P}_3 \rightarrow {\cal P}_2$.  From calculus, we
know that, for any functions $f$ and $g$:
\[ \frac{d}{dt}(f + g)(t) = \frac{d}{dt}f(t) + \frac{d}{dt}g(t), \]
and that, for any scalar $c$:
\[ \frac{d}{dt}(cf)(t) = c\frac{d}{dt}f(t). \]
Let $f$ and $g$ be elements of ${\cal P}_3$.  Then
$\frac{d}{dt}: {\cal P}_3 \rightarrow {\cal P}_2$ is a linear mapping.

\exer{c7.2.2b}
Let $p(t) = p_1 + p_2t + p_3t^2$.  Then the transformation $L$
maps $p(t) \mapsto L(p(t)) = p_1t + \frac{1}{2}p_2t^2 +
\frac{1}{3}p_3t^3$, so $L$ is indeed a mapping ${\cal P}_2
\rightarrow {\cal P}_3$.  We know from calculus that, for any
functions $f$ and $g$:
\[ \int_0^t(f + g)(t) = \int_0^tf(t) + \int_0^tg(t) \]
And, for any scalar $c \in \R$,
\[ \int_0^t(cf)(t) = c\int_0^tf(t). \]
Let $f$ and $g$ be elements of ${\cal P}_2$.  Then $L$ is a linear
mapping.

\exer{c7.2.2c}
Let $M = \frac{d}{dt} \circ L$ be a mapping ${\cal P}_2
\rightarrow {\cal P}_2$.  The fundamental
theorem of calculus states that, for any function $g$,
\[ \frac{d}{dt}\int_0^t g(\tau)d\tau = g(t). \]
Thus $M(g) = g$ is valid for all values of $g$, so $M$ is the
identity map.

\para To prove this fact explicitly for this case, note that $M$ is
the identity mapping if it maps every polynomial in ${\cal P}_2$ to
itself.  Lemma~\ref{L:linmapfrombasis} states that this mapping can
be uniquely determined by a basis of ${\cal P}_2$.  According to
Exercise~\ref{c7.2.2}, $\{1,t,t^2\}$ is a basis for ${\cal P}_2$. 
Therefore, $M$ is the identity map if $M(1) = 1$, $M(t) = t$ and
$M(t^2) = t^2$:
\[ \frac{d}{dt} \circ L (1) = \frac{d}{dt}\left(\int_0^tds\right) =
\frac{d}{dt}(t) = 1. \]
\[ \frac{d}{dt} \circ L (t) = \frac{d}{dt}\left(\int_0^tsds\right) =
\frac{d}{dt}\left(\frac{t^2}{2}\right) = t. \]
\[ \frac{d}{dt} \circ L (t^2) = \frac{d}{dt}\left(\int_0^ts^2ds\right) =
\frac{d}{dt}\left(\frac{t^3}{3}\right) = t^2. \]
So $\frac{d}{dt} \circ L$ is indeed the identity map for ${\cal P}_2$.

\exer{c7.2.3}
The space $\C$ is a two dimensional real vector space since every
element of $\C$ can be written as $c = \sigma + \tau i$, a linear
combination of the linearly independent two dimensional set $\{1,i\}$.

\para Let $z_1$ and $z_2$ be elements of $\R$.  Then,
\[ L(z_1 + z_2) = \lambda(z_1 + z_2) =
\lambda z_1 + \lambda z_2 = L(z_1) + L(z_2). \]
For any real scalar $c$,
\[ L(cz_1) = \lambda(cz_1) = 
c\lambda z_1 = cL(z_1). \]
Therefore, $L(z) = \lambda z$ is a linear mapping.

\newpage
\exer{c7.2.4}
Let $X$ and $Y$ be elements of ${\cal M}(n)$.  Then,
\[ L(X + Y) = A(X + Y) - (X + Y)A = (AX - XA)
+ (AY - YA) = L(X) + L(Y). \]
For any real scalar $c$,
\[ L(cX) = A(cX) - (cX)A = c(AX) - c(XA) = c(AX - XA) = cL(X). \]
Therefore, $L$ is a linear mapping.

\para The null space of $L$ consists of all matrices $X$ such that
$AX - XA = 0$, or $AX = XA$.  By definition, these are the matrices
such that $X$ commutes with $A$.  Let $X$ and $Y$ be elements of
the null space of $L$.  Then show that $X + Y$ is in the null space
by calculating:
\[ L(X + Y) = A(X + Y) - (X + Y)A = AX + AY - XA - YA = AX + AY - AX
- AY = 0. \]
Show that, for any real scalar $c$, $cX$ is in the null space by
calculating:
\[ L(cX) = A(cX) - (cX)A = cAX - cXA = cAX - cAX = 0. \]
Therefore, the null space of $L$ is a subspace consisting of all
matrices that commute with $A$.

\exer{c7.2.5}
Let $f$ and $g$ be functions in ${\cal C}^1$.  Then,
\[ \begin{array}{rcl}
 L(f + g) & = & \int_0^{2\pi}(f(t) + g(t))\cos(t)dt \\
& = & \int_0^{2\pi}(f(t)\cos(t)dt + g(t)\cos(t)dt) \\
& = & \int_0^{2\pi}f(t)\cos(t)dt + \int_0^{2\pi}g(t)\cos(t)dt \\
& = & L(f) + L(g). \end{array} \]
For any real scalar $c$,
\[ L(cf) = \int_0^{2\pi}cf(t)\cos(t)dt = c\int_0^{2\pi}f(t)\cos(t)dt
= cL(f). \]
So $L$ is a linear mapping.

\exer{c7.2.6}
Let $p$ and $q$ be elements of ${\cal P}$.  Then,
\[ \begin{array}{rcl}
L(p + q)(x) & = & \int_0^x(t - 1)(p(t) + q(t))dt \\
& = & \int_0^x((t - 1)p(t)dt + (t - 1)q(t)dt) \\
& = & \int_0^x(t - 1)p(t)dt + \int_0^x(t - 1)q(t)dt \\
& = & L(p(x)) + L(q(x)). \end{array} \]
For any real scalar $c$,
\[ L(cp(x)) = \int_0^x(t - 1)cp(t)dt = c\int_0^x(t - 1)p(t)dt
= cL(p(x)). \]
Therefore, $L$ is a linear mapping.



\newpage
\subsection*{Section~\protect{\ref{S:5.8}} Row Rank Equals Column Rank}
\rhead{S:5.8}{ROW RANK EQUALS COLUMN RANK}

\exer{c5.8.1}
\ans The possible choices for the scalars $\alpha_j$ are
$\alpha = (\alpha_1,\alpha_2,\alpha_3) = \alpha_3(-1,-1,1)$ and
the possible choices for the scalars $\beta_j$ are $\beta = 
(\beta_1,\beta_2,\beta_3) = \beta_3(-\frac{7}{5},-\frac{9}{5},1)$.

\soln Find $A^t$ and solve by row reduction the equation
$A^t\alpha = 0$.  To find the scalars $\beta_j$, solve $A\beta =
0$.  These equations yield
\[
-r_1 - r_2 + r_3 = 0 \AND -7c_1 - 9c_2 + 5c_3 = 0.
\]

\exer{c5.8.2} The largest row rank that a $5 \times 3$ matrix can have
is $3$, since, by Theorem~\ref{T:rowrank=columnrank} the row rank is
equal to the column rank, and the matrix has $3$ columns.

\exer{c5.8.3}
(a) \ans The vectors $(1,0,1,0)$, $(0,1,-1,0)$ and $(0,0,0,1)$ form a
basis for the row space of $A$, and the row rank of $A$ is $3$.

\soln Row reduce $A$:
\[
\left(\begin{array}{rrrr} 1 & 1 & 0 & 1 \\ 0 & -1 & 1 & 2 \\1 & 2
& -1 & 3 \end{array}\right) \longrightarrow \left(\begin{array}{rrrr}
1 & 0 & 1 & 0 \\ 0 & 1 & -1 & 0 \\0 & 0 & 0 & 1 \end{array}\right).
\]

(b) \ans The column rank of $A$ is $3$, and the vectors $(1,0,0)$,
$(0,1,0)$, and $(0,0,1)$ form a basis for the column space of $A$.

\soln Row reduce $A^t$:
\[
\left(\begin{array}{rrr} 1 & 0 & 1 \\ 1 & -1 & 2 \\ 0 & 1 & -1
\\ 1 & 2 & 3 \end{array}\right) \longrightarrow \left(\begin{array}{rrr}
1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{array}\right)
\]

(c) \ans The vector $(-1,1,1,0)$ is a basis for the null space.  Since one
vector forms the basis, the nullity of $A$ is $1$.  

\soln Solve $Ax = 0$ by row reducing $A$, which we have already done.

(d) \ans The null space is trivial and the nullity of $A^t$ is $0$.

\soln Find a basis by solving $A^tx = 0$ by row reduction.  The row
reduced matrix:
\[
\left(\begin{array}{rrr} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1
\\ 0 & 0 & 0 \end{array}\right)
\]
implies $x = (0,0,0)$.


\exer{c5.8.4}
Since $A$ is a nonzero $3 \times 3$ matrix, $\rank(A)$ can equal $1$,
$2$, or $3$.  If $\rank(A) = 3$, then $A$ is invertible, so there
exists a matrix $B$ such that $AB = I_3$.  Then $A^2B^2 = AABB = I_3$,
so $\rank(A^2) = 3$, which contradicts the assumption that $A^2 = 0$.
Therefore, $\rank(A) \neq 3$.

\para Let $\rank(A) = 2$ and let $v_1$ and $v_2$ be linearly independent
vectors such that $Av_1 \neq 0$ and $Av_2 \neq 0$.  By
Theorem~\ref{T:dimsoln}, the nullity of $A$ is $1$.  However,
$A^2v = 0$ for all vectors $v$.  In particular,
\[
A^2v_1 = A(Av_1) = 0 \AND A^2v_2 = A(Av_2) = 0.
\]
Since there are linearly independent vectors $Av_1$ and $Av_2$ such
that $A(Av_1) = A(Av_2) = 0$, $\null(A) \geq 2$, contradicting
Theorem~\ref{T:dimsoln}.  Thus, $\rank(A) \neq 2$, so, by elimination
$\rank(A) = 1$.

\exer{c5.8.5}
We show that $\rank(A) \leq \min\{\rank(B),\rank(C)\}$ by noting that,
if $A = BC$, then the columns of $A$ are linear combinations of the
columns of $B$, so the span of the column space of $A$ cannot exceed
the span of the column space of $B$.  Therefore, $\rank(A) \leq
\rank(B)$.  Next, note that $A^t = C^tB^t$.  By a similar argument,
$\rank(A^t) \leq \rank(C^t)$.  Since $\rank(A) = \rank(A^t)$,
$\rank(A) \leq \min\{\rank(B),\rank(C)\}$.

\exer{c5.8.6}
(a) \ans The vectors $(1,0,0,\frac{1}{12})$, $(0,1,0,\frac{3}{4})$,
and $(0,0,1,\frac{7}{12})$ form a basis for the row space of $A$ and
$\rank(A) = 3$.

\soln Row reducing {\tt A} in \Matlab yields:
\begin{verbatim}
ans =
    1.0000         0         0    0.0833
         0    1.0000         0    0.7500
         0         0    1.0000    0.5833
         0         0         0         0
\end{verbatim}

(b) \ans The vectors $(1,0,0,1)$, $(0,1,0,2)$, and $(0,0,1,-1)$ form a
basis for the column space of $A$.

\soln Row reduce $A^t$ in \Matlab with the command {\tt rref(A')} to obtain
\begin{verbatim}
ans =
      1            0            0            1      
      0            1            0            2      
      0            0            1           -1      
      0            0            0            0      
\end{verbatim}

(c) \ans $Ax = 0$ when $x = s(-\frac{1}{12},-\frac{3}{4}, -\frac{7}{12},1)$.

\soln The solutions to the homogeneous system $Ax = 0$ can be found
using the row reduced matrix $A$, which we computed in part (a).

(d) \ans The vector $(4,2,2,1)$ is not in the span of the columns of
$A$.

\soln Row reducing the augmented matrix
\[
\left(\begin{array}{rrrr|r} 1 & 1 & 2 & 2 & 4 \\ 0 & -1 & 3 &
1 & 2 \\ 2 & -1 & 1 & 0 & 2 \\ -1 & 0 & 7 & 4 & 1 \end{array}\right)
\]
in \Matlab yields
\begin{verbatim}
ans =
      1            0            0           1/12          0      
      0            1            0           3/4           0      
      0            0            1           7/12          0      
      0            0            0            0            1      
\end{verbatim}
Since there is a pivot point in the last column, the system is
inconsistent.



\subsection*{Section~\protect{\ref{S:coordinates}} Vectors and Matrices in
Coordinates}
\rhead{S:coordinates}{VECTORS AND MATRICES IN COORDINATES}

\exer{c7.1.1}
\ans $[v]_{\cal W} = (7,4)$.

\soln Find the scalars $\alpha_1$ and $\alpha_2$ such that $v = \alpha_1w_1
+ \alpha_2w_2$.  That is, solve the linear system
\[ \begin{array}{rrrrr}
\alpha_1 & - & 2\alpha_2 & = & -1 \\
4\alpha_1 & + & \alpha_2 & = & 32 \end{array} \]
to obtain $(\alpha_1,\alpha_2) = (7,4)$, the coordinates
of $v$ in the ${\cal W}$ basis.

\exer{c7.3.1}
From Section~\ref{S:coordinates},
\[
[L]_{\cal W} = (w_1^t|w_2^t)L(w_1^t|w_2^t)^{-1} =
\mattwo{1}{0}{2}{1}\mattwo{2}{1}{-1}{0}\mattwo{1}{0}{-2}{1} =
\mattwo{0}{1}{-1}{2}.
\]

\exer{c7.1.3}
(a) By Theorem~\ref{basis=span+indep},
the subset ${\cal V}$ is a basis for the vector space of $2 \times
3$ matrices if the vectors of ${\cal V}$ are linearly independent and
span the vector space.  Let
\[ B = \left(\begin{array}{rrr} b_{11} & b_{12} & b_{13} \\ b_{21} &
b_{22} & b_{23} \end{array}\right). \]
We show that $B$ is in the span of ${\cal V}$ by noting that
$B = b_{11}E_{11} + b_{12}E_{12} + b_{13}E_{13} + b_{21}E_{21}
+ b_{22}E_{22} + b_{23}E_{23}$.  To show that the matrices $E_{ij}$
are linearly independent, suppose $b_{11}E_{11} + b_{12}E_{12} +
b_{13}E_{13} + b_{21}E_{21} + b_{22}E_{22} + b_{23}E_{23} = 0$.  Then,
\[  B = \left(\begin{array}{rrr} b_{11} & b_{12} & b_{13} \\ b_{21} &
b_{22} & b_{23} \end{array}\right) = 0, \]
so $b_{ij} = 0$.
Therefore, {\cal V} is a basis for the given vector space.

(b) \ans $[A]_{\cal V} = (-1,0,2,3,-2,4)$.

\soln Compute $A = -E_{11} + 2E_{13} + 3E_{21} - 2E_{22} + 4E_{23}$.

\exer{c7.1.4}
\ans If $p(t) = t$, then
$[p]_{\cal V} = (\frac{4}{7}, -\frac{1}{7}, -\frac{2}{7})$.

\soln In order to verify that ${\cal V}$ is a basis for ${\cal P}_2$, first
show that the set $\{1,t,t^2\}$ is a basis for ${\cal P}_2$.  To prove
this, note that any polynomial in ${\cal P}_2$ can be written as
$p = \alpha_1 + \alpha_2t + \alpha_3t^2$, so the set spans ${\cal P}_2$.
Also, $0 = \alpha_1 + \alpha_2t + \alpha_3t^2$ if and only if
$\alpha_1 = \alpha_2 = \alpha_3 = 0$, so the set is linearly
independent.

\para The set $\{1,t,t^2\}$ has dimension 3 and is a basis for
${\cal P}_2$.  Therefore, any linearly independent set of three vectors
in ${\cal P}_2$ will span ${\cal P}_2$.  So we need only show that
${\cal V}$ is a linearly independent set, which we do by solving:
\[
\begin{array}{rcl}
0 & = & \alpha_1p_1(t) + \alpha_2p_2(t) + \alpha_3p_3(t) \\
& = & \alpha_1(1 + 2t) + \alpha_2(t + 2t^2) + \alpha_3(2 - t^2) \\
& = & (\alpha_1 + 2\alpha_3) + (2\alpha_1 + \alpha_2)t +
(2\alpha_2 - \alpha_3). \end{array}
\]
This equation is identically $0$ if
\[
\matthree{1}{2}{0}{2}{0}{1}{0}{2}{1}\vecthree{\alpha_1}{\alpha_2}
{\alpha_3} = 0.
\]
The only solution to this system is $\alpha_1 = \alpha_2 = \alpha_3
 = 0$, so the elements are linearly independent and ${\cal V}$ is
a basis for ${\cal P}_2$.

\para Let $p(t) = t$.  Then find this vector $[p]_{\cal V}$ by solving
$p(t) = \alpha_1p_1(t) + \alpha_2p_2(t) + \alpha_3p_3(t)$. 
That is,
\[ \vecthree{0}{1}{0} = \matthree{1}{0}{2}{2}{1}{0}{0}{2}{-1}
\vecthree{\alpha_1}{\alpha_2}{\alpha_3}. \]
Solve by substitution to obtain $\alpha_1 = \frac{4}{7}$, $\alpha_2
= -\frac{1}{7}$, and $\alpha_3 = -\frac{2}{7}$.

\exer{c7.1.6}
\ans $[v]_W = (-2,2,-1)$.

\soln Use \Matlab to row reduce the augmented matrix
$(w_1^t|w_2^t|w_3^t|v)$, obtaining:
\begin{verbatim}
ans = 
     1     0     0    -2
     0     1     0     2
     0     0     1    -1
\end{verbatim}

\exer{c7.1.7}
\ans $[v]_{\cal W} \approx (-58.3171, 79.7282, -25.6754, 10.2308)$.

\soln Using \Matlab, create the augmented matrix
{\tt [w1' w2' w3' w4' v']} and row reduce to obtain
\begin{verbatim}
ans =
    1.0000         0         0         0  -58.3171
         0    1.0000         0         0   79.7282
         0         0    1.0000         0  -25.6754
         0         0         0    1.0000   10.2308
\end{verbatim}

\exer{c7.3.4}
\ans The matrix $[L]_{\cal W}$ is diagonal in the basis:
\[
{\cal W} = \left\{\vectwo{1}{2},\vectwo{2}{3}\right\}
\]

\soln Theorem~\ref{T:putinform2} states
that the matrix $[L]_{\cal W}$ is diagonal if ${\cal W}$ consists of
eigenvectors of $L$ corresponding to real eigenvalues.  By
computation, we find that $\lambda_1 = 2$ and $\lambda_2 = -1$ are
the eigenvalues of $L$.  We then find that $Lw_1 = 2w_1$ when
$w_1 = (1,2)^t$ and $Lw_2 = -w_2$ when $w_2 = (2,3)^t$, so $w_1$ and
$w_2$ are the eigenvectors of $L$.

\exer{c7.3.5} \ans Let $L$ be the linear transformation with matrix $A$ in
the standard basis.  Then:
\[
[L]_{\cal W} = \frac{1}{41}\left(\begin{array}{rrrr} 92 & -21 & 55 &
-9 \\ -54 & 56 & -10 & -58 \\ 901 & 531 & 179 & 292 \\ 254 & 180 & 3 &
124 \end{array}\right).
\]

\soln Verify that ${\cal W}$ is a basis of $\R^4$ by noting that
four linearly independent vectors in $\R^4$ span $\R^4$ and
therefore form a basis.  So, row reduce the matrix
$P_{\cal W} = (w_1^t|w_2^t|w_3^t|w_4^t)$ to find that the vectors are indeed
linearly independent.

\para Use the formula $[L]_{\cal W} = P_{\cal W}^{-1}AP_{\cal W}$ to compute
\[
[L]_{\cal W} = \left(\begin{array}{rrrr} 1 & 0 & 2 & -1 \\ 2 & -1 & 0
& 1 \\ 3 & 1 & 0 & 3 \\ 4 & 3 & 1 & 0 \end{array}\right)^{-1} A
\left(\begin{array}{rrrr} 1 & 0 & 2 & -1 \\ 2 & -1
& 0 & 1 \\ 3 & 1 & 0 & 3 \\ 4 & 3 & 1 & 0 \end{array}\right).
\]
Use the {\tt format rational} command so that \Matlab displays
the matrix elements as fractions.


\subsection*{Section~\protect{\ref{MALT}} Matrices of Linear Maps on a
Vector Space}
\rhead{MALT}{MATRICES OF LINEAR MAPS ON A VECTOR SPACE}

\exer{c7.1.2}
\ans
\[ C_{\cal WZ} = \mattwo{2}{3}{-1}{-2}. \]

\soln Substitute into Equation~\Ref{e:coordformn} as follows:
\[ C_{\cal WZ} = (w_1^t|w_2^t)^{-1}(z_1^t|z_2^t) =
\mattwo{1}{0}{2}{1}^{-1}\mattwo{2}{3}{3}{4} =
\mattwo{2}{3}{-1}{-2}. \]

\exer{c7.3.2}
\ans \[ [L]_{\cal F} = \mattwo{0}{1}{-1}{0}. \]

\soln By Definition~\ref{D:matrixincoord}, the $j^{th}$
column of $[L]_{\cal F}$ is $[L(f_j)]_{\cal F}$.  In this case,
\[
L(f_1) = \frac{d}{dt}(\cos t) = -\sin t \AND
L(f_2) = \frac{d}{dt}(\sin t) = \cos t.
\]
The basis $\{f_1,f_2\}$ is $\{\cos t,\sin t\}$.  In this basis,
$-\sin t$ has coordinates $(0,-1)$, and $\cos t$ has coordinates
$(1,0)$.  Therefore, the $1^{st}$ column of $[L]_{\cal F}$ is
$(0,-1)$ and the $2^{nd}$ column is $(1,0)$.

\exer{c7.3.3}
If there exists a nonzero vector $v$ such that $M\circ L(v) = 0$,
then the nullity of $M \circ L$ is nonzero, so $M \circ L$ is not
invertible.  If $L(v) = 0$, then $M \circ L(v) = 0$.  We know that
$\rm{nullity}(L) = \dim V - \dim W > 0$.  Therefore, $M\circ L(v)$
is not invertible

\exer{c7.1.5}
\ans \Matlab gives the values for $[v]_{\cal W}$, $[v]_{\cal Z}$, and
$C_{\cal WZ}$ as:
\begin{verbatim}
vW =               vZ =                   CWZ =
    1.7137            -0.5200                -3.6480   0.1431
    1.2108            -1.2800                -3.2998   0.3946
\end{verbatim}

\soln In \Matlabp, find $[v]_{\cal W}$ by row reducing
the matrix $(w_1^t|w_2^t | v^t)$.  Row reduce 
$(z_1^t|z_2^t | v^t)$ to solve for $[v]_{\cal Z}$.  To find
$C_{\cal WZ}$, compute $(w_1^t|w_2^t)^{-1}(z_1^t|z_2^t)$.

\exer{c7.5.A}
(a) \ans The matrix $A$ fixes $w_1$, moves $w_2$ to $w_3$, and moves
$w_3$ to $-w_2$.

\soln Load the matrix and vectors into \Matlabp.  Then compute $Aw_j$ for
each vector $w_j$, obtaining $Aw_1 = w_1$, $Aw_2 = w_3$, and $Aw_3 = -w_2$.

(b) \ans $[L_A]_{\cal W} = \matthree{1}{0}{0}{0}{0}{-1}{0}{1}{0}$.

\soln From Section~\ref{S:coordinates}, we know that
\[
[L_A]_{\cal W} = P^{-1}AP,
\]
where $P = (w_1|w_2|w_3)$.  Enter {\tt P} into \Matlab and compute this
matrix.

(c) \ans The matrix $[L_A]_{\cal W}$ fixes $e_1$, moves $e_2$ to
$e_3$, and moves $e_3$ to $-e_2$.

\soln Compute $[L_A]_{\cal W}e_1 = e_1$, $[L_A]_{\cal W}e_2 = e_3$, and
$[L_A]_{\cal W}e_3 = -e_2$.  This result is consistent with part (a),
since $[L_A]_{\cal W}$ maps the standard basis vectors to one another
in the same way that $A$ maps the basis vectors of ${\cal W}$ to one
another.
\end{document}
