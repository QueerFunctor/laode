\documentclass{ximera}

\input{./preamble.tex}

\title{c5.tex}

\begin{document}
\begin{abstract}
BADBAD
\end{abstract}
\maketitle

\chapter{Vector Spaces}
\label{C:vectorspaces}

\normalsize

In Chapter~\ref{lineq} we discussed how to solve systems of $m$
linear equations in $n$ unknowns.  We found that solutions of
these equations are vectors $(x_1,\ldots,x_n)\in\R^n$.  
In Chapter~\ref{chap:matrices} we discussed how the notation of
matrices and matrix multiplication drastically simplifies the
presentation of linear systems and how matrix multiplication
leads to linear mappings.  We also discussed briefly how linear
mappings lead to methods for solving linear systems ---
superposition, eigenvectors, inverses.  In
Chapter~\ref{chap:SolveOdes} we discussed how to solve systems
of $n$ linear differential equations in $n$ unknown functions.
These chapters have
provided an introduction to many of the ideas of linear algebra
and now we begin the task of formalizing these ideas.

Sets having the two operations of vector addition and scalar
multiplication are called {\em vector spaces\/}.  This concept
is introduced in Section~\ref{S:5.1} along with the two
primary examples --- the set $\R^n$ in which solutions to systems
of linear equations sit and the set $\CCone$ of differentiable
functions in which solutions to systems of ordinary differential
equations sit.  Solutions to systems of homogeneous linear equations
form subspaces of $\R^n$ and solutions of systems of linear
differential equations form subspaces of $\CCone$.  These issues
are discussed in Sections~\ref{S:5.1} and \ref{S:5.2}.

When we {\em solve\/} a homogeneous system of equations, we write
every solution as a superposition of a finite number of specific
solutions.  Abstracting this process is one of the main points of this
chapter.  Specifically, we show that every vector in many commonly
occurring vector spaces
(in particular, the subspaces of solutions) can be written as a
{\em linear combination\/} (superposition) of a few 
solutions.  The minimum number of solutions needed is called
the {\em dimension\/} of that vector space.  Sets of vectors that 
generate all solutions by superposition and that consist of that minimum 
number of vectors are called {\em bases\/}.  These ideas are discussed
in detail in Sections~\ref{S:5.3}--\ref{S:5.5}.  The proof of
the main theorem (Theorem~\ref{basis=span+indep}), which gives a
computable method for determining when a set is a basis, is given in
Section~\ref{S:5.6}.  This proof may be omitted on a first reading,
but the statement of the theorem is most important and must be
understood.


\section{Vector Spaces and Subspaces} \label{S:5.1}

Vector spaces abstract the arithmetic properties of addition and
scalar multiplication of vectors.  In $\R^n$ we know how to add
vectors and to multiply vectors by scalars.  Indeed, it is
straightforward to verify that each of the eight
properties listed in Table~\ref{vectorspacelist} is valid for
vectors in $V=\R^n$.  Remarkably, sets that satisfy these eight
properties have much in common with $\R^n$.  So we define:
\begin{Def}  \label{vectorspace}
Let $V$ be a set having the two operations of addition and scalar
multiplication.  Then $V$ is a {\em vector space\/} if the eight
properties listed in Table~\ref{vectorspace} hold.  The elements
of a vector space are called {\em vectors}\index{vector}.
\end{Def} \index{vector!space}\index{vector!space}

The vector $0$ mentioned in (A3) in Table~\ref{vectorspacelist}
is called the {\em zero vector}\index{zero vector}.

\begin{table}
\caption{Properties of Vector Spaces: suppose $u,v,w\in V$ and
$r,s\in\R$.}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
(A1) & Addition is commutative & $v+w=w+v$ \\
\hline
(A2) & Addition is associative & $(u+v)+w = u+(v+w)$ \\
\hline
(A3) & Additive identity $0$ exists & $v+0=v$ \\
\hline
(A4) & Additive inverse $-v$ exists & $v+(-v) = 0$ \\
\hline
(M1) & Multiplication is associative & $(rs)v = r(sv)$ \\
\hline
(M2) & Multiplicative identity exists & $1v=v$ \\
\hline
(D1) & Distributive law for scalars & $(r+s)v = rv+sv$ \\
\hline
(D2) & Distributive law for vectors & $r(v+w) = rv+rw$ \\
\hline
\end{tabular}
\end{center} \index{scalar multiplication} \index{vector!addition}
\index{associative} \index{distributive}
\index{commutative} \index{inverse}
\label{vectorspacelist}
\end{table}

When we say that a vector space $V$ has the two operations of
addition and scalar multiplication we mean that the sum of two
vectors in $V$ is again a vector in $V$ and the scalar product
of a vector with a number is again a vector in $V$.  These two
properties are called {\em closure under addition\/} and
{\em closure under scalar multiplication}.
\index{closure!under addition}
\index{closure!under scalar multiplication}

In this discussion we focus on just two types of vector spaces:
$\R^n$ and function spaces.  The reason that we make this choice
is that solutions to linear equations are vectors in $\R^n$ while
solutions to linear systems of differential equations are vectors
of functions.

\subsubsection*{An Example of a Function Space}
\index{function space}

For example, let ${\cal F}$ denote the set of all functions $f:\R\to\R$.
Note that functions like $f_1(t)=t^2-2t+7$ and $f_2(t)=\sin t$ are in
${\cal F}$ since they are defined for all real numbers $t$, but that functions
like $g_1(t)=\frac{1}{t}$ and $g_2(t)=\tan t$ are not in ${\cal F}$ since
they are not defined for all $t$.

We can add two functions $f$ and $g$ by defining the function $f+g$ to be:
\[
(f+g)(t) = f(t) + g(t).
\]
We can also multiply a function $f$ by a scalar $c\in\R$ by defining the
function $cf$ to be:
\[
(cf)(t) = cf(t).
\]
With these operations of addition and scalar multiplication, ${\cal F}$
is a vector space; that is, ${\cal F}$ satisfies the eight vector space
properties in Table~\ref{vectorspacelist}.  More precisely:
\begin{itemize}
\item[(A3)] Define the zero function ${\cal O}$ by
\[
     {\cal O}(t) = 0 \quad \mbox{for all $t\in\R$.}
\]
For every $x$ in ${\cal F}$ the function ${\cal O}$ satisfies:
\[
     (x+{\cal O})(t) = x(t) + {\cal O}(t) = x(t) + 0 = x(t).
\]
Therefore, $x+{\cal O}=x$ and ${\cal O}$ is the additive
identity in ${\cal F}$.
\item[(A4)] Let $x$ be a function in ${\cal F}$ and define $y(t)=-x(t)$.
Then $y$ is also a function in ${\cal F}$, and
\[
    (x+y)(t) =  x(t) + y(t) = x(t) + (-x(t)) = 0 = {\cal O}(t).
\]
Thus, $x$ has the additive inverse $-x$.
\end{itemize}
After these comments it is straightforward to verify that the
remaining six properties in Table~\ref{vectorspacelist} are
satisfied by functions in ${\cal F}$.

\subsubsection*{Sets that are not Vector Spaces}
\index{vector!space}\index{vector!space}

It is worth considering how closure under vector addition and
scalar multiplication can fail.  Consider the following three
examples.

\begin{enumerate}
\item[(i)] Let $V_1$ be the set that consists of just the $x$
and $y$ axes in the plane.  Since $(1,0)$ and $(0,1)$ are in
$V_1$ but
\[
(1,0) + (0,1) = (1,1)
\]
is not in $V_1$, we see that $V_1$ is not closed under vector
addition. On the other hand, $V_1$ is closed under scalar
multiplication.  \index{closure!under addition}
\index{closure!under scalar multiplication}


\item[(ii)] Let $V_2$ be the set of all vectors $(k,\ell)\in\R^2$
where $k$ and $\ell$ are integers.  The set $V_2$ is closed under
addition but not under scalar multiplication since
$\half(1,0)=(\half,0)$ is not in $V_2$.

\item[(iii)] Let $V_3=[1,2]$ be the closed interval in $\R$. The
set $V_3$ is neither closed under addition ($1+1.5=2.5\not\in
V_3$) nor under scalar multiplication ($4\cdot 1.5 = 6\not\in
V_3$).  Hence the set $V_3$ is not closed under vector addition
and not closed under scalar multiplication.
\end{enumerate}

\subsection*{Subspaces}

\begin{Def} \label{subspaces}
Let $V$ be a vector space.  A nonempty subset $W\subset V$ is a
{\em subspace\/} if $W$ is a vector space using the operations of 
addition and scalar multiplication defined on $V$.
\end{Def} \index{subspace}

Note that in order for a subset $W$ of a vector space $V$ to be a 
subspace it must be {\em closed under addition\/} and {\em closed under
scalar multiplication\/}.  That is, suppose $w_1,w_2\in W$ and 
$r\in\R$.  Then
\begin{itemize}
\item[(i)]  $w_1+w_2 \in W$, and
\item[(ii)]  $rw_1\in W$.
\end{itemize}
\index{vector!addition} \index{scalar multiplication}

The $x$-axis and the $xz$-plane are examples of subsets of $\R^3$
that are closed under addition and closed under scalar multiplication.
Every vector on the $x$-axis has the form $(a,0,0)\in\R^3$.
The sum of two vectors $(a,0,0)$ and $(b,0,0)$ on the $x$-axis is
$(a+b,0,0)$ which is also on the $x$-axis.  The $x$-axis
is also closed under scalar multiplication as $r(a,0,0)=(ra,0,0)$,
and the $x$-axis is a subspace of $\R^3$.  Similarly, every vector
in the $xz$-plane in $\R^3$ has the form $(a_1,0,a_3)$. As in the
case of the $x$-axis, it is easy to verify that this set of vectors
is closed under addition and scalar multiplication.  Thus, the
$xz$-plane is also a subspace of $\R^3$.

In Theorem~\ref{T:subspaces} we show that every subset of a vector space 
that is closed under addition and scalar multiplication is a subspace.  
To verify this statement, we need the following lemma in which some special 
notation is used.  Typically, we use the same notation $0$ to denote the
real number zero and the zero vector\index{zero vector}.  In the following
lemma it is convenient to distinguish the two different uses of $0$, and we 
write the zero vector in boldface.

\begin{lemma}  \label{lem:AddId}
Let $V$ be a vector space, and let ${\bf 0}\in V$ be the zero vector.  Then
\[
0v={\bf 0} \quad \mbox{and}\quad (-1)v=-v
\]
for every vector in $v\in V$.
\end{lemma}

\proof Let $v$ be a vector in $V$ and use (D1) to compute
\[
     0v + 0v  = (0+0)v = 0v.
\]
By (A4) the vector $0v$ has an additive inverse $-0v$.  Adding $-0v$ 
to both sides yields 
\[
(0v + 0v) + (-0v) = 0v + (-0v) = {\bf 0}.
\] 
Associativity of addition (A2) now implies 
\[
0v + (0v + (-0v)) = {\bf 0}.
\]
A second application of (A4) implies that 
\[
0v + {\bf 0} = {\bf 0}
\]
and (A3) implies that $0v={\bf 0}$.

Next, we show that the additive inverse $-v$ of a vector $v$ is unique. 
That is, if $v + a = {\bf 0}$, then $a=-v$.  

Before beginning the proof, note that commutativity of addition (A1) together 
with (A3) implies that ${\bf 0}+v=v$.  Similarly, (A1) and (A4) imply that 
$-v+v={\bf 0}$.  

To prove uniqueness of additive inverses, add $-v$ to both sides of the 
equation $v + a = {\bf 0}$ yielding 
\[
-v + (v + a) = -v + {\bf 0}. 
\]
Properties (A2) and (A3) imply
\[
(-v + v) + a = -v.
\]
But 
\[
(-v + v) + a = {\bf 0} + a = a.
\]
Therefore $a = -v$, as claimed.

To verify that $(-1)v = -v$, we show that $(-1)v$ is the
additive inverse of $v$.  Using (M1), (D1), and the fact that $0v = {\bf 0}$, 
calculate
\[
     v + (-1)v = 1v + (-1)v =(1-1)v = 0v = {\bf 0}.
\]
Thus, $(-1)v$ is the additive inverse of $v$ and must equal $-v$, as
claimed.  \qed

\begin{thm}  \label{T:subspaces}
Let $W$ be a subset of the vector space $V$. If $W$ is closed under addition 
and closed under scalar multiplication, then $W$ is a subspace.
\end{thm}\index{subspace}
\proof  We have to show that $W$ is a vector space using the operations 
of addition and scalar multiplication defined on $V$.  That is, we need to
verify that the eight properties listed in Table~\ref{vectorspacelist} are 
satisfied.  Note that properties (A1), (A2), (M1), (M2), (D1), and (D2) are 
valid for vectors in $W$ since they are valid for vectors in $V$.

It remains to verify (A3) and (A4).  Let $w\in W$ be any vector.
Since $W$ is closed under scalar multiplication, it follows that
$0w$ and $(-1)w$ are in $W$. Lemma~\ref{lem:AddId} states that
$0w=0$ and $(-1)w=-w$; it follows that $0$ and $-w$ are in $W$.
Hence, properties (A3) and (A4) are valid for vectors in $W$,
since they are valid for vectors in $V$.  \qed

\subsubsection*{Examples of Subspaces of $\R^n$}

\begin{exam}  \label{EX:subspaces}
{\rm
\begin{itemize}
\item[(a)] Let $V$ be a vector space.  Then the subsets $V$ and $\{0\}$ are
always subspaces of $V$.  A subspace $W\subset V$ is {\em proper\/} if
$W\neq 0$ and $W\neq V$. \index{subspace!proper}
\item[(b)] Lines through the origin are subspaces of $\R^n$.  Let $w\in \R^n$
be a nonzero vector and let $W=\{rw:r\in\R\}$.  The set $W$ is closed under
addition and scalar multiplication and is a subspace of $\R^n$ by 
Theorem~\ref{T:subspaces}.
The subspace $W$ is just a {\em line through the origin\/} in $\R^n$, since
the vector $rw$ points in the same direction as $w$ when $r>0$ and the exact
opposite direction when $r<0$.
\item[(c)]  Planes containing the origin are subspaces of $\R^3$.  To verify
this point, let $P$ be a plane through the origin and let $N$ be a vector
perpendicular\index{perpendicular} to $P$.  Then $P$ consists of all vectors
$v\in\R^3$ perpendicular to $N$; using the dot-product (see Chapter~\ref{lineq},
\Ref{XX_0}) we recall that such vectors satisfy the linear equation
$N\cdot v = 0$.  By superposition\index{superposition}, the set of all
solutions to this equation is closed under addition and scalar multiplication
and is therefore a subspace by Theorem~\ref{T:subspaces}.
\end{itemize}
}
\end{exam}
In a sense that will be made precise all subspaces of $\R^n$ can be written
as the span of a finite number of vectors generalizing
Example~\ref{EX:subspaces}(b) or as solutions to a system of linear equations
generalizing Example~\ref{EX:subspaces}(c).


\subsubsection*{Examples of Subspaces of the Function Space ${\cal F}$}
\index{function space!subspace of}\index{subspace!of function space}

Let ${\cal P}$ be the set of all polynomials in ${\cal F}$.  The sum of
two polynomials is a polynomial and the scalar multiple of a polynomial is a
polynomial.  Thus, ${\cal P}$ is closed under addition and scalar
multiplication, and ${\cal P}$ is a subspace of ${\cal F}$.

As a second example of a subspace of ${\cal F}$, let $\CCone$ be the set of
all continuously differentiable functions $u:\R\to\R$.  A function $u$ is
in $\CCone$ if $u$ and $u'$ exist and are continuous for all $t\in\R$.
Examples of functions in $\CCone$ are:
\begin{enumerate}
\item[(i)] Every polynomial $p(t)=a_mt^m+a_{m-1}t^{m-1}+\cdots
+a_1t+a_0$ is in $\CCone$. \index{polynomial}
\item[(ii)]  The function $u(t)=e^{\lambda t}$ is in $\CCone$
for each constant $\lambda\in\R$.
\item[(iii)]  The trigonometric functions $u(t)=\sin(\lambda t)$
and $v(t)=\cos(\lambda t)$ are in $\CCone$ for each constant
$\lambda\in\R$. \index{trigonometric function}
\item[(iv)] $u(t)=t^{7/3}$ is twice differentiable everywhere and is
in $\CCone$.
\end{enumerate}
Equally there are many commonly used functions that are not in
$\CCone$.  Examples include:
\begin{enumerate}
\item[(i)] $u(t)=\frac{1}{t-5}$ is neither defined nor
continuous at $t=5$.
\item[(ii)] $u(t)=|t|$ is not differentiable (at $t=0$).
\item[(iii)] $u(t)=\csc(t)$ is neither defined nor continuous at
$t=k\pi$ for any integer $k$.
\end{enumerate}

The subset $\CCone\subset{\cal F}$  is a subspace and hence a vector space.
\index{vector!in $\CCone$}
The reason is simple.  If $x(t)$ and $y(t)$ are continuously differentiable,
then
\[
\frac{d}{dt}(x+y) = \frac{dx}{dt}+\frac{dy}{dt}.
\]
Hence $x+y$ is differentiable and is in $\CCone$ and $\CCone$ is closed under
addition.  Similarly, $\CCone$ is closed under scalar multiplication.  Let
$r\in\R$ and let $x\in\CCone$. Then
\[
\frac{d}{dt}(rx)(t) = r\frac{dx}{dt}(t).
\]
Hence $rx$ is differentiable and is in $\CCone$.

\subsubsection*{The Vector Space $(\CCone)^n$}

Another example of a vector space that combines the features of
both $\R^n$ and $\CCone$ is $(\CCone)^n$.  Vectors
$u\in (\CCone)^n$ have the form
\[
u(t) = (u_1(t),\ldots,u_n(t)),
\]
where each coordinate function $u_j(t)\in\CCone$.  Addition and
scalar multiplication in $(\CCone)^n$ are defined coordinatewise
--- just like addition and scalar multiplication in $\R^n$.
That is, let $u,v$ be in $(\CCone)^n$ and let $r$ be in $\R$, then
\begin{eqnarray*}
(u+v)(t) & = & (u_1(t)+v_1(t),\ldots,u_n(t)+v_n(t)) \\
(ru)(t) & = & (ru_1(t),\ldots,ru_n(t)).
\end{eqnarray*}
The set $(\CCone)^n$ satisfies the eight properties of vector spaces and
is a vector space.  Solutions to systems of $n$ linear ordinary differential
equations are vectors in $(\CCone)^n$.


\EXER

\TEXER

\begin{exercise} \label{c5.1.1}
Verify that the set $V_1$ consisting of all scalar multiples of
$(1,-1,-2)$ is a subspace of $\R^3$.
\end{exercise}

\begin{exercise} \label{c5.1.2}
Let $V_2$ be the set of all $2\times 3$ matrices.   Verify that
$V_2$ is a vector space.
\end{exercise}

\begin{exercise} \label{c5.1.3}
Let
\[
A=\left(\begin{array}{rrr} 1 & 1 & 0\\ 1 & -1 & 1 \end{array}
\right).
\]
Let $V_3$ be the set of vectors $x\in\R^3$ such that $Ax=0$.
Verify that $V_3$ is a subspace of $\R^3$.  Compare $V_1$ with
$V_3$.
\end{exercise}

\noindent In Exercises~\ref{c5.1.4a} -- \ref{c5.1.4f} you are given a
vector space $V$ and a subset $W$.  For each pair, decide whether or
not $W$ is a subspace of $V$.
\begin{exercise} \label{c5.1.4a}
$V=\R^3$ and $W$ consists of vectors in $\R^3$
     that have a $0$ in their first component.
\end{exercise}
\begin{exercise} \label{c5.1.4b}
$V=\R^3$ and $W$ consists of vectors in $\R^3$
     that have a $1$ in their first component.
\end{exercise}
\begin{exercise} \label{c5.1.4d}
$V=\R^2$ and $W$ consists of vectors in $\R^2$
     for which the sum of the components is $1$.
\end{exercise}
\begin{exercise} \label{c5.1.4c}
$V=\R^2$ and $W$ consists of vectors in $\R^2$
     for which the sum of the components is $0$.
\end{exercise}
\begin{exercise} \label{c5.1.4g}
$V=\CCone$ and $W$ consists of functions
     $x(t)\in\CCone$ satisfying $\int_{-2}^4x(t)dt =0$.
\end{exercise}
\begin{exercise} \label{c5.1.4e}
$V=\CCone$ and $W$ consists of functions
     $x(t)\in\CCone$ satisfying $x(1)=0$.
\end{exercise}
\begin{exercise} \label{c5.1.4f}
$V=\CCone$ and $W$ consists of functions
     $x(t)\in\CCone$ satisfying $x(1)=1$.
\end{exercise}

\noindent In Exercises~\ref{c5.1.5a} -- \ref{c5.1.5e} which of the
sets $S$ are subspaces?
\begin{exercise} \label{c5.1.5a}
$S = \{(a,b,c)\in\R^3 : a\ge 0, \; b\ge 0,\; c\ge 0\}$.
\end{exercise}
\begin{exercise} \label{c5.1.5b}
$S = \{(x_1,x_2,x_3)\in\R^3 : a_1x_1+a_2x_2+a_3x_3=0
\mbox{ where } a_1,a_2,a_3\in\R \mbox{ are fixed}\}$.
\end{exercise}
\begin{exercise} \label{c5.1.5c}
$S = \{(x,y)\in\R^2: (x,y) \mbox{ is on the line through }
(1,1) \mbox{ with slope } 1\}$.
\end{exercise}
\begin{exercise} \label{c5.1.5d}
$S = \{x\in\R^2: Ax=0\}$ where $A$ is a $3\times 2$ matrix.
\end{exercise}
\begin{exercise} \label{c5.1.5e}
$S = \{x\in\R^2: Ax=b\}$ where $A$ is a $3\times 2$ matrix
	and $b\in\R^3$ is a fixed nonzero vector.
\end{exercise}

\begin{exercise} \label{c5.1.6}
Let $V$ be a vector space and let $W_1$ and $W_2$ be subspaces.
Show that the intersection $W_1\cap W_2$ is also a subspace of $V$.
\end{exercise}

\begin{exercise} \label{c5.1.7a}
For which scalars $a,b,c$ do the solutions to the equation
\[
ax+by = c
\]
form a subspace of $\R^2$?
\end{exercise}
\begin{exercise} \label{c5.1.7b}
For which scalars $a,b,c,d$ do the solutions to the equation
\[
ax+by+cz = d
\]
form a subspace of $\R^3$?
\end{exercise}

\begin{exercise} \label{c5.1.8}
Show that the set of all solutions to the differential equation
$\dot{x}=2x$ is a subspace of $\CCone$.
\end{exercise}

\begin{exercise} \label{c5.1.9}
Recall from equation~\Ref{e:solnODE} of Section~\ref{S:IVP&E}
that solutions to the system of differential equations
\[
\frac{dX}{dt} = \mattwo{-1}{3}{3}{-1} X
\]
are
\[
X(t) = \alpha e^{2t}\vectwo{1}{1} + \beta e^{-4t}\vectwo{1}{-1}.
\]
Use this formula for solutions to show that the set of solutions
to this system of differential equations is a vector subspace of
$(\CCone)^2$.
\end{exercise}

\section{Construction of Subspaces} \label{S:5.2}

The principle of superposition shows that the set of all
solutions to a homogeneous system of linear equations is closed under 
addition and scalar multiplication and is a
subspace.  Indeed, there are two ways to describe subspaces:
first as solutions to linear systems, and second as the span of
a set of vectors.  We shall see that solving a homogeneous linear system of
equations just means writing the solution set as the span of a
finite set of vectors.

\subsection*{Solutions to Homogeneous Systems Form Subspaces}
\index{homogeneous} \index{subspace}

\begin{Def} \label{D:nullspace}
Let $A$ be an $m\times n$ matrix.  The {\em null space\/} of $A$
is the set of solutions to the homogeneous system of linear equations
\begin{equation} \label{Ax=0}
Ax=0.
\end{equation}
\end{Def} \index{null space}

\begin{lemma}
Let $A$ be an $m\times n$ matrix.  Then the null space of $A$
is a subspace of $\R^n$.
\end{lemma}

\proof
Suppose that $x$ and $y$ are solutions to \Ref{Ax=0}.  Then
\[
A(x+y) = Ax+Ay = 0+0 = 0;
\]
so $x+y$ is a solution of \Ref{Ax=0}.  Similarly, for $r\in\R$
\[
A(rx) = rAx = r0 = 0;
\]
so $rx$ is a solution of \Ref{Ax=0}.  Thus, $x+y$ and $rx$ are
in the null space of $A$, and the null space is closed under addition 
and scalar multiplication.  So Theorem~\ref{T:subspaces} implies that
the null space is a subspace of the vector space $\R^n$.   \qed

\subsubsection*{Solutions to Linear Systems of Differential Equations Form
Subspaces}

Let $C$ be an $n\times n$ matrix and let $W$ be the set of solutions to
the linear system of ordinary differential equations
\begin{equation} \label{Cx(t)}
\frac{dx}{dt}(t) = Cx(t).
\end{equation}
We will see later that a solution to \Ref{Cx(t)} has coordinate
functions $x_j(t)$ in $\CCone$.  The principle of superposition
then shows that $W$ is a subspace of $(\CCone)^n$.  Suppose
$x(t)$ and $y(t)$ are solutions of \Ref{Cx(t)}.  Then
\[
\frac{d}{dt}(x(t)+y(t)) = \frac{dx}{dt}(t) + \frac{dy}{dt}(t) =
Cx(t) + Cy(t) = C(x(t)+y(t));
\]
so $x(t)+y(t)$ is a solution of \Ref{Cx(t)} and in $W$.  A
similar calculation shows that $rx(t)$ is also in $W$ and
that $W\subset(\CCone)^n$ is a subspace.


\subsection*{Writing Solution Subspaces as a Span}
\index{homogeneous}\index{span}

The way we solve homogeneous systems of equations gives a second
method for defining subspaces.  For example, consider the system
\[
Ax=0,
\]
where
\[
A=\left(\begin{array}{rccc} 2 & 1 & 4 & 0 \\ -1 & 0 & 2 & 1
        \end{array}\right).
\]
The matrix $A$ is row equivalent to the reduced echelon form matrix
\[
E=\left(\begin{array}{ccrr} 1 & 0 & -2 & -1 \\ 0 & 1 & 8 & 2
        \end{array}\right).
\]
Therefore $x=(x_1,x_2,x_3,x_4)$ is a solution of $Ex=0$ if and
only if $x_1 = 2x_3+x_4$ and $x_2 = -8x_3 - 2x_4$.
It follows that every solution of $Ex=0$ can be written as:
\[
x = x_3\left(\begin{array}{r} 2 \\ -8\\ 1\\ 0 \end{array}\right)
+x_4\left(\begin{array}{r} 1 \\ -2\\ 0\\ 1 \end{array}\right).
\]
Since row operations do not change the set of solutions, it
follows that every solution of $Ax=0$ has this form. We have
also shown that every solution is generated by two vectors by
use of vector addition\index{vector!addition} and
scalar multiplication\index{scalar multiplication}.  We say that
this subspace is {\em spanned\/} by the two vectors
\[
\left(\begin{array}{r} 2 \\ -8\\ 1\\ 0 \end{array}\right)
\AND
\left(\begin{array}{r} 1 \\ -2\\ 0\\ 1 \end{array}\right).
\]
For example, a calculation verifies that the vector
\[
\left(\begin{array}{r} -1 \\ -2\\ 1\\ -3 \end{array}\right)
\]
is also a solution of $Ax=0$.  Indeed, we may write it as
\begin{equation}
\label{eq:SpanSol}
\left(\begin{array}{r} -1 \\ -2\\ 1\\ -3 \end{array}\right)
=
\left(\begin{array}{r} 2 \\ -8\\ 1\\ 0 \end{array}\right) -
3 \left(\begin{array}{r} 1 \\ -2\\ 0\\ 1 \end{array}\right).
\end{equation}


\subsection*{Spans}

Let $v_1,\ldots,v_k$ be a set of vectors in a vector space $V$.  A vector
$v\in V$ is a {\em linear combination\/} of $v_1,\ldots,v_k$
if
\[
v = r_1v_1 + \cdots + r_kv_k
\]
for some scalars $r_1,\ldots,r_k$.  \index{linear!combination}


\begin{Def}  \label{span}
The set of all linear combinations of the vectors $v_1,\ldots,v_k$
in a vector space $V$ is the {\em span\/} of $v_1,\ldots,v_k$ and is
denoted by $\Span\{v_1,\ldots,v_k\}$.
\end{Def} \index{span}

For example, the vector on the left hand side in \Ref{eq:SpanSol}
is a linear combination of the two vectors on the right hand side.

The simplest example of a span is $\R^n$ itself.  Let $v_j=e_j$
where $e_j\in\R^n$ is the vector with a $1$ in the $j^{th}$
coordinate and $0$ in all other coordinates.  Then every vector
$x=(x_1,\ldots,x_n)\in\R^n$ can be written as
\[
x = x_1e_1 + \cdots + x_ne_n.
\]
It follows that
\[
\R^n=\Span\{e_1,\ldots,e_n\}.
\]
Similarly, the set $\Span\{e_1,e_3\}\subset\R^3$ is just the
$x_1x_3$-plane, since vectors in this span are
\[
x_1e_1+x_3e_3 = x_1(1,0,0) + x_3(0,0,1) = (x_1,0,x_3).
\]

\begin{prop} \label{spansubspace} Let $V$ be a vector space and
let $w_1,\ldots,w_k\in V$. Then $W=\Span\{w_1,\ldots,w_k\}
\subset V$ is a subspace.
\end{prop} \index{span} \index{subspace}\index{vector!space}

\proof  Suppose $x,y\in W$.  Then
\begin{eqnarray*}
x & = & r_1w_1 + \cdots + r_kw_k \\
y & = & s_1w_1 + \cdots + s_kw_k
\end{eqnarray*}
for some scalars $r_1,\ldots,r_k$ and $s_1,\ldots,s_k$.  It
follows that
\[
x+y = (r_1+s_1)w_1 + \cdots + (r_k+s_k)w_k
\]
and
\[
rx = (rr_1)w_1 + \cdots + (rr_k)w_k
\]
are both in $\Span\{w_1,\ldots,w_k\}$. Hence $W\subset V$ is
closed under addition and scalar multiplication, and is a
subspace by Theorem~\ref{T:subspaces}. \qed

For example, let
\begin{equation}  \label{e:vandw}
v=(2,1,0) \AND w=(1,1,1)
\end{equation}
be vectors in $\R^3$. Then linear combinations of the vectors
$v$ and $w$ have the form
\[
\alpha v + \beta w = (2\alpha+\beta, \alpha+\beta, \beta)
\]
for real numbers $\alpha$ and $\beta$.  Note that every one of
these vectors is a solution to the linear equation
\begin{equation} \label{ex1}
x_1 - 2x_2 + x_3 = 0,
\end{equation}
that is, the $1^{st}$ coordinate minus twice the $2^{nd}$ coordinate 
plus the $3^{rd}$ coordinate equals zero.  Moreover, you may verify 
that every solution of \Ref{ex1} is a linear combination
of the vectors $v$ and $w$ in \Ref{e:vandw}.  Thus, the set of
solutions to the homogeneous\index{homogeneous} linear equation
\Ref{ex1} is a
subspace, and that subspace can be written as the span of
all linear combinations of the vectors $v$ and $w$.

In this language we see that the process of solving a
homogeneous system of linear equations is just the process of
finding a set of vectors that span the subspace of all
solutions.  Indeed,
we can now restate Theorem~\ref{number} of Chapter~\ref{lineq}.
Recall that a matrix $A$ has {\em rank\/} $\ell$ if it is row
equivalent to a matrix in echelon form with $\ell$ nonzero rows.
\index{rank}

\begin{prop}  \label{P:n-rank}
Let $A$ be an $m\times n$ matrix with rank $\ell$. Then the
null space of $A$ is the span of $n-\ell$ vectors.
\end{prop} \index{null space} \index{span}

We have now seen that there are two ways to describe subspaces ---
as solutions of homogeneous systems of linear equations and as a
span of a set of vectors, the {\em spanning set}\index{spanning set}.
Much of linear algebra is concerned
with determining how one goes from one description of a subspace
to the other.

\EXER

\TEXER

\noindent In Exercises~\ref{c5.2.1a} -- \ref{c5.2.1d} a single equation in
three variables is given.  For each equation write the subspace of solutions
in $\R^3$ as the span of two vectors in $\R^3$.
\begin{exercise} \label{c5.2.1a}
$4x - 2y + z = 0$.
\end{exercise}
\begin{exercise} \label{c5.2.1b}
$x - y + 3z = 0$.
\end{exercise}
\begin{exercise} \label{c5.2.1c}
$x + y + z = 0$.
\end{exercise}
\begin{exercise} \label{c5.2.1d}
$y=z$.
\end{exercise}

\noindent In Exercises~\ref{c5.2.2a} -- \ref{c5.2.2d} each of the
given matrices is in reduced echelon form.  Write solutions of the
corresponding homogeneous system of linear equations as a span of vectors.
\begin{exercise} \label{c5.2.2a}
$A = \left(\begin{array}{rrrrr} 1 & 2 & 0 & 1 & 0 \\
	0 & 0 & 1 & 4 & 0 \\ 0 & 0 & 0 & 0 & 1 \end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c5.2.2b}
$B = \left(\begin{array}{rrrr} 1 & 3 & 0 & 5 \\
	0 & 0 & 1 & 2 \end{array}\right)$.
\end{exercise}

\begin{exercise} \label{c5.2.2c}
$A = \left(\begin{array}{rrr} 1 & 0 & 2 \\
        0 & 1 & 1\end{array}\right)$.
\end{exercise}
\begin{exercise} \label{c5.2.2d}
$B = \left(\begin{array}{rrrrrr} 1 & -1 & 0 & 5 & 0 & 0\\
        0 & 0 & 1 & 2 & 0 & 2\\
        0 & 0 & 0 & 0 & 1 & 2\end{array}\right)$.
\end{exercise}

\begin{exercise} \label{c5.2.3}
Write a system of two linear equations of the form $Ax=0$ where
$A$ is a $2\times 4$ matrix whose subspace of solutions in $\R^4$
is the span of the two vectors
\[
v_1 = \left(\begin{array}{r} 1 \\ -1 \\ 0 \\  0 \end{array}\right) \AND
v_2 = \left(\begin{array}{r} 0 \\  0 \\ 1 \\ -1 \end{array}\right).
\]
\end{exercise}

\begin{exercise} \label{c5.2.4}
Write the matrix $A=\mattwo{2}{2}{-3}{0}$ as a linear combination
of the matrices
\[
B=\mattwo{1}{1}{0}{0} \AND C=\mattwo{0}{0}{1}{0}.
\]
\end{exercise}

\begin{exercise} \label{c5.2.5}
Is $(2,20,0)$ in the span of $w_1=(1,1,3)$ and $w_2=(1,4,2)$?
Answer this question by setting up a system of linear equations
and solving that system by row reducing the associated augmented
matrix\index{matrix!augmented}.
\end{exercise}

\noindent In Exercises~\ref{c5.2.6a} -- \ref{c5.2.6d} let $W\subset\CCone$
be the subspace spanned by the two polynomials $x_1(t) = 1$ and
$x_2(t)=t^2$.  For the given function $y(t)$ decide whether or not $y(t)$
is an element of $W$.  Furthermore, if $y(t)\in W$, determine whether the set
$\{y(t),x_2(t)\}$ is a spanning set for $W$.
\begin{exercise} \label{c5.2.6a}
$y(t) = 1-t^2$,
\end{exercise}
\begin{exercise} \label{c5.2.6b}
$y(t) = t^4$,
\end{exercise}
\begin{exercise} \label{c5.2.6c}
$y(t) = \sin t$,
\end{exercise}
\begin{exercise} \label{c5.2.6d}
$y(t) = 0.5 t^2$
\end{exercise}

\begin{exercise} \label{c5.2.7}
Let $W\subset\R^4$ be the subspace that is spanned by the vectors
\[
        w_1=(-1,2,1,5)\AND w_2=(2,1,3,0).
\]
Find a linear system of two equations such that
$W=\Span\{w_1,w_2\}$ is the set of solutions of this system.
\end{exercise}

\begin{exercise} \label{c5.2.8a}
Let $V$ be a vector space and let $v\in V$ be a nonzero vector.  Show that
\[
\Span\{v,v\}=\Span\{v\}.
\]
\end{exercise}
\begin{exercise} \label{c5.2.8b}
Let $V$ be a vector space and let $v,w\in V$ be vectors.  Show that
\[
\Span\{v,w\}=\Span\{v,w,v+3w\}.
\]
\end{exercise}

\begin{exercise} \label{c5.2.9}
Let $W=\Span\{w_1,\ldots,w_k\}$ be a subspace of the vector
space $V$ and let $w_{k+1}\in W$ be another vector.  Prove that
$W=\Span\{w_1,\ldots,w_{k+1}\}$.
\end{exercise}

\begin{exercise} \label{c5.2.10}
Let $Ax=b$ be a system of $m$ linear equations in $n$ unknowns,
and let $r=\rank(A)$ and $s=\rank(A|b)$.  Suppose that this system
has a unique solution.  What can you say about the relative
magnitudes of $m,n,r,s$?
\end{exercise}


\section{Spanning Sets and MATLAB} \label{S:5.3}

In this section we discuss:
\begin{itemize}
\item	how to find a spanning set for the subspace of solutions to a
homogeneous system of linear equations using the \Matlab command {\tt null},
and
\item	how to determine when a vector is in the subspace spanned by a
set of vectors using the \Matlab command {\tt rref}.
\end{itemize}

\subsection*{Spanning Sets for Homogeneous Linear Equations}

In Chapter~\ref{lineq} we saw how to use Gaussian elimination,
back substitution, and \Matlab to compute solutions to a system
of linear equations.  For systems of
homogeneous equations\index{homogeneous}, \Matlab
provides a command to find a spanning set for the subspace of solutions.
That command is {\tt null}.  For example, if we type
\begin{verbatim}
A = [2 1 4 0; -1 0 2 1]
B = null(A)
\end{verbatim} \index{\computer!null}
then we obtain
\begin{verbatim}
B =
    0.4830         0
   -0.4140    0.8729
   -0.1380   -0.2182
    0.7591    0.4364
\end{verbatim}
The two columns of the matrix $B$ span the set of solutions of
the equation $Ax=0$.  In particular, the vector $(2,-8,1,0)$ is a
solution to $Ax=0$ and is therefore a
linear combination \index{linear!combination} of the
column vectors of {\tt B}.  Indeed, type
\begin{verbatim}
4.1404*B(:,1)-7.2012*B(:,2)
\end{verbatim}
and observe that this linear combination is the desired one.

Next we describe how to find the coefficients {\tt 4.1404} and
{\tt -7.2012} by showing that these coefficients themselves are
solutions to another system of linear equations.

\subsection*{When is a Vector in a Span?} \index{span}

Let $w_1,\ldots,w_k$ and $v$ be vectors in $\R^n$.  We now
describe a method that allows us to decide whether $v$ is in
$\Span\{w_1,\ldots,w_k\}$.  To answer this question one has
to solve a system of $n$ linear equations in $k$ unknowns.
The unknowns correspond to the coefficients in the linear
combination of the vectors $w_1,\ldots,w_k$ that gives $v$.

Let us be more precise.  The vector\index{vector} $v$ is in
$\Span\{w_1,\ldots,w_k\}$ if and only if there are constants
$r_1,\ldots,r_k$ such that the equation
\begin{equation}  \label{e:lindepeqn}
     r_1 w_1 + \cdots + r_k w_k = v
\end{equation}
is valid.  Define the $n\times k$ matrix $A$ as the one having
$w_1,\ldots,w_k$ as its columns; that is,
\begin{equation}  \label{E:Abycol}
A = (w_1| \cdots |w_k).
\end{equation}
Let  $r$ be the $k$-vector
\[
r= \left(\begin{array}{c} r_1 \\ \vdots \\ r_k\end{array}\right).
\]
Then we may rewrite equation \Ref{e:lindepeqn} as
\begin{equation}  \label{E:Ar=v}
   Ar=v.
\end{equation}
To summarize:
\begin{lemma}
Let $w_1,\ldots,w_k$ and $v$ be vectors in $\R^n$.  Then $v$
is in $\Span\{w_1,\ldots,w_k\}$ if and only if the system of linear
equations \Ref{E:Ar=v} has a solution where $A$ is the $n\times k$
defined in \Ref{E:Abycol}.
\end{lemma}

To solve \Ref{E:Ar=v} we row reduce the
augmented matrix\index{matrix!augmented} $(A|v)$.
For example, is $v=(2,1)$ in the span of $w_1=(1,1)$ and $w_2=(1,-1)$?
That is, do there exist scalars $r_1,r_2$ such that
\[
r_1\vectwo{1}{1} + r_2\vectwo{1}{-1} = \vectwo{2}{1}?
\]
As noted, we can rewrite this equation as
\[
\mattwo{1}{1}{1}{-1}\vectwo{r_1}{r_2} = \vectwo{2}{1}.
\]
We can solve this equation by row reducing the augmented
matrix
\[
\left(\begin{array}{rr|r}
1 & 1 & 2 \\ 1 & -1 & 1 \end{array}\right)
\]
to obtain
\[
\left(\begin{array}{rr|r}
1 & 0 & \frac{3}{2} \\ 0 & 1 & \frac{1}{2}
\end{array}\right).
\]
So $v = \frac{3}{2}w_1 + \frac{1}{2}w_2$.

Row reduction to reduced echelon form\index{echelon form!reduced}
has been preprogrammed in the
\Matlab command {\tt rref}. \index{\computer!rref}  Consider the
following example.  Let
\begin{equation}  \label{e:w1w2}
     w_1=(2,0,-1,4) \AND w_2=(2,-1,0,2)
\end{equation}
and ask the question whether $v=(-2,4,-3,4)$ is in $\Span\{w_1,w_2\}$.

In \Matlab load the matrix $A$ having $w_1$ and
$w_2$ as its columns and the vector $v$ by typing {\tt e5\_3\_5}
\begin{equation*}  \label{e:Aandv}
A=\left(\begin{array}{rr} 2 & 2 \\ 0 & -1 \\ -1 & 0 \\ 4 & 2
\end{array}\right) \AND
v=\left(\begin{array}{r} -2 \\ 4 \\ -3 \\ 4 \end{array}\right).
\end{equation*}%
We can solve the system of equations using \Matlabp.
First, form the augmented matrix by typing
\begin{verbatim}
aug = [A v]
\end{verbatim}
Then solve the system by typing {\tt rref(aug)} to obtain
\begin{verbatim}
ans =
     1   0   3
     0   1  -4
     0   0   0
     0   0   0
\end{verbatim}
It follows that $(r_1,r_2)=(3,-4)$ is a solution and $v=3w_1-4w_2$.

Now we change the $4^{th}$ entry in $v$ slightly by typing
{\tt v(4) = 4.01}.  There is no solution to the system of equations
\[
Ar = \left(\begin{array}{r} -2 \\ 4 \\ -3 \\ 4.01
\end{array}\right)
\]
as we now show.  Type
\begin{verbatim}
aug = [A v]
rref(aug)
\end{verbatim}
which yields
\begin{verbatim}
ans =
     1    0    0
     0    1    0
     0    0    1
     0    0    0
\end{verbatim}
This matrix corresponds to an inconsistent\index{inconsistent} system;
thus $v$ is no longer in the span\index{span} of $w_1$ and $w_2$.

\EXER

\CEXER

\noindent In Exercises~\ref{c5.3.1a} -- \ref{c5.3.1c} use the {\tt null}
command in \Matlab to find all the solutions of the linear system of
equations $Ax=0$.  \index{\computer!null}
\begin{exercise} \label{c5.3.1a}
\begin{equation*} \label{e:BCDa}
          A=    \left(\begin{array}{cccc}
                -4 & 0 & -4 & 3\\
                -4 & 1 & -1 & 1
                \end{array}\right) \quad
\end{equation*}
\end{exercise}
\begin{exercise} \label{c5.3.1b}
\begin{equation*} \label{e:BCDb}
           A=    \left(\begin{array}{rr}
                1 & 2 \\
                1 & 0 \\
                3 & -2
                \end{array}\right) \quad
\end{equation*}
\end{exercise}
\begin{exercise} \label{c5.3.1c}
\begin{equation*} \label{e:BCDc}
          A=      \left(\begin{array}{rrr}
               1  &  1  &  2\\
              -1  &  2  & -1
                \end{array}\right).
\end{equation*}
\end{exercise}

\begin{exercise} \label{c5.3.2}
Use the {\tt null} command in \Matlab to verify your answers to
Exercises~\ref{c5.2.2a} and \ref{c5.2.2b}.
\end{exercise}

\begin{exercise} \label{c5.3.3}
Use row reduction to find the solutions to $Ax=0$ where $A$ is
given in \Ref{e:BCDa}.  Does your answer agree with the \Matlab
answer using {\tt null}?  If not, explain why.
\end{exercise}

\noindent In Exercises~\ref{c5.3.4a} -- \ref{c5.3.4c} let $W\subset\R^5$
be the subspace spanned by the vectors
\begin{equation*}
     w_1=(2,0,-1,3,4),\quad w_2=(1,0,0,-1,2),\quad w_3=(0,1,0,0,-1).
\end{equation*}
Use \Matlab to decide whether the given vectors are elements of $W$.
\begin{exercise} \label{c5.3.4a}
$v_1=(2,1,-2,8,3)$.
\end{exercise}
\begin{exercise} \label{c5.3.4b}
$v_2=(-1,12,3,-14,-1)$.
\end{exercise}
\begin{exercise} \label{c5.3.4c}
$v_3=(-1,12,3,-14,-14)$.
\end{exercise}

\section{Linear Dependence and Linear Independence} \label{S:5.4}

An important question in linear algebra concerns finding spanning
sets for subspaces having the smallest
number of vectors. Let $w_1,\ldots,w_k$ be vectors in a vector
space $V$ and let $W=\Span\{w_1,\ldots,w_k\}$.  \index{span}
Suppose that $W$ is generated by a subset of these $k$ vectors.
Indeed, suppose that the $k^{th}$ vector is redundant in the
sense that $W=\Span\{w_1,\ldots,w_{k-1}\}$.  Since $w_k\in W$,
this is possible only if $w_k$ is a linear combination of the
$k-1$ vectors $w_1,\ldots,w_{k-1}$; that is, only if
\begin{equation}  \label{e:depend}
w_k = r_1w_1 + \cdots + r_{k-1}w_{k-1}.
\end{equation}
\begin{Def}  \label{lineardependence}
Let $w_1,\ldots,w_k$ be vectors in the vector space $V$.  The set
$\{w_1,\ldots,w_k\}$ is {\em linearly dependent\/} if one of the vectors
$w_j$ can be written as a linear combination of the remaining $k-1$ vectors.
\end{Def} \index{linearly!dependent} \index{linear!combination}
Note that when $k=1$, the phrase `$\{w_1\}$ is linearly dependent'
means that $w_1=0$.

If we set $r_k=-1$, then we may rewrite \Ref{e:depend} as
\[
r_1w_1 + \cdots + r_{k-1}w_{k-1} + r_k w_k =0.
\]
It follows that:
\begin{lemma}  \label{L:lindep}
The set of vectors $\{w_1,\ldots,w_k\}$ is linearly dependent if and
only if there exist scalars $r_1,\ldots,r_k$ such that
\begin{itemize}
\item[(a)]   at least one of the $r_j$ is nonzero, and
\item[(b)]   $r_1w_1 + \cdots + r_k w_k =0.$
\end{itemize}
\end{lemma}

For example, the vectors $w_1=(2,4,7)$, $w_2=(5,1,-1)$, and
$w_3=(1,-7,-15)$ are linearly dependent since $2w_1-w_2+w_3=0$.

\begin{Def}  \label{linearindependence}
A set of $k$ vectors $\{w_1,\ldots,w_k\}$ is {\em linearly
independent\/} if none of the $k$ vectors can be written as a
linear combination of the other $k-1$ vectors.
\end{Def} \index{linearly!independent}

Since linear independence means {\em not\/} linearly dependent,
Lemma~\ref{L:lindep} can be rewritten as:
\begin{lemma}  \label{L:linindep}
The set of vectors $\{w_1,\ldots,w_k\}$ is linearly independent if and
only if whenever
\[
r_1w_1 + \cdots + r_kw_k = 0,
\]
it follows that
\[
r_1 = r_2 = \cdots = r_k = 0.
\]
\end{lemma}

Let $e_j$ be the vector in $\R^n$ whose $j^{th}$ component is $1$
and all of whose other components are $0$. The set of vectors
$e_1,\ldots,e_n$ is the simplest example of a set of linearly
independent vectors in $\R^n$.  We use Lemma~\ref{L:linindep} to
verify independence by supposing that
\[
r_1e_1 + \cdots + r_ne_n = 0.
\]
A calculation shows that
\[
0 = r_1e_1 + \cdots + r_ne_n = (r_1,\ldots,r_n).
\]
It follows that each $r_j$ equals $0$, and the vectors
$e_1,\ldots,e_n$ are linearly independent.


\subsection*{Deciding Linear Dependence and Linear Independence}
\index{linearly!dependent}

Deciding whether a set of $k$ vectors in $\R^n$ is linearly
dependent or linearly independent is equivalent to solving a
system of linear equations.  Let $w_1,\ldots,w_k$ be vectors
in $\R^n$, and view these vectors as column vectors. Let
\begin{equation}  \label{E:Ank}
A=(w_1|\cdots|w_k)
\end{equation}
be the $n\times k$ matrix whose columns are the vectors $w_j$.
Then a vector
\[
R = \vect{r}{k}
\]
is a solution to the system of equations $AR=0$ precisely when
\begin{equation}
r_1w_1 + \cdots + r_kw_k = 0.
\end{equation}
If there is a nonzero solution $R$ to $AR=0$, then the vectors
$\{w_1,\ldots,w_k\}$ are linearly dependent; if the only solution
to $AR=0$ is $R=0$, then the vectors are linearly independent.

The preceding discussion is summarized by:
\begin{lemma}
The vectors $w_1,\ldots,w_k$ in $\R^n$ are linearly dependent if the
null space of the $n\times k$ matrix $A$ defined in \Ref{E:Ank} is
nonzero and linearly independent if the null space of $A$ is zero.
\end{lemma} \index{linearly!dependent}\index{linearly!independent}
\index{null space}


\subsubsection*{A Simple Example of Linear Independence with Two Vectors}

The two vectors
\[
w_1 = \left(\begin{array}{r} 2 \\ -8\\ 1\\ 0 \end{array}\right)
\AND
w_2 = \left(\begin{array}{r} 1 \\ -2\\ 0\\ 1 \end{array}\right)
\]
are linearly independent.  To see this suppose that
$r_1 w_1 + r_2 w_2 = 0$.  Using the components of $w_1$ and $w_2$
this equality is equivalent to the system of four equations
\[
2r_1 + r_2 = 0,\quad -8r_1 - 2r_2 = 0,\quad r_1 = 0, \AND r_2 = 0.
\]
In particular, $r_1 = r_2 = 0$; hence $w_1$ and $w_2$ are
linearly independent.


\subsubsection*{Using \Matlab to Decide Linear Dependence}

Suppose that we want to determine whether or not the vectors
\begin{equation*}
w_1 = \left(\begin{array}{r} 1 \\ 2 \\ -1 \\ 3 \\ 5 \end{array}\right)
\quad
w_2 = \left(\begin{array}{r} -1 \\ 1 \\ 4 \\ -2 \\ 0 \end{array}\right)
\quad
w_3 = \left(\begin{array}{r} 1 \\ 1 \\ -1 \\ 3 \\ 12 \end{array}\right)
\quad
w_4 = \left(\begin{array}{r} 0 \\ 4 \\ 3 \\ 1 \\ -2 \end{array}\right)
\end{equation*}%
are linearly dependent.  After typing {\tt e5\_4\_4} in \Matlabp, form
the $5\times 4$ matrix $A$ by typing
\begin{verbatim}
A = [w1 w2 w3 w4]
\end{verbatim}
Determine whether there is a nonzero solution to $AR=0$ by typing
\begin{verbatim}
null(A)
\end{verbatim} \index{\computer!null}
The response from \Matlab is
\begin{verbatim}
ans =
   -0.7559
   -0.3780
    0.3780
    0.3780
\end{verbatim}
showing that there is a nonzero solution to $AR=0$ and the vectors
$w_j$ are linearly dependent.  Indeed, this solution for $R$ shows
that we can solve for $w_1$ in terms of $w_2,w_3,w_4$.
We can now ask whether or not $w_2,w_3,w_4$ are linearly dependent.
To answer this question form the matrix
\begin{verbatim}
B = [w2 w3 w4]
\end{verbatim}
and type {\tt null(B)} to obtain
\begin{verbatim}
ans =
   Empty matrix: 3-by-0
\end{verbatim}
showing that the only solution to $BR=0$ is the zero solution $R=0$.
Thus, $w_2,w_3,w_4$ are linearly independent.  For these particular
vectors, any three of the four are linearly independent.

\EXER

\TEXER

\begin{exercise} \label{c5.4.1}
Let $w$ be a vector in the vector space $V$.  Show that the sets of vectors
$\{w,0\}$ and $\{w,-w\}$ are linearly dependent.
\end{exercise}

\begin{exercise} \label{c5.4.2}
For which values of $b$ are the vectors $(1,b)$ and $(3,-1)$
linearly independent?
\end{exercise}

\begin{exercise} \label{c5.4.3}
Let
\[
u_1=(1,-1,1) \quad u_2=(2,1,-2) \quad u_3 = (10,2,-6).
\]
Is the set $\{u_1,u_2,u_3\}$ linearly dependent or linearly independent?
\end{exercise}


\begin{exercise} \label{c5.4.4}
For which values of $b$ are the vectors $(1,b,2b)$ and $(2,1,4)$
linearly independent?
\end{exercise}

\begin{exercise} \label{c5.4.5}
Show that the polynomials $p_1(t) = 2+t$, $p_2(t) = 1+t^2$, and
$p_3(t) = t-t^2$ are linearly independent vectors in the vector
space $\CCone$.
\end{exercise}

\begin{exercise} \label{c5.4.6}
Show that the functions $f_1(t) = \sin t$, $f_2(t)=\cos t$, and
$f_3(t)=\cos\left(t+\frac{\pi}{3}\right)$ are linearly dependent
vectors in $\CCone$.
\end{exercise}

\begin{exercise} \label{c5.4.7}
Suppose that the three vectors $u_1,u_2,u_3\in\R^n$ are linearly
independent.  Show that the set
\[
\{u_1+u_2, u_2+u_3,u_3+u_1\}
\]
is also linearly independent.
\end{exercise}

\CEXER

\noindent In Exercises~\ref{c5.4.8a} -- \ref{c5.4.8a}, determine whether
the given sets of vectors are linearly independent or linearly dependent.
\begin{exercise} \label{c5.4.8a}
\begin{equation*}
v_1 = (2,1,3,4) \quad v_2 = (-4,2,3,1) \quad v_3 = (2,9,21,22)
\end{equation*}
\end{exercise}
\begin{exercise} \label{c5.4.8b}
\begin{equation*}
w_1 = (1,2,3)\quad w_2 = (2,1,5) \quad w_3 = (-1,2,-4)
\quad w_4 = (0,2,-1)
\end{equation*}
\end{exercise}
\begin{exercise} \label{c5.4.8c}
\begin{equation*}
x_1 = (3,4,1,2,5) \quad x_2 = (-1,0,3,-2,1)\quad x_3 = (2,4,-3,0,2)
\end{equation*}
\end{exercise}

\begin{exercise} \label{c5.4.9}
Perform the following experiments.
\begin{itemize}
\item[(a)]   Use \Matlab to choose randomly three column vectors in
$\R^3$.  The \Matlab commands to choose these vectors are:
\begin{verbatim}
y1 = rand(3,1)
y2 = rand(3,1)
y3 = rand(3,1)
\end{verbatim}\index{\computer!rand}
Use the methods of this section to determine whether these vectors
are linearly independent or linearly dependent.
\item[(b)]  Now perform this exercise five times and record the number
of times a linearly independent set of vectors is chosen and the
number of times a linearly dependent set is chosen.
\item[(c)]  Repeat the experiment in (b) --- but this time randomly
choose four vectors in $\R^3$ to be in your set.
\end{itemize}
\end{exercise}

\section{Dimension and Bases} \label{S:5.5}

The minimum number of vectors that span a vector space has special
significance.

\begin{Def}
The vector space $V$ has {\em finite dimension\/} if $V$ is the
span of a finite number of vectors.  If $V$ has finite dimension, then the
smallest number of vectors that span $V$ is called the {\em dimension\/} of
$V$ and is denoted by $\dim V$.
\end{Def} \index{dimension}\index{dimension!finite} \index{span}

For example, recall that $e_j$ is the vector in $\R^n$ whose $j^{th}$
component is $1$ and all of whose other components are $0$.
Let $x=(x_1,\ldots,x_n)$ be in $\R^n$. Then
\begin{equation}  \label{e:spanrn}
x =  x_1e_1 + \cdots + x_ne_n.
\end{equation}
Since every vector in $\R^n$ is a linear combination of the
vectors $e_1,\ldots,e_n$, it follows that
$\R^n=\Span\{e_1,\ldots,e_n\}$.  Thus, $\R^n$ is finite
dimensional.  Moreover, the dimension of $\R^n$ is at most $n$,
since $\R^n$ is spanned by $n$ vectors. It seems unlikely that
$\R^n$ could be spanned by fewer than $n$ vectors--- but this
point needs to be proved.

\subsubsection*{An Example of a Vector Space that is Not Finite Dimensional}
\index{dimension!infinite}

Next we discuss an example of a vector space that does not have finite 
dimension.  Consider the subspace ${\cal P}\subset\CCone$ consisting of 
polynomials \index{subspace!of polynomials} of all degrees.  We show that 
${\cal P}$ is not the span of a finite number of vectors and hence that 
${\cal P}$ does not have finite dimension.  Let $p_1(t),p_2(t),\ldots,p_k(t)$ 
be a set of $k$ polynomials and let $d$ be the maximum degree of these $k$ 
polynomials.  Then every polynomial in the span of $p_1(t),\ldots,p_k(t)$ has 
degree less than or equal to $d$.  In particular, $p(t)=t^{d+1}$ is a 
polynomial that is not in the span of $p_1(t),\ldots,p_k(t)$ and ${\cal P}$ 
is not spanned by finitely many vectors.


\subsection*{Bases and The Main Theorem}

\begin{Def} \label{basis}
Let ${\cal B} = \{w_1,\ldots,w_k\}$ be a set of vectors in a
vector space $W$.  The subset ${\cal B}$ is a {\em basis\/} for $W$
if ${\cal B}$ is a spanning set for $W$ with the smallest number
of elements in a spanning set for $W$.
\end{Def} \index{basis} \index{spanning set}

It follows that if $\{w_1,\ldots,w_k\}$ is a basis for $W$, then
$k=\dim W$. The main theorem about bases is:

\begin{thm}  \label{basis=span+indep}
A set of vectors ${\cal B} =\{w_1,\ldots,w_k\}$ in a vector space $W$
is a basis for $W$ if and only if the set ${\cal B}$ is linearly
independent and spans $W$.
\end{thm} \index{linearly!independent} \index{basis} \index{span}

\noindent {\bf Remark:}  The importance of Theorem~\ref{basis=span+indep} is
that we can show that a set of vectors is a basis by verifying spanning
and linear independence.   We never have to check directly that the spanning
set has the minimum number of vectors for a spanning set.


For example, we have shown previously that the set of vectors
$\{e_1,\ldots,e_n\}$ in $\R^n$ is linearly independent and spans $\R^n$.  It
follows from Theorem~\ref{basis=span+indep} that this set is a basis,
and that the dimension of $\R^n$ is $n$\index{dimension!of $\R^n$}.
In particular, $\R^n$ cannot be spanned by fewer than $n$ vectors.

The proof of Theorem~\ref{basis=span+indep} is given in Section~\ref{S:5.6}.


\subsection*{Consequences of Theorem~\protect\ref{basis=span+indep}}

We discuss two applications of Theorem~\ref{basis=span+indep}.  First,
we use this theorem to derive a way of determining the dimension of the
subspace spanned by a finite number of vectors.  Second, we show that the
dimension of the subspace of solutions to a homogeneous system of linear
equation $Ax=0$ is $n-\rank(A)$ where $A$ is an $m\times n$ matrix.

\subsubsection*{Computing the Dimension of a Span} \index{dimension}

We show that the dimension of a span of vectors can be found using
elementary row operations on $M$. \index{elementary row operations}

\begin{lemma}  \label{L:computerank}
Let $w_1,\ldots,w_k$ be $k$ row vectors in $\R^n$ and let
$W=\Span\{w_1,\ldots,w_k\}\subset\R^n$.  Define
\[
M =\left(\begin{array}{c} w_1\\ \vdots \\w_k \end{array}\right)
\]
to be the matrix whose rows are the $w_j$s.  Then
\begin{equation}  \label{e:dimW=rankM}
\dim(W) = \rank(M).
\end{equation}
\end{lemma}\index{dimension}\index{rank}

\proof To verify \Ref{e:dimW=rankM}, observe that the span of
$w_1,\ldots,w_k$ is unchanged by
\begin{itemize}
\item[(a)] swapping $w_i$ and $w_j$,
\item[(b)] multiplying $w_i$ by a nonzero scalar, and
\item[(c)] adding a multiple of $w_i$ to $w_j$.
\end{itemize}
That is, if we perform elementary row operations on $M$, the
vector space spanned by the rows of $M$ does not change. So we
may perform elementary row operations on $M$ until we arrive at
the matrix $E$ in reduced echelon form.  \index{echelon form!reduced} 
Suppose that $\ell=\rank(M)$; that is, suppose that $\ell$
is the number of nonzero rows in $E$.  Then
\[
E =\left(\begin{array}{c} v_1\\ \vdots \\v_\ell\\ 0 \\ \vdots
\\ 0 \end{array}\right),
\]
where the $v_j$ are the nonzero rows in the reduced echelon form
matrix.

We claim that the vectors $v_1,\ldots,v_\ell$ are linearly
independent.  It then follows from Theorem~\ref{basis=span+indep} that
$\{v_1,\ldots,v_\ell\}$ is a basis for $W$ and that the dimension of
$W$ is $\ell$.  To verify the claim, suppose
\begin{equation} \label{e:rowsums}
a_1v_1 + \cdots + a_\ell v_\ell = 0.
\end{equation}
We show that $a_i$ must equal $0$ as follows.  In the $i^{th}$
row, the pivot\index{pivot} must occur in some column --- say in the $j^{th}$
column.  It follows that the $j^{th}$ entry in the vector of the
left hand side of \Ref{e:rowsums} is
\[
0a_1 + \cdots + 0a_{i-1} +1a_i + 0a_{i+1} + \cdots + 0a_\ell =
a_i,
\]
since all entries in the $j^{th}$ column of $E$ other than the
pivot must be zero, as $E$ is in reduced echelon form.  \qed

For instance, let $W=\Span\{w1,w2,w3\}$ in $\R^4$ where
\begin{equation*} \label{eq:vectors}
w1 = (3, -2, 1,-1), \quad w2 = (1,5,10,12), \quad
w3 = (1,-12,-19,-25).
\end{equation*}%
To compute $\dim W$ in \Matlab, type \verb+e5_5_4+ to load the
vectors and type
\begin{verbatim}
M = [w1; w2; w3]
\end{verbatim}
Row reduction\index{row!reduction} of the matrix {\tt M} in \Matlab
leads to the reduced echelon form matrix
\begin{verbatim}
ans =
     1.0000         0    1.4706    1.1176
         0     1.0000    1.7059    2.1765
         0          0         0         0
\end{verbatim}
indicating that the dimension of the subspace $W$ is two, and
therefore $\{w_1,w_2,w_3\}$ is not a basis of $W$. Alternatively,
we can use the \Matlab command {\tt rank(M)}\index{\computer!rank}
to compute the rank of $M$ and the dimension of the span $W$.

However, if we change one of the entries in $w_3$, for instance
{\tt w3(3)=-18} then indeed the command {\tt rank([w1;w2;w3])}
gives the answer three indicating that for this choice of vectors
$\{w1,w2,w3\}$ is a basis for $\Span\{w1,w2,w3\}$.

\subsubsection*{Solutions to Homogeneous Systems Revisited}
\index{homogeneous}

We return to our discussions in Chapter~\ref{lineq} on solving
linear equations.  Recall that we can write all solutions to
the system of homogeneous equations $Ax=0$ in terms of a few
parameters, and that the null space of $A$ is the subspace of
solutions (See Definition~\ref{D:nullspace}).
More precisely, Proposition~\ref{P:n-rank} states that the number of
parameters needed is $n-\rank(A)$ where $n$ is the number of
variables in the homogeneous system.  We claim that the dimension
of the null space\index{dimension!of null space} is exactly
$n - \rank(A)$.

For example, consider the reduced echelon form $3\times 7$ matrix
\begin{equation}  \label{E:nullityexamp}
A=\left(\begin{array}{rrrrrrr}
1 & -4 & 0 &  2 & -3 & 0 & 8 \\
0 &  0 & 1 &  3 &  2 & 0 & 4 \\
0 &  0 & 0 &  0 &  0 & 1 & 2  \end{array}\right)
\end{equation}
that has rank three. Suppose that the unknowns for this system of
equations are
$x_1,\ldots,x_7$.  We can solve the equations associated with
$A$ by solving the first equation for $x_1$, the second equation
for $x_3$, and the third equation for $x_6$, as follows:
\begin{eqnarray*}
x_1 & = & 4x_2 - 2x_4 + 3x_5 - 8x_7 \\
x_3 & = &      - 3x_4 - 2x_5 - 4x_7 \\
x_6 & = &                    - 2x_7
\end{eqnarray*}
Thus, all solutions to this system of equations have the form
{\small
\begin{equation}   \label{e:expandsoln}
\left(\begin{array}{c} 4x_2 - 2x_4 + 3x_5 - 8x_7 \\ x_2 \\
-3x_4 - 2x_5 - 4x_7 \\ x_4 \\ x_5 \\  - 2x_7 \\ x_7 \end{array} \right)  =
x_2 \left(\begin{array}{r}  4 \\ 1 \\  0 \\ 0 \\ 0 \\  0 \\ 0
\end{array} \right) +
x_4 \left(\begin{array}{r} -2 \\ 0 \\ -3 \\ 1 \\ 0 \\  0 \\ 0
\end{array} \right) +
x_5 \left(\begin{array}{r}  3 \\ 0 \\ -2 \\ 0 \\ 1 \\  0 \\ 0
\end{array} \right) +
x_7 \left(\begin{array}{r} -8 \\ 0 \\ -4 \\ 0 \\ 0 \\ -2 \\ 1
\end{array} \right)
\end{equation} }
\noindent We can rewrite the right hand side of \Ref{e:expandsoln}
as a linear combination\index{linear!combination} of four
vectors $w_2,w_4,w_5,w_7$
\begin{equation}   \label{e:w'scomb}
x_2w_2 + x_4w_4 + x_5w_5 + x_7w_7.
\end{equation}

This calculation shows that the null space of $A$, which is
$W=\{x\in\R^7:Ax=0\}$, is spanned by the four vectors
$w_2,w_4,w_5,w_7$.  Moreover, this same calculation shows that
the four vectors  are linearly independent.
From the left hand side of \Ref{e:expandsoln} we see that if this
linear combination sums to zero, then $x_2=x_4=x_5=x_7=0$.  It
follows from Theorem~\ref{basis=span+indep} that $\dim W = 4$.

\begin{Def}  \label{D:nullity}
The {\em nullity\/} of $A$ is the dimension of the null space of $A$.
\end{Def} \index{nullity}\index{null space!dimension}

\begin{thm}  \label{T:dimsoln}
Let $A$ be an $m\times n$ matrix. Then
\[
{\rm nullity}(A) + \rank(A) = n.
\]
\end{thm} \index{rank}

\proof	Neither the rank nor the null space of $A$ are changed by
elementary row operations.  So we can assume that $A$ is in reduced
echelon form.  The rank of $A$ is the number of nonzero rows in
the reduced echelon form matrix.  Proposition~\ref{P:n-rank} states that
the null space is spanned by $p$ vectors where $p=n-\rank(A)$.  We
must show that these vectors are linearly independent.

Let $j_1,\ldots,j_p$ be the columns of $A$ that do not contain pivots.
In example \Ref{E:nullityexamp} $p=4$ and
\[
j_1 = 2, \qquad j_2 = 4, \qquad j_3 = 5, \qquad j_4 = 7.
\]
After solving for the variables corresponding to pivots, we find that
the spanning set of the null space consists of $p$ vectors in $\R^n$,
which we label as $\{w_{j_1},\ldots,w_{j_p}\}$.  See \Ref{e:expandsoln}.
Note that the $j_m$$^{th}$  entry of $w_{j_m}$ is $1$ while the
$j_m$$^{th}$ entry in all of the other $p-1$ vectors is $0$.  Again,
see \Ref{e:expandsoln} as an example that supports this statement.  It
follows that the set of spanning vectors is a linearly independent set.
That is, suppose that
\[
r_1w_{j_1} + \cdots + r_pw_{j_p} = 0.
\]
From the $j_m$$^{th}$ entry in this equation, it follows that $r_m=0$;
and the vectors are linearly independent.  \qed

Theorem~\ref{T:dimsoln} has an interesting and useful interpretation.
We have seen in the previous subsection that the rank of a matrix $A$
is just the number of linearly independent rows in $A$.
In linear systems each row of the coefficient matrix corresponds
to a linear equation.  Thus, the rank of $A$ may be thought of as the
number of independent equations in a system of linear equations.
This theorem just states that the space of solutions loses a dimension
for each independent equation.


\EXER

\TEXER

\begin{exercise} \label{c5.5.1}
Show that ${\cal U}=\{u_1,u_2,u_3\}$ where
\[
u_1=(1,1,0) \quad u_2=(0,1,0) \quad u_3=(-1,0,1)
\]
is a basis for $\R^3$.
\end{exercise}


\begin{exercise} \label{c5.5.2}
Let $S=\Span\{v_1,v_2,v_3\}$ where
\[
v_1=(1,0,-1,0) \quad v_2=(0,1,1,1) \quad v_3=(5,4,-1,4).
\]
Find the dimension of $S$ and find a basis for $S$.
\end{exercise}

\begin{exercise} \label{c5.5.3}
Find a basis for the null space of
\[
A =\left(\begin{array}{rrrr} 1 & 0 & -1 & 2\\ 1 & -1 & 0 & 0\\
4 & -5 & 1 & -2 \end{array} \right).
\]
What is the dimension of the null space of $A$?
\end{exercise}

\begin{exercise} \label{c5.5.4}
Show that the set $V$ of all $2\times 2$ matrices is a vector space.
Show that the dimension of $V$ is four by finding a basis of $V$
with four elements.  Show that the space $M(m,n)$ of all $m\times n$
matrices is also a vector space.  What is $\dim M(m,n)$?
\end{exercise}

\begin{exercise} \label{c5.5.5}
Show that the set ${\cal P}_n$ of all polynomials of degree less than
or equal to $n$ is a subspace of $\CCone$.  What is $\dim {\cal P}_2$?
What is $\dim {\cal P}_n$?
\end{exercise}

\begin{exercise} \label{c5.5.6}
Let ${\cal P}_3$ be the vector space of polynomials of degree at
most three in one variable $t$.  Let $p(t)=t^3+a_2t^2+a_1t+a_0$ where
$a_0,a_1,a_2\in\R$ are fixed constants.  Show that
\[
\left\{ p, \frac{dp}{dt}, \frac{d^2p}{dt^2}, \frac{d^3p}{dt^3}\right\}
\]
is a basis for ${\cal P}_3$.
\end{exercise}

\begin{exercise} \label{c5.5.7}
Let $u\in\R^n$ be a nonzero row vector.
\begin{itemize}
\item[(a)]  Show that the $n\times n$ matrix $A=u^tu$ is symmetric and that
$\rank(A)=1$.  {\bf Hint:}  Begin by showing that $Av^t=0$ for every vector
$v\in\R^n$ that is perpendicular to $u$ and that $Au^t$ is a nonzero multiple
of $u^t$.
\item[(b)]  Show that the matrix $P=I_n+u^tu$ is invertible.  {\bf Hint:}
Show that $\rank(P)=n$.
\end{itemize}
\end{exercise}


\section{The Proof of the Main Theorem} \label{S:5.6}

We begin the proof of Theorem~\ref{basis=span+indep} with two
lemmas on linearly independent and spanning sets.

\begin{lemma}  \label{reducetoindep}
Let $\{w_1,\ldots,w_k\}$ be a set of vectors in a vector space
$V$ and let $W$ be the subspace spanned by these vectors.  Then
there is a linearly independent subset of $\{w_1,\ldots,w_k\}$
that also spans $W$.
\end{lemma}\index{linearly!independent}

\proof If $\{w_1,\ldots,w_k\}$ is linearly independent, then the
lemma is proved.  If not, then the set $\{w_1,\ldots,w_k\}$ is
linearly dependent.  If this set is linearly dependent, then at
least one of the vectors is a linear combination of the others.
By renumbering if necessary, we can assume that $w_k$ is a
linear combination of $w_1,\ldots,w_{k-1}$; that is,
\[
w_k = a_1w_1 + \cdots + a_{k-1}w_{k-1}.
\]
Now suppose that $w\in W$.  Then
\[
w = b_1w_1 + \cdots + b_kw_k.
\]
It follows that
\[
w = (b_1+b_ka_1)w_1 + \cdots + (b_{k-1}+b_ka_{k-1})w_{k-1},
\]
and that $W=\Span\{w_1,\ldots,w_{k-1}\}$.  If the vectors
$w_1,\ldots,w_{k-1}$ are linearly independent, then the proof of
the lemma is complete.  If not, continue inductively until a
linearly independent subset of the $w_j$ that also spans $W$ is
found.  \qed

The important point in proving that linear independence together
with spanning imply that we have a basis is discussed in the next
lemma.

\begin{lemma}  \label{lem:lindep}
Let $W$ be an $m$-dimensional vector space and let $k>m$ be an integer.
Then any set of $k$ vectors in $W$ is linearly dependent.
\end{lemma} \index{linearly!dependent}

\proof Since the dimension of $W$ is $m$ we know that
this vector space can be written as $W=\Span\{v_1,\ldots,v_m\}$.
Moreover, Lemma~\ref{reducetoindep} implies that the vectors
$v_1,\ldots,v_m$ are linearly independent.  Suppose that
$\{w_1,\ldots,w_k\}$ is another set of vectors where $k>m$.
We have to show that the vectors $w_1,\ldots,w_k$ are linearly
dependent; that is, we must show that there exist scalars
$r_1,\ldots,r_k$ not all of which are zero that satisfy
\begin{equation} \label{independence1}
r_1w_1 + \cdots + r_kw_k = 0.
\end{equation}
We find these scalars by solving a system of linear equations, as
we now show.

The fact that $W$ is spanned by the vectors $v_j$ implies that
\begin{eqnarray*}
w_1 & = & a_{11}v_1 + \cdots + a_{m1}v_m\ \\
w_2 & = & a_{12}v_1 + \cdots + a_{m2}v_m\ \\
 & \vdots & \\\
w_k & = & a_{1k}v_1 + \cdots + a_{mk}v_m.
\end{eqnarray*}
It follows that $r_1w_1 + \cdots + r_kw_k$ equals
\[
\begin{array}{ll}
r_1(a_{11}v_1 + \cdots + a_{m1}v_m) & +  \\
r_2(a_{12}v_1 + \cdots + a_{m2}v_m) & + \cdots + \\
r_k(a_{1k}v_1 + \cdots + a_{mk}v_m) &
\end{array}
\]
Rearranging terms leads to the expression:
\begin{equation}   \label{e:r1v1etc}
\begin{array}{ll}
(a_{11}r_1 + \cdots + a_{1k}r_k)v_1\ & + \\
(a_{21}r_1 + \cdots + a_{2k}r_k)v_2\ & + \cdots + \\
(a_{m1}r_1 + \cdots + a_{mk}r_k)v_m. &
\end{array}
\end{equation}
Thus, \Ref{independence1} is valid if and only if \Ref{e:r1v1etc}
sums to zero.  Since the set $\{v_1,\ldots,v_m\}$ is linearly
independent, \Ref{e:r1v1etc} can equal zero if and only if
\begin{eqnarray*}
a_{11}r_1 + \cdots + a_{1k}r_k & = & 0\ \\
a_{21}r_1 + \cdots + a_{2k}r_k & = & 0\ \\
          & \vdots &   \\
a_{m1}r_1 + \cdots + a_{mk}r_k & = & 0.
\end{eqnarray*}
Since $m<k$, Chapter~\ref{lineq}, Theorem~\ref{number} implies that
this system of homogeneous linear equations always has a nonzero
solution $r=(r_1,\ldots,r_k)$ --- from which it follows that the
$w_i$ are linearly dependent.  \qed

\begin{cor}  \label{basis<n}
Let $V$ be a vector space of dimension $n$ and let $\{u_1,\ldots,u_k\}$ be a 
linearly independent set of vectors in $V$.  Then $k\le n$.
\end{cor} \index{linearly!independent}
\index{dimension}\index{vector!space}

\proof If $k>n$ then Lemma~\ref{lem:lindep} implies that
$\{u_1,\ldots,u_k\}$ is linearly dependent.  Since we have
assumed that this set is linearly independent, it follows that
$k\leq n$. \qed

\vspace{0.1in}

\noindent {\bf Proof of Theorem~\ref{basis=span+indep}}:
Suppose that ${\cal B}=\{w_1,\ldots,w_k\}$ is a basis for $W$.
By definition, ${\cal B}$ spans $W$ and $k=\dim W$.  We must
show that ${\cal B}$ is linearly independent.  Suppose ${\cal B}$
is linearly dependent, then Lemma~\ref{reducetoindep} implies
that there is a proper subset of ${\cal B}$ that spans $W$ (and
is linearly independent). This contradicts the fact that as a
basis ${\cal B}$ has the smallest number of elements of any
spanning set for $W$.

Suppose that ${\cal B}=\{w_1,\ldots,w_k\}$ both spans $W$ and is
linearly independent.  Linear independence and Corollary~\ref{basis<n}
imply that $k\leq\dim W$.  Since, by definition, any spanning set
of $W$ has at least $\dim W$ vectors, it follows that $k\geq\dim W$.
Thus, $k = \dim W$ and  ${\cal B}$ is a basis.   \qed

\subsection*{Extending Linearly Independent Sets to Bases} \index{basis}

Lemma~\ref{reducetoindep} leads to one approach to finding
bases.  Suppose that the subspace $W$ is spanned by a finite
set of vectors $\{w_1,\ldots,w_k\}$.  Then, we can throw out
vectors one by one until we arrive at a linearly independent
subset of the $w_j$.  This subset is a basis for $W$.

We now discuss a second approach to finding a basis for a
nonzero subspace $W$ of a finite dimensional vector space $V$.

\begin{lemma}  \label{extendindep}
Let $\{u_1,\ldots,u_k\}$ be a linearly independent set of
vectors in a vector space $V$ and assume that
\[
u_{k+1}\not\in\Span\{u_1,\ldots,u_k\}.
\]
Then $\{u_1,\ldots,u_{k+1}\}$ is also a linearly independent
set.
\end{lemma} \index{linearly!independent}

\proof  Let $r_1,\ldots,r_{k+1}$ be scalars such that
\begin{equation}  \label{rk+1}
r_1u_1 + \cdots + r_{k+1}u_{k+1} = 0.
\end{equation}
To prove independence, we need to show that all $r_j=0$.
Suppose $r_{k+1}\neq 0$.  Then we can solve \Ref{rk+1} for
\[
u_{k+1} = -\frac{1}{r_{k+1}}(r_1u_1+\cdots +r_ku_k),
\]
which implies that $u_{k+1}\in \Span\{u_1,\ldots,u_k\}$.  This
contradicts the choice of $u_{k+1}$.  So $r_{k+1}=0$ and
\[
r_1u_1 + \cdots + r_ku_k = 0.
\]
Since $\{u_1,\ldots,u_k\}$ is linearly independent, it follows
that $r_1=\cdots =r_k=0$.  \qed

The second method for constructing a basis is:
\index{basis!construction}
\begin{itemize}
\item        Choose a nonzero vector $w_1$ in $W$.
\item If $W$ is not spanned by $w_1$, then choose a vector $w_2$
that is not on the line spanned by $w_1$.
\item        If $W\neq\Span\{w_1,w_2\}$, then choose a vector
$w_3\not\in
\Span\{w_1,w_2\}$.
\item        If $W\neq\Span\{w_1,w_2,w_3\}$, then choose a vector
$w_4\not\in
\Span\{w_1,w_2,w_3\}$.
\item Continue until a spanning set\index{spanning set} for $W$ is
found.  This set is a basis for $W$.
\end{itemize}

We now justify this approach to finding bases for subspaces.
Suppose that $W$ is a subspace of a finite dimensional vector
space $V$.  For example, suppose that $W\subset\R^n$. Then
our approach to finding a basis of $W$ is as follows.  Choose a
nonzero vector $w_1\in W$.  If $W=\Span\{w_1\}$, then we are
done.  If not, choose a vector $w_2\in W\setmin\Span\{w_1\}$.
It follows from Lemma~\ref{extendindep} that $\{w_1,w_2\}$ is
linearly independent.  If $W=\Span\{w_1,w_2\}$, then
Theorem~\ref{basis=span+indep} implies that $\{w_1,w_2\}$ is
a basis for $W$, $\dim W=2$, and we are done.  If not, choose
$w_3\in W\setmin\Span\{w_1,w_2\}$ and $\{w_1,w_2,w_3\}$ is
linearly independent.  The finite dimension of $V$ implies that
continuing inductively must lead to a spanning set of linear
independent vectors for $W$ --- which by
Theorem~\ref{basis=span+indep} is a basis. This discussion proves:
\begin{cor}  \label{c:extendindependent}
Every linearly independent subset of a finite dimensional vector
space $V$ can be extended to a basis of $V$.
\end{cor}


\subsection*{Further consequences of Theorem~\ref{basis=span+indep}}

We summarize here several important facts about dimensions.

\begin{cor}  \label{dimensiondecreases}
Let $W$ be a subspace of a finite dimensional vector space $V$.
\begin{itemize}
\item[(a)]   Suppose that $W$ is a proper subspace\index{subspace!proper}.
Then $\dim W < \dim V$\index{dimension}.
\item[(b)]   Suppose that $\dim W = \dim V$.  Then $W=V$.
\end{itemize}
\end{cor}

\proof
(a) Let $\dim W = k$ and let $\{w_1,\ldots,w_k\}$ be a basis for
$W$.  Since $W$ is a proper subspace of $V$, there is a vector
$w\in V\setmin W$.  It follows from Lemma~\ref{extendindep} that
$\{w_1,\ldots,w_k,w\}$ is a linearly independent set.  Therefore,
Corollary~\ref{basis<n} implies that $k+1\le n$.

(b) Let $\{w_1,\ldots,w_k\}$ be a basis for $W$.
Theorem~\ref{basis=span+indep} implies that this set is linearly
independent.  If $\{w_1,\ldots,w_k\}$ does not span $V$, then it
can be extended to a basis as above.  But then $\dim V > \dim W$,
which is a contradiction.  \qed

\begin{cor} \label{C:dim=n}
Let ${\cal B}=\{w_1,\ldots,w_n\}$ be a set of $n$ vectors in an
$n$-dimensional vector space $V$.  Then the following are equivalent:
\begin{itemize}
\item[(a)]  ${\cal B}$ is a spanning set of $V$, \index{span}
\item[(b)]  ${\cal B}$ is a basis for $V$, and \index{basis}
\item[(c)] ${\cal B}$ is a linearly independent set.
\index{linearly!independent}
\end{itemize}
\end{cor}

\proof    By definition, (a) implies (b) since a basis is a spanning
set with the number of vectors equal to the dimension of the space.
Theorem~\ref{basis=span+indep} states that a basis is a linearly
independent set; so (b) implies (c). If ${\cal B}$ is a linearly
independent set of $n$ vectors, then it spans a subspace $W$ of
dimension $n$.  It follows from Corollary~\ref{dimensiondecreases}(b)
that $W=V$ and that (c) implies (a).  \qed


\subsubsection*{Subspaces of $\R^3$}
\index{$\R^3$!subspaces}

We can now classify all subspaces of $\R^3$.  They are:  the origin, lines
through the origin, planes through the origin, and $\R^3$.  All of these
sets were shown to be subspaces in Example~\ref{EX:subspaces}(a--c).

To verify that these sets are the only subspaces of $\R^3$, note that
Theorem~\ref{basis=span+indep} implies that proper subspaces of $\R^3$ have
dimension equal either to one or two. (The zero dimensional subspace is the
origin and the only three dimensional subspace is $\R^3$ itself.)  One
dimensional subspaces of $\R^3$ are spanned by one nonzero vector and are just
lines through the origin.  See Example~\ref{EX:subspaces}(b).  We claim that
all two dimensional subspaces are planes through the origin.

Suppose that $W\subset\R^3$ is a subspace spanned by two non-collinear vectors
$w_1$ and $w_2$.\index{collinear}  We show that $W$ is a plane \index{plane}
through the origin using results in Chapter~\ref{lineq}.  Observe that there
is a vector $N=(N_1,N_2,N_3)$ perpendicular to $w_1=(a_{11},a_{12},a_{13})$
and $w_2=(a_{21},a_{22},a_{23})$.  Such a vector $N$ satisfies the two linear
equations:
\begin{eqnarray*}
w_1\cdot N & = & a_{11}N_1 + a_{12}N_2 + a_{13}N_3 = 0 \\
w_2\cdot N & = & a_{21}N_1 + a_{22}N_2 + a_{23}N_3 = 0.
\end{eqnarray*}
Chapter~\ref{lineq}, Theorem~\ref{number} implies that a system of two linear
equations in three unknowns has a nonzero solution.  Let $P$ be the plane
perpendicular \index{perpendicular} to $N$ that contains the origin.  We show
that $W=P$ and hence that the claim is valid.

The choice of $N$ shows that the vectors $w_1$ and $w_2$ are both in $P$. In
fact, since $P$ is a subspace it contains every vector in $\Span\{w_1,w_2\}$.
Thus $W\subset P$.  If $P$ contains just one additional vector $w_3\in\R^3$
that is not in $W$, then the span of $w_1,w_2,w_3$ is three dimensional and
$P=W=\R^3$.



\EXER

\TEXER



\noindent In Exercises~\ref{c5.7.1a} -- \ref{c5.7.1c} you are
given a pair of vectors $v_1,v_2$ spanning a subspace of $\R^3$.
Decide whether that subspace is a line or a plane through the
origin.  If it is a plane, then compute a vector $N$ that is
perpendicular to that plane.
\begin{exercise} \label{c5.7.1a}
$v_1=(2,1,2) \AND v_2=(0,-1,1)$.
\end{exercise}
\begin{exercise} \label{c5.7.1b}
$v_1=(2,1,-1) \AND v_2=(-4,-2,2)$.
\end{exercise}
\begin{exercise} \label{c5.7.1c}
$v_1=(0,1,0) \AND v_2=(4,1,0)$.
\end{exercise}

\begin{exercise} \label{c5.7.2}
The pairs of vectors
\[
     v_1=(-1,1,0) \AND v_2=(1,0,1)
\]
span a plane $P$ in $\R^3$.  The pairs of vectors
\[
        w_1=(0,1,0) \AND w_2=(1,1,0)
\]
span a plane $Q$ in $\R^3$.  Show that $P$ and $Q$
are different and compute the subspace of $\R^3$ that
is given by the intersection $P\cap Q$.
\end{exercise}

\begin{exercise} \label{c5.6.1}
Let $A$ be a $7\times 5$ matrix with $\rank(A)=r$.\index{rank}
\begin{itemize}
\item[(a)]	What is the largest value that $r$ can have?
\item[(b)]	Give a condition equivalent to the system of
	equations $Ax=b$ having a solution.
\item[(c)]	What is the dimension of the null space of $A$?
\item[(d)]	If there is a solution to $Ax=b$, then how many
parameters are needed to describe the set of all solutions?
\end{itemize}
\end{exercise}

\begin{exercise} \label{c5.6.2}
Let
\[
A=\left(\begin{array}{rrrr} 1 & 3 & -1 & 4\\ 2& 1 & 5 & 7\\ 3 & 4 & 4 & 11
\end{array}\right).
\]
\begin{itemize}
\item[(a)] Find a basis for the subspace ${\cal C}\subset\R^3$ spanned by the
columns of $A$.
\item[(b)] Find a basis for the subspace ${\cal R}\subset\R^4$ spanned by the
rows of $A$.
\item[(c)] What is the relationship between $\dim {\cal C}$ and
$\dim {\cal R}$?
\end{itemize}
\end{exercise}


\begin{exercise} \label{c5.6.3}
Show that the vectors
\[
v_1=(2,3,1) \AND v_2=(1,1,3)
\]
are linearly independent.   Show that the span of $v_1$ and $v_2$
forms a plane in $\R^3$ by showing that every linear combination
 is the solution to a single linear equation.  Use this equation
to determine the normal vector $N$ to this plane.  Verify
Lemma~\ref{extendindep} by verifying directly that $v_1,v_2,N$
are linearly independent vectors.
\end{exercise}

\begin{exercise}  \label{c5.6.3A}
Let $W$ be an infinite dimensional subspace of the vector space $V$.
Show that $V$ is infinite dimensional.
\end{exercise}


\CEXER

\begin{exercise} \label{c5.6.4}
Consider the following set of vectors
\[
w_1 = (2, -2, 1), \quad
w_2 = (-1, 2, 0), \quad
w_3 = (3, -2, \lambda), \quad
w_4 = (-5, 6, -2),
\]
where $\lambda$ is a real number.
\begin{itemize}
\item[(a)] Find a value for $\lambda$ such that the
dimension of $\Span\{w_1,w_2,w_3,w_4\}$ is three. Then decide
whether $\{w_1,w_2,w_3\}$ or $\{w_1,w_2,w_4\}$ is a basis for
$\R^3$.
\item[(b)] Find a value for $\lambda$ such that the
dimension of $\Span\{w_1,w_2,w_3,w_4\}$ is two.
\end{itemize}
\end{exercise}

\begin{exercise} \label{c5.6.5}
Find a basis for $\R^5$ as follows.  Randomly choose vectors $x_1,x_2\in\R^5$
by typing {\tt x1 = rand(5,1)} and {\tt x2 = rand(5,1)}.  Check that these
vectors are linearly independent.  If not, choose another pair of vectors
until you find a linearly independent set.  Next choose a vector $x_3$ at
random and check that $x_1,x_2,x_3$ are linearly independent.  If not,
randomly choose another vector for $x_3$. Continue until you have five
linearly independent vectors --- which by a dimension count must be a
basis and span $\R^5$.  Verify this comment by using \Matlab
to write the vector
\[
\left(\begin{array}{r} 2 \\ 1 \\ 3\\ -2 \\ 4 \end{array}\right)
\]
as a linear combination of $x_1,\ldots,x_5$.
\end{exercise}

\begin{exercise} \label{c5.6.6}
Find a basis for the subspace of $\R^5$ spanned by
\begin{equation*}
\begin{array}{rcl}
u_1 & = & (1,1,0,0,1) \\
u_2 & = & (0,2,0,1,-1)  \\
u_3 & = & (0,-1,1,0,2)   \\
u_4 & = & (1,4,1,2,1)  \\
u_5 & = & (0,0,2,1,3).
\end{array}
\end{equation*}
\end{exercise}


\end{document}
